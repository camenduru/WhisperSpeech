[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "If you have questions or you want to help you can find us in the #audio-generation channel on the LAION Discord server.\nAn Open Source text-to-speech system built by inverting Whisper. Previously known as spear-tts-pytorch.\nWe want this model to be like Stable Diffusion but for speech – both powerful and easily customizable.\nWe are working only with properly licensed speech recordings and all the code is Open Source so the model will be always safe to use for commercial applications.\nCurrently the models are trained on the English LibreLight dataset. In the next release we want to target multiple languages (Whisper and EnCodec are both multilanguage)."
  },
  {
    "objectID": "index.html#progress-update-2023-12-10",
    "href": "index.html#progress-update-2023-12-10",
    "title": "WhisperSpeech",
    "section": "Progress update [2023-12-10]",
    "text": "Progress update [2023-12-10]\nAnother trio of models, this time they support multiple languages (English and Polish). Here are two new samples for a sneak peek. You can check out our Colab to try it yourself!\nEnglish speech, female voice (transferred from a Polish language dataset):\nhttps://github.com/collabora/WhisperSpeech/assets/107984/aa5a1e7e-dc94-481f-8863-b022c7fd7434\nA Polish sample, male voice:\nhttps://github.com/collabora/WhisperSpeech/assets/107984/4da14b03-33f9-4e2d-be42-f0fcf1d4a6ec\nOlder progress updates are archived here"
  },
  {
    "objectID": "index.html#downloads",
    "href": "index.html#downloads",
    "title": "WhisperSpeech",
    "section": "Downloads",
    "text": "Downloads\nWe encourage you to start with the Google Colab link above or run the provided notebook locally. If you want to download manually or train the models from scratch then both the WhisperSpeech pre-trained models as well as the converted datasets are available on HuggingFace."
  },
  {
    "objectID": "index.html#roadmap",
    "href": "index.html#roadmap",
    "title": "WhisperSpeech",
    "section": "Roadmap",
    "text": "Roadmap\n\nExtract acoustic tokens\nExtract Whisper embeddings and quantize them to semantic tokens\nSemantic token to acoustic token (S-&gt;A) model\nText token to semantic token (T-&gt;S) model\nImprove the EnCodec speech quality\nImprove inference of short sentences\nGather a bigger emotive speech dataset\nDocument the LibriLight derived datasets we released on Hugginface\nCreate a community effort to gather freely licensed speech in multiple languages\nTrain final multi-language models"
  },
  {
    "objectID": "index.html#architecture",
    "href": "index.html#architecture",
    "title": "WhisperSpeech",
    "section": "Architecture",
    "text": "Architecture\nThe general architecture is similar to AudioLM, SPEAR TTS from Google and MusicGen from Meta. We avoided the NIH syndrome and built it on top of powerful Open Source models: Whisper from OpenAI to generate semantic tokens and perform transcription, EnCodec from Meta for acoustic modeling and Vocos from Charactr Inc as the high-quality vocoder.\n\nWhisper for modeling semantic tokens\nWe utilize the OpenAI Whisper encoder block to generate embeddings which we then quantize with a small 2-layer model to get semantic tokens.\nIf the language is already supported by Whisper then this process requires only audio files (without ground truth transcriptions).\n\n\n\nUsing Whisper for semantic token extraction diagram"
  },
  {
    "objectID": "index.html#encodec-for-modeling-acoustic-tokens",
    "href": "index.html#encodec-for-modeling-acoustic-tokens",
    "title": "WhisperSpeech",
    "section": "EnCodec for modeling acoustic tokens",
    "text": "EnCodec for modeling acoustic tokens\nWe use EnCodec to model the audio waveform. Out of the box it delivers reasonable quality at 1.5kbps and we can bring this to high-quality by using Vocos – a vocoder pretrained on EnCodec tokens.\n\n\n\nEnCodec block diagram"
  },
  {
    "objectID": "index.html#appreciation",
    "href": "index.html#appreciation",
    "title": "WhisperSpeech",
    "section": "Appreciation",
    "text": "Appreciation\n      \nThis work would not be possible without the generous sponsorships from:\n\nCollabora – code development and model training\nLAION – community building and datasets\n\nWe are available to help you with both Open Source and proprietary AI projects. You can reach us via the Collabora website or on Discord ( and )"
  },
  {
    "objectID": "index.html#citations",
    "href": "index.html#citations",
    "title": "WhisperSpeech",
    "section": "Citations",
    "text": "Citations\nWe rely on many amazing Open Source projects and research papers:\n@article{SpearTTS,\n  title = {Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision},\n  url = {https://arxiv.org/abs/2302.03540},\n  author = {Kharitonov, Eugene and Vincent, Damien and Borsos, Zalán and Marinier, Raphaël and Girgin, Sertan and Pietquin, Olivier and Sharifi, Matt and Tagliasacchi, Marco and Zeghidour, Neil},\n  publisher = {arXiv},\n  year = {2023},\n}\n@article{MusicGen,\n  title={Simple and Controllable Music Generation}, \n  url = {https://arxiv.org/abs/2306.05284},\n  author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre Défossez},\n  publisher={arXiv},\n  year={2023},\n}\n@article{Whisper\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  publisher = {arXiv},\n  year = {2022},\n}\n@article{EnCodec\n  title = {High Fidelity Neural Audio Compression},\n  url = {https://arxiv.org/abs/2210.13438},\n  author = {Défossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},\n  publisher = {arXiv},\n  year = {2022},\n}\n@article{Vocos\n  title={Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis}, \n  url = {https://arxiv.org/abs/2306.00814},\n  author={Hubert Siuzdak},\n  publisher={arXiv},\n  year={2023},\n}"
  },
  {
    "objectID": "5b. multi-lang text to semantic token modeling.html",
    "href": "5b. multi-lang text to semantic token modeling.html",
    "title": "Text to semantic tokens model",
    "section": "",
    "text": "from whisperspeech.wer_metrics import *\nimport torchaudio\n\n\nfrom fastprogress import master_bar\n\n\nDataset\n\nsource\n\nload_dataset\n\n load_dataset (txt_shard_spec:str, stoks_shard_dir:str, samples:int,\n               txt_kind:str='small.en-txt', vq_codes:int=4096,\n               language:str='en', weight:float=1, validation:bool=False,\n               exclude_files:str=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntxt_shard_spec\nstr\n\ntranscription webdataset shards\n\n\nstoks_shard_dir\nstr\n\nstoks webdataset base dir\n\n\nsamples\nint\n\nsamples per epoch\n\n\ntxt_kind\nstr\nsmall.en-txt\n\n\n\nvq_codes\nint\n4096\n\n\n\nlanguage\nstr\nen\n\n\n\nweight\nfloat\n1\n\n\n\nvalidation\nbool\nFalse\n\n\n\nexclude_files\nstr\nNone\n\n\n\n\n\nsource\n\n\nlang_to_id\n\n lang_to_id (lang)\n\n\n\n\nAutomatic pdb calling has been turned OFF\n\n\n\ntrain_ds = load_dataset('../wolnelektury-wds2/wolnelektury-medium-txt-*.tar.gz', '../wolnelektury-vqv2/', 190000,\n                        txt_kind='medium-txt', vq_codes=512, language='pl',\n                        exclude_files='../wolnelektury-wds2/validation-samples')\nval_ds = load_dataset('../wolnelektury-wds2/validation-eqvad.tar.gz', '../wolnelektury-vqv2/', 520,\n                      txt_kind='medium-txt', vq_codes=512, language='pl', validation=True)\n\n\ntrain_ds = load_dataset('../librilight/librilight-medium-small.en-txt-*.tar.gz', '../librilight-vq-en+pl/', 190000,\n                        txt_kind='small.en-txt', vq_codes=512, language='en',\n                        exclude_files='')\nval_ds = load_dataset('../librilight/librilight-medium-small.en-txt-000000.tar.gz', '../librilight-vq-en+pl/', 512,\n                      txt_kind='small.en-txt', vq_codes=512, language='en', validation=True)\n\n\nfor x in progress_bar(train_ds, total=100): pass\nx\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:01&lt;00:00]\n    \n    \n\n\n[tensor([[  0,  83, 117,  ...,   0,   0,   0],\n         [  0,  73, 102,  ...,   0,   0,   0],\n         [  0,  72, 111,  ...,   0,   0,   0],\n         ...,\n         [  0,  70, 111,  ...,   0,   0,   0],\n         [  0,  83, 105,  ...,   0,   0,   0],\n         [  0,  97, 110,  ...,   0,   0,   0]]),\n tensor([[ 83, 117,  99,  ...,   0,   0,   0],\n         [ 73, 102,  32,  ...,   0,   0,   0],\n         [ 72, 111, 119,  ...,   0,   0,   0],\n         ...,\n         [ 70, 111, 114,  ...,   0,   0,   0],\n         [ 83, 105, 114,  ...,   0,   0,   0],\n         [ 97, 110, 100,  ...,   0,   0,   0]]),\n array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n array([15.20912548, 15.37558685, 14.93775934, 15.3280543 , 14.52830189,\n        11.8       , 16.25615764, 13.63636364, 10.37037037, 13.24451411,\n        12.83497884, 13.65582192, 12.32091691, 15.17241379, 13.61236802,\n        14.93055556, 11.51639344, 11.33633634, 10.5125523 , 12.17105263,\n        12.12374582, 13.22869955, 12.79434851, 15.33957845, 13.87262079,\n        12.99342105, 14.13043478, 14.77832512,  8.81849315, 13.0075188 ,\n        12.39130435, 15.65934066, 15.02192982, 12.98701299, 17.10526316,\n        17.92035398, 10.28963415,  6.77710843,  9.48275862, 15.625     ,\n        12.84916201, 10.42993631, 17.2439759 , 18.32706767, 16.6893733 ,\n        17.83333333, 19.2       , 16.96035242, 16.05504587, 15.6167979 ,\n        16.57894737, 16.24830393, 15.56737589, 14.63709677, 17.83216783,\n        14.82263514, 13.99371069, 14.60980036, 14.78494624, 14.66666667,\n        14.22413793, 14.14092664, 14.48801743, 16.21287129]),\n tensor([[511, 335, 135,  ..., 511, 511, 511],\n         [511, 309, 240,  ..., 511, 511, 511],\n         [511, 418, 191,  ..., 511, 511, 511],\n         ...,\n         [511, 153, 100,  ..., 511, 511, 511],\n         [511, 309,  51,  ..., 511, 511, 511],\n         [511, 206, 486,  ..., 511, 511, 511]]),\n tensor([[335, 135, 135,  ..., 511, 511, 511],\n         [309, 240, 240,  ..., 511, 511, 511],\n         [418, 191, 191,  ..., 511, 511, 511],\n         ...,\n         [153, 100, 100,  ..., 511, 511, 511],\n         [309,  51, 268,  ..., 511, 511, 511],\n         [206, 486, 160,  ..., 511, 511, 511]])]\n\n\n\nfor x in progress_bar(val_ds, total='noinfer'): pass\nx\n\n\n\n\n\n\n    \n      \n      100.00% [8/8 00:00&lt;00:00]\n    \n    \n\n\n[tensor([[  0,  73,  32,  ...,   0,   0,   0],\n         [  0,  66, 101,  ...,   0,   0,   0],\n         [  0,  84, 104,  ...,   0,   0,   0],\n         ...,\n         [  0,  65, 110,  ...,   0,   0,   0],\n         [  0,  77, 105,  ...,   0,   0,   0],\n         [  0,  69, 110,  ...,   0,   0,   0]]),\n tensor([[ 73,  32, 115,  ...,   0,   0,   0],\n         [ 66, 101, 102,  ...,   0,   0,   0],\n         [ 84, 104,  97,  ...,   0,   0,   0],\n         ...,\n         [ 65, 110, 100,  ...,   0,   0,   0],\n         [ 77, 105, 115,  ...,   0,   0,   0],\n         [ 69, 110, 100,  ...,   0,   0,   0]]),\n array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n array([14.30555556, 16.02040816, 15.10752688, 15.43696275, 15.34749035,\n        15.30612245, 16.00529101, 16.10898662, 13.84920635, 14.84771574,\n        14.93453355, 15.77134986, 16.34887006, 15.625     , 15.27131783,\n        16.68490153, 14.34931507,  2.75482094, 15.79949239, 15.50991501,\n        16.43026005, 15.51724138, 15.8008658 , 13.80718954, 15.17482517,\n        19.04145078, 15.9733777 , 15.8872077 , 15.82397004, 16.42754663,\n        16.79810726, 16.33959044, 16.09848485, 17.43772242, 15.86687307,\n        16.894061  , 15.96385542, 15.74074074, 16.57239819, 16.14832536,\n        16.41221374, 15.53398058, 14.9103139 , 16.5922619 , 16.82489451,\n        17.57679181, 16.2       , 17.46031746, 16.39105058, 17.6182708 ,\n        18.16939891, 17.67399267, 11.58038147, 17.53846154, 17.35197368,\n        18.87755102, 16.39433551, 16.56021898, 16.20192308, 16.40759931,\n        17.21991701, 15.91591592, 17.33021077, 15.        ]),\n tensor([[511, 400, 400,  ..., 511, 511, 511],\n         [511, 206,  38,  ..., 511, 511, 511],\n         [511, 206, 465,  ..., 511, 511, 511],\n         ...,\n         [511,  19, 486,  ..., 511, 511, 511],\n         [511, 206, 452,  ..., 511, 511, 511],\n         [511, 398,  59,  ..., 511, 511, 511]]),\n tensor([[400, 400, 400,  ..., 511, 511, 511],\n         [206,  38,  38,  ..., 511, 511, 511],\n         [206, 465, 440,  ..., 511, 511, 511],\n         ...,\n         [ 19, 486, 150,  ..., 511, 511, 511],\n         [206, 452, 450,  ..., 511, 511, 511],\n         [398,  59, 391,  ..., 511, 511, 511]])]\n\n\n\n\n\nModeling\n\nsource\n\nTunables\n\n Tunables (init_std:float=1, embeddings_std:float=0.01,\n           embeddings_lr_scale:float=5,\n           embedding_projector_lr_scale:float=2.5, output_mult:float=0.35,\n           query_mult:float=1, encoder_depth_ratio:float=0.25,\n           eot_dropout_p:float=0.5, cps_input:bool=True, cps_bins:int=32,\n           lr0:float=0.0015, clip_gradient_norm:float=0.2,\n           weight_decay:float=0.1, warmup_steps:float=4000,\n           random:bool=False)\n\n\nsource\n\n\nrand\n\n rand (start, end)\n\n\nsource\n\n\nEncoder\n\n Encoder (depth=6, width=384, n_head=6, length=1500, codes=1024,\n          emb_width=384, ffn_mult=4, pos_embs=None,\n          tunables=Tunables(init_std=1, embeddings_std=0.01,\n          embeddings_lr_scale=5, embedding_projector_lr_scale=2.5,\n          output_mult=0.35, query_mult=1, encoder_depth_ratio=0.25,\n          eot_dropout_p=0.5, cps_input=True, cps_bins=32, lr0=0.0015,\n          clip_gradient_norm=0.2, weight_decay=0.1, warmup_steps=4000,\n          random=False))\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nDecoder\n\n Decoder (depth=6, stoks_width=384, width=384, n_head=6, length=1500,\n          codes=1024, ffn_mult=4, pos_embs=None,\n          tunables=Tunables(init_std=1, embeddings_std=0.01,\n          embeddings_lr_scale=5, embedding_projector_lr_scale=2.5,\n          output_mult=0.35, query_mult=1, encoder_depth_ratio=0.25,\n          eot_dropout_p=0.5, cps_input=True, cps_bins=32, lr0=0.0015,\n          clip_gradient_norm=0.2, weight_decay=0.1, warmup_steps=4000,\n          random=False))\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nTSARTransformer\n\n TSARTransformer (depth=6, n_head=6, head_width=64, ffn_mult=4,\n                  ttoks_len=200, ttoks_codes=256, ttoks_width=None,\n                  stoks_len=1500, stoks_codes=1024, stoks_width=None,\n                  tunables=Tunables(init_std=1, embeddings_std=0.01,\n                  embeddings_lr_scale=5, embedding_projector_lr_scale=2.5,\n                  output_mult=0.35, query_mult=1,\n                  encoder_depth_ratio=0.25, eot_dropout_p=0.5,\n                  cps_input=True, cps_bins=32, lr0=0.0015,\n                  clip_gradient_norm=0.2, weight_decay=0.1,\n                  warmup_steps=4000, random=False))\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nmake_model\n\n make_model (size:str, frozen_embeddings_model:str=None,\n             tunables:__main__.Tunables=Tunables(init_std=1,\n             embeddings_std=0.01, embeddings_lr_scale=5,\n             embedding_projector_lr_scale=2.5, output_mult=0.35,\n             query_mult=1, encoder_depth_ratio=0.25, eot_dropout_p=0.5,\n             cps_input=True, cps_bins=32, lr0=0.0015,\n             clip_gradient_norm=0.2, weight_decay=0.1, warmup_steps=4000,\n             random=False), dataset:torch.utils.data.dataset.Dataset=None)\n\n\nmodel = make_model('small', frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model', dataset=val_ds)\nmodel.load_checkpoint('../t2s_up_wds_mlang_enclm-ryan_cadetblue-5-61631-acc=0.50.ckpt')\nmodel.save_model('t2s-small-en+pl-ryan_cadetblue.model')\n\n\n\n\nTSARTransformer(\n  (lang_embeddings): Embedding(99, 768)\n  (cps_embeddings): Embedding(32, 768)\n  (encoder): Encoder(\n    (embedding): FlexEmbeddings(\n      (main): Embedding(256, 768)\n    )\n    (layers): ModuleList(\n      (0-5): 6 x ResidualAttentionBlock(\n        (attn): MultiHeadAttention(\n          (query): QueryHead(in_features=768, out_features=768, bias=True)\n          (key): Linear(in_features=768, out_features=768, bias=False)\n          (value): Linear(in_features=768, out_features=768, bias=True)\n          (out): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (mlp_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): Decoder(\n    (embedding): FlexEmbeddings(\n      (main): Embedding(513, 64)\n      (emb_to_hidden): EmbeddingProjector(in_features=64, out_features=768, bias=True)\n      (hidden_to_emb): EmbeddingProjector(in_features=768, out_features=64, bias=True)\n      (special): Embedding(1, 768)\n    )\n    (layers): ModuleList(\n      (0-17): 18 x ResidualAttentionBlock(\n        (attn): MultiHeadAttention(\n          (query): QueryHead(in_features=768, out_features=768, bias=True)\n          (key): Linear(in_features=768, out_features=768, bias=False)\n          (value): Linear(in_features=768, out_features=768, bias=True)\n          (out): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (cross_attn): MultiHeadAttention(\n          (query): QueryHead(in_features=768, out_features=768, bias=True)\n          (key): Linear(in_features=768, out_features=768, bias=False)\n          (value): Linear(in_features=768, out_features=768, bias=True)\n          (out): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=768, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (mlp_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)\n\n\n\n# baseline\ntrain_ds, val_ds = load_datasets('librilight-mlang-t2s/*.tar.gz', 150000, vq_codes=512+1)\nmodel = make_model('tiny', dataset=train_ds, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model', tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0/2, epochs=2,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [2/2 12:45&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n1.81118\n1.68061\n02:07\n\n\n100000\n1.69434\n1.48629\n04:14\n\n\n150016\n1.59371\n1.40083\n06:23\n\n\n200000\n1.51022\n1.34603\n08:29\n\n\n250016\n1.49748\n1.31845\n10:38\n\n\n299968\n1.43387\n1.30509\n12:45\n\n\n\n\n\n    \n      \n      100.00% [4687/4687 06:22&lt;00:00 #149984/150000 loss: 1.434 / 1.305]\n    \n    \n\n\n\n\n\n\n# baseline\n# train_ds, val_ds = load_datasets('librilight-mlang-t2s/*.tar.gz', 150000, vq_codes=512+1)\nmodel = make_model('tiny', dataset=train_ds, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model', tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0/2, epochs=2,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [2/2 10:39&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n1.92771\n2.02235\n01:23\n\n\n100000\n1.43315\n1.58909\n02:48\n\n\n150016\n1.19694\n1.22153\n04:12\n\n\n200000\n1.08593\n1.11246\n05:36\n\n\n250016\n1.08340\n1.04531\n07:00\n\n\n300000\n1.02086\n1.01358\n08:24\n\n\n350016\n1.01914\n0.99932\n09:49\n\n\n379968\n1.07987\n0.99130\n10:39\n\n\n\n\n\n    \n      \n      100.00% [5937/5937 05:20&lt;00:00 #189984/190000 loss: 1.080 / 0.991]\n    \n    \n\n\n\n\n\n\n# baseline + causal encoder\ntrain_ds, val_ds = load_datasets('librilight-mlang-t2s/*.tar.gz', 150000, vq_codes=512+1)\nmodel = make_model('tiny', dataset=train_ds, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model', tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0/2, epochs=2,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [2/2 12:31&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n1.78109\n1.67754\n02:05\n\n\n100000\n1.86647\n1.50500\n04:10\n\n\n150016\n1.66956\n1.40646\n06:15\n\n\n200000\n1.48269\n1.35387\n08:21\n\n\n250016\n1.44419\n1.31720\n10:26\n\n\n299968\n1.47340\n1.30115\n12:31\n\n\n\n\n\n    \n      \n      100.00% [4687/4687 06:15&lt;00:00 #149984/150000 loss: 1.473 / 1.301]\n    \n    \n\n\n\n\n\n\n# baseline + causal encoder + encoder next token prediction\ntrain_ds, val_ds = load_datasets('librilight-mlang-t2s/*.tar.gz', 150000, vq_codes=512+1)\nmodel = make_model('tiny', dataset=train_ds, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model', tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0/2, epochs=2,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [2/2 12:37&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n2.90977\n2.62941\n02:07\n\n\n100000\n2.21458\n2.06109\n04:13\n\n\n150016\n1.66289\n1.63312\n06:19\n\n\n200000\n1.72132\n1.50796\n08:26\n\n\n250016\n1.64924\n1.44136\n10:32\n\n\n299968\n1.70916\n1.41862\n12:37\n\n\n\n\n\n    \n      \n      100.00% [4687/4687 06:18&lt;00:00 #149984/150000 loss: 1.709 / 1.419]\n    \n    \n\n\n\n\n\n\n# baseline + causal encoder + encoder next token prediction\ntrain_ds, val_ds = load_datasets('librilight-mlang-t2s/*.tar.gz', 150000, vq_codes=512+1)\nmodel = make_model('tiny', dataset=train_ds, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model', tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0/2, epochs=2,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [2/2 12:37&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n2.55960\n1.67898\n02:07\n\n\n100000\n2.27045\n1.33314\n04:13\n\n\n150016\n1.86790\n1.03826\n06:19\n\n\n200000\n1.60350\n0.94349\n08:25\n\n\n250016\n1.59656\n0.90378\n10:31\n\n\n299968\n1.81595\n0.88994\n12:37\n\n\n\n\n\n    \n      \n      100.00% [4687/4687 06:18&lt;00:00 #149984/150000 loss: 1.816 / 0.890]\n    \n    \n\n\n\n\n\n\n# baseline + causal encoder + encoder next token prediction\ntrain_ds, val_ds = load_datasets('librilight-mlang-t2s/*.tar.gz', 150000, vq_codes=512+1)\nmodel = make_model('base', dataset=train_ds, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model', tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=8,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [8/8 1:14:59&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n2.30464\n1.56358\n03:20\n\n\n100000\n1.72967\n0.93605\n06:27\n\n\n150016\n1.38447\n0.84605\n09:34\n\n\n200000\n1.45703\n0.80012\n12:40\n\n\n250016\n1.46006\n0.77872\n15:47\n\n\n300000\n1.41666\n0.75976\n18:54\n\n\n350016\n1.39032\n0.74583\n22:01\n\n\n400000\n1.41846\n0.73250\n25:07\n\n\n450016\n1.38481\n0.72326\n28:15\n\n\n500000\n1.43020\n0.72691\n31:21\n\n\n550016\n1.37707\n0.71749\n34:29\n\n\n600000\n1.28683\n0.70754\n37:36\n\n\n650016\n1.33603\n0.70108\n40:43\n\n\n700000\n1.29093\n0.69551\n43:50\n\n\n750016\n1.17670\n0.68443\n46:57\n\n\n800000\n1.24391\n0.68625\n50:04\n\n\n850016\n1.33066\n0.68405\n53:11\n\n\n900000\n1.28942\n0.68458\n56:18\n\n\n950016\n1.29603\n0.67639\n59:26\n\n\n1000000\n1.31182\n0.67060\n1:02:32\n\n\n1050016\n1.28628\n0.66346\n1:05:40\n\n\n1100000\n1.23122\n0.66500\n1:08:47\n\n\n1150016\n1.19196\n0.66517\n1:11:53\n\n\n1199872\n1.26802\n0.66425\n1:15:00\n\n\n\n\n\n    \n      \n      100.00% [4687/4687 09:20&lt;00:00 #149984/150000 loss: 1.268 / 0.664]\n    \n    \n\n\n\n\n\n\n# baseline + causal encoder + encoder next token prediction\n# train_ds, val_ds = load_datasets('librilight-mlang-t2s/*.tar.gz', 150000, vq_codes=512+1)\nmodel = make_model('base', dataset=train_ds, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model', tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=8,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [8/8 1:14:44&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n1.77722\n1.56204\n03:07\n\n\n100000\n1.08727\n0.95343\n06:13\n\n\n150016\n1.03234\n0.85848\n09:20\n\n\n200000\n1.03775\n0.81874\n12:26\n\n\n250016\n1.01491\n0.78222\n15:33\n\n\n300000\n0.98004\n0.76420\n18:40\n\n\n350016\n0.90916\n0.75254\n21:47\n\n\n400000\n0.92307\n0.74475\n24:54\n\n\n450016\n0.91725\n0.73827\n28:01\n\n\n500000\n0.84421\n0.73274\n31:08\n\n\n550016\n0.88127\n0.72657\n34:14\n\n\n600000\n0.93613\n0.71008\n37:22\n\n\n650016\n0.87409\n0.70398\n40:29\n\n\n700000\n0.91990\n0.69744\n43:35\n\n\n750016\n0.86767\n0.69162\n46:42\n\n\n800000\n0.95336\n0.68982\n49:49\n\n\n850016\n0.84717\n0.68376\n52:56\n\n\n900000\n0.91001\n0.68244\n56:03\n\n\n950016\n0.86809\n0.67930\n59:10\n\n\n1000000\n0.87008\n0.67432\n1:02:17\n\n\n1050016\n0.81858\n0.67386\n1:05:24\n\n\n1100000\n0.85216\n0.67201\n1:08:31\n\n\n1150016\n0.91217\n0.66891\n1:11:38\n\n\n1199872\n0.85340\n0.66491\n1:14:44\n\n\n\n\n\n    \n      \n      100.00% [4687/4687 09:20&lt;00:00 #149984/150000 loss: 0.853 / 0.665]\n    \n    \n\n\n\n\n\n\nmodel.save_model('t2s-.1enclm-base.model')"
  },
  {
    "objectID": "2c. whisper quantization (semantic token) evaluation.html",
    "href": "2c. whisper quantization (semantic token) evaluation.html",
    "title": "VQ semantic token extraction evaluation",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nimport io\nimport time\nimport torch\nimport torchaudio\nfrom pathlib import Path\nimport json\nfrom fastprogress import progress_bar, master_bar\nimport fastprogress\nimport numpy as np\nimport pylab as plt\nimport pandas as pd\nimport random\nimport IPython\n\nimport whisper\n\nfrom fastcore.script import *\ndef show_chunk(data, i):\n    row = data.loc[i]\n    snd, sr = torchaudio.load('/data/librilight/'+row['fname'])\n    IPython.display.display(IPython.display.Audio(snd[0,int(row['tstart']*sr):int(row['tend']*sr)], rate=sr))\n    print(row['len'], row['txt'])\ndef test_incremental(model_name, Tmax=15):\n    whmodel = whisper.load_model(model_name)\n    for i in range(Tmax):\n        print(i, whmodel.transcribe(snd[0,:int(i*16000)])['text'])"
  },
  {
    "objectID": "2c. whisper quantization (semantic token) evaluation.html#how-whisper-works-with-speech-cut-at-different-lengths",
    "href": "2c. whisper quantization (semantic token) evaluation.html#how-whisper-works-with-speech-cut-at-different-lengths",
    "title": "VQ semantic token extraction evaluation",
    "section": "How Whisper works with speech cut at different lengths",
    "text": "How Whisper works with speech cut at different lengths\n\ntest_incremental('tiny.en')\n\n0 \n1  Chapter\n2  Chapter 5 of the\n3  Chapter 5 of the things in our garden.\n4  Chapter 5 of the Things in Our Garden by Arthur Rachael.\n5  Chapter 5 of the things in our garden by Arthur Ransom.\n6  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox Recordings.\n7  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox recording is in the public.\n8  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox recording is in the public domain.\n9  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox recording is in the public domain. Chapter 5\n10  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox recording is in the public domain. Chapter 5, their own garden.\n11  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox recording is in the public domain. Chapter 5, Their Own Gardens.\n12  Chapter 5 of the Things in Our Garden by Arthur Ransom. This Libra-Vox recording is in the public domain. Chapter 5, Their Own Gardens.\n13  Chapter 5 of the Things in Our Garden by Arthur Ransom. This Libra-Vox recording is in the public domain. Chapter 5, their own gardens, close by the wood at the\n14  Chapter 5 of the Things in Our Garden by Arthur Ransom. This Libra box recording is in the public domain. Chapter 5, Their Own Gardens, Close by the wood at the bottom of the garden.\n\n\n\ntest_incremental('base.en')\n\n0 \n1  Chapter 4\n2  Chapter 5 of the\n3  Chapter 5 of the Things in our Guard.\n4  Chapter 5 of The Things in Our Garden by Arthur Raffy\n5  Chapter 5 of The Things in Our Garden by Arthur Ransom.\n6  Chapter 5 of The Things in Our Garden by Arthur Ransom.\n7  CHAPTER V.\n8  CHAPTER V.\n9  CHAPTER V.\n10  CHAPTER V.\n11  CHAPTER V.\n12  CHAPTER V. Their Own Gardens.\n13  CHAPTER V. Their Own Gardens.\n14  CHAPTER V.\n\n\n\ntest_incremental('large-v2')\n\n0 \n1  Chapter 4.\n2  Chapter 5 of the\n3  Chapter 5 of The Things in Our Garden\n4  V. THE THINGS IN OUR GARDEN\n5  V. THE THINGS IN OUR GARDEN.\n6  CHAPTER V\n7  V. THE THINGS IN OUR GARDEN\n8  CHAPTER V\n9  CHAPTER V\n10  V. THEIR OWN GARDEN\n11  V. THEIR OWN GARDENS\n12  V. THEIR OWN GARDENS\n13  V. THEIR OWN GARDENS CLOSE BY THE WOOD\n14  V. THEIR OWN GARDENS CLOSE BY THE WOOD AT THE BOTTOM OF THE GARDEN"
  },
  {
    "objectID": "2c. whisper quantization (semantic token) evaluation.html#entropy-of-the-token-stream",
    "href": "2c. whisper quantization (semantic token) evaluation.html#entropy-of-the-token-stream",
    "title": "VQ semantic token extraction evaluation",
    "section": "Entropy of the token stream",
    "text": "Entropy of the token stream\n\nfrom whisperspeech.vq_stoks import RQBottleneckTransformer\n\n\nimport collections\ndef calc_model_entropy(ds, modelfile):\n    vqmodel = RQBottleneckTransformer.load_model(local_filename=modelfile).cuda()\n    cnts = collections.Counter()\n    for snd,txt in ds:\n        stoks = vqmodel.encode_audio(snd.cuda())\n        cnts.update(stoks[0].tolist())\n    pdf = torch.tensor([cnts[i] for i in range(max(cnts)+1)])\n    pdf = pdf / pdf.sum()\n    return -torch.nansum(pdf * np.log2(pdf))\n\n\n# the original semantic token model from early 2023\ncalc_model_entropy(make_test_ds(), None)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n6.097853445304322\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-ce9.2.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n6.357563112144668\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-256c.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n3.0997004132066834\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-256c-cosine.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n5.6921860685011225\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-2d-256c.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n2.899952018598168\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-2d-256c-cosine.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n5.769594466589709\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-2d-256c-cosine-padfix2.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n7.741530540488036\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-2d-512c-cosine-padfix-premlp-learnpos-5e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n8.164144580014993\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-2d-512c-cosine32-padfix-premlp-learnpos-5e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n11.37221612373814\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:08&lt;00:00]\n    \n    \n\n\n11.240560444030649\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-1024c-cosine32-padfix-premlp-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:17&lt;00:00]\n    \n    \n\n\n/tmp/ipykernel_276/107266959.py:11: RuntimeWarning: divide by zero encountered in log2\n  return -torch.nansum(pdf * np.log2(pdf))\n\n\ntensor(9.6971)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:06&lt;00:00]\n    \n    \n\n\n/tmp/ipykernel_276/107266959.py:11: RuntimeWarning: divide by zero encountered in log2\n  return -torch.nansum(pdf * np.log2(pdf))\n\n\ntensor(11.4108)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-cosine32-padfix-premlp-preconv-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:17&lt;00:00]\n    \n    \n\n\n/tmp/ipykernel_103351/107266959.py:11: RuntimeWarning: divide by zero encountered in log2\n  return -torch.nansum(pdf * np.log2(pdf))\n\n\ntensor(9.9410)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:17&lt;00:00]\n    \n    \n\n\n/tmp/ipykernel_9385/107266959.py:11: RuntimeWarning: divide by zero encountered in log2\n  return -torch.nansum(pdf * np.log2(pdf))\n\n\ntensor(11.2880)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-60k.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:20&lt;00:00]\n    \n    \n\n\ntensor(11.4831)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-60k.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:20&lt;00:00]\n    \n    \n\n\ntensor(11.4831)\n\n\n\n# 4096 tokens, we later found out that tokens from this model do carry speaker information\ncalc_model_entropy(make_test_ds(), \"vqmodel-4e-hyptuned-32gpu.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:05&lt;00:00]\n    \n    \n\n\ntensor(11.6404)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vqmodel-256c-4e-hyptuned-32gpu.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:05&lt;00:00]\n    \n    \n\n\ntensor(8.7963)\n\n\n\n# the final model\ncalc_model_entropy(make_test_ds(), \"vqmodel-256c-dim64-4e-hyptuned-32gpu.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:14&lt;00:00]\n    \n    \n\n\ntensor(8.7499)"
  },
  {
    "objectID": "2c. whisper quantization (semantic token) evaluation.html#word-error-rate-measurements",
    "href": "2c. whisper quantization (semantic token) evaluation.html#word-error-rate-measurements",
    "title": "VQ semantic token extraction evaluation",
    "section": "Word Error Rate measurements",
    "text": "Word Error Rate measurements\n\nfrom whisperspeech.wer_metrics import *\n\n\nVanilla Whisper models\n\ndef test_wh_model(whmodel):\n    decoding_options=whisper.DecodingOptions(language='en')\n    stats = WERStats()\n    for snd, gt_text in progress_bar(librispeech_data('/data/LibriSpeech/test-clean'), total=1000):\n        text = whmodel.decode(whisper.log_mel_spectrogram(whisper.pad_or_trim(snd[0])).cuda(), decoding_options).text\n        diff = stats.push_sample(snd, gt_text, text)\n        last_diff = diff.alignments[0][-1]\n        stats.push(hallucination = last_diff.type == 'insert' and last_diff.hyp_end_idx - last_diff.hyp_start_idx &gt; 3)\n    stats = stats.df().sort_values('wer')\n    print(f\"WER: {stats.wer.mean()*100:.2f}%\")\n    print(f\"WER (w/o hallucinations): {stats[~stats['hallucination']].wer.mean()*100:.2f}%\")\n    return stats\n\n\ntest_wh_model(whisper.load_model('tiny.en'))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:05&lt;00:00]\n    \n    \n\n\nWER: 6.91%\nWER (w/o hallucinations): 6.91%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n355\n2.885\nNone\nI'M AFRAID I DON'T KNOW MUCH ABOUT THE LAND OF OZ\nI'm afraid I don't know much about the land of...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n353\n5.870\nNone\nTHE FIRST LOT WE TESTED ON OUR GLASS CAT WHICH...\nThe first lot we tested on our glass cat, whic...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n674\n2.295\nNone\nHE ONLY SHOOK HIS HEAD\nHe only shook his head.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n675\n11.545\nNone\nWELL BUT NOW SAID THE PRINCESS AND SHE FILLED ...\nWell, but now said the princess, and she fille...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n524\n3.195\nNone\nBROTHER MAC ARDLE BROTHER KEOGH\nBrother Maccardo, Brother Keoff.\n0.600000\n0.600000\n0.800000\n0.200000\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHans Stairz-Nied.\n0.666667\n0.666667\n0.888889\n0.111111\nFalse\n\n\n820\n2.155\nNone\nTHE FORMER BOOLOOROO GROANED\nThe former Billie Rook-Round\n0.750000\n0.600000\n0.800000\n0.200000\nFalse\n\n\n918\n3.000\nNone\nTHAT IS TRUE BADAUDERIE\nThat is true bad dealt gree.\n0.750000\n0.500000\n0.625000\n0.375000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCause A was my man's servant.\n1.250000\n0.714286\n0.857143\n0.142857\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_wh_model(whisper.load_model('base.en'))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:41&lt;00:00]\n    \n    \n\n\nWER: 5.08%\nWER (w/o hallucinations): 5.08%\n\n\n\n\n\n\n\n\n\nsecs\ngt_text\ntext\nwer\nhallucination\n\n\n\n\n0\n8.230\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said while on her lap ...\n0.000000\nFalse\n\n\n403\n5.370\nDEPARTING FROM FIVE HUNDRED THOUSAND THROATS T...\nDeparting from 500,000 throats, three cheers b...\n0.000000\nFalse\n\n\n404\n13.140\nTHOUSANDS OF HANDKERCHIEFS WERE WAVING ABOVE T...\nThousands of handkerchiefs were waving above t...\n0.000000\nFalse\n\n\n405\n2.695\nIT'S ALMOST BEYOND CONJECTURE\nIt's almost beyond conjecture.\n0.000000\nFalse\n\n\n406\n7.805\nTHIS REALITY BEGINS TO EXPLAIN THE DARK POWER ...\nThis reality begins to explain the dark power ...\n0.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n524\n3.195\nBROTHER MAC ARDLE BROTHER KEOGH\nBrother McCartill, Brother Kiaff.\n0.600000\nFalse\n\n\n592\n1.805\nHANS STIRS NOT\nHans Sturznide.\n0.666667\nFalse\n\n\n918\n3.000\nTHAT IS TRUE BADAUDERIE\nThat is true, bad girl degree.\n0.750000\nFalse\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCas￩ was my man's servant.\n1.000000\nFalse\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.500000\nFalse\n\n\n\n\n1000 rows ￗ 5 columns\n\n\n\n\ntest_wh_model(whisper.load_model('small.en'))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 02:53&lt;00:00]\n    \n    \n\n\nWER: 3.89%\nWER (w/o hallucinations): 3.84%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n789\n5.945\nNone\nAND THIS PLAN WAS ADOPTED TOO IN ORDER TO EXTR...\nand this plan was adopted too in order to extr...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n461\n10.980\nNone\nSHE MEANWHILE PASSED HER LIFE WITH HER PARENTS...\nShe, meanwhile, passed her life with her paren...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n464\n8.845\nNone\nONE DAY WHEN THE BOY WAS SENT BY HIS GRANDFATH...\nOne day when the boy was sent by his grandfath...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n465\n8.785\nNone\nTHE BED SHE TOO WELL REMEMBERED WAS THERE AND ...\nThe bed she too well remembered was there, and...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n524\n3.195\nNone\nBROTHER MAC ARDLE BROTHER KEOGH\nBrother McCardle. Brother Kiyof.\n0.600000\n0.600000\n0.800000\n0.200000\nFalse\n\n\n288\n1.905\nNone\nI DELIGHT IN YOUR KITCHEN\nby delighting your kitchen.\n0.600000\n0.600000\n0.800000\n0.200000\nFalse\n\n\n121\n15.270\nNone\nAT LAST THE LITTLE MICE STAYED AWAY ALSO AND T...\nAt last the little mice stayed away also, and ...\n0.636364\n0.636364\n0.636364\n0.363636\nFalse\n\n\n918\n3.000\nNone\nTHAT IS TRUE BADAUDERIE\nThat is true Bad Delt Grey.\n0.750000\n0.500000\n0.625000\n0.375000\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedtlos\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_wh_model(whisper.load_model('medium.en'))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 06:22&lt;00:00]\n    \n    \n\n\nWER: 4.19%\nWER (w/o hallucinations): 3.19%\n\n\n\n\n\n\n\n\n\nsecs\ngt_text\ntext\nwer\nhallucination\n\n\n\n\n386\n5.915\nYES WE ARE CERTAINLY I REPLIED EVASIVELY BUT A...\nYes, we are, certainly, I replied evasively, b...\n0.00\nFalse\n\n\n507\n6.480\nHIS CONDUCT AND PRESENCE OF MIND IN THIS EMERG...\nHis conduct and presence of mind in this emerg...\n0.00\nFalse\n\n\n865\n4.315\nTHEIR SUFFERINGS HAVE NEVER YET BEEN FITLY CHR...\nTheir sufferings have never yet been fitly chr...\n0.00\nFalse\n\n\n509\n13.610\nFROM THE SAME MEN NEW REGIMENTS AND NEW COMPAN...\nFrom the same men new regiments and new compan...\n0.00\nFalse\n\n\n511\n12.655\nTHOUGH THE DISCIPLINE OF THE FORMER PARLIAMENT...\nThough the discipline of the former parliament...\n0.00\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n782\n2.260\nTO DAY I SHOUTED\nToday, I shouted.\n0.50\nFalse\n\n\n524\n3.195\nBROTHER MAC ARDLE BROTHER KEOGH\nBrother McCardle, Brother Kiyof.\n0.60\nFalse\n\n\n918\n3.000\nTHAT IS TRUE BADAUDERIE\nThat is true bad health grief.\n0.75\nFalse\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefanos Daedalus\n1.00\nFalse\n\n\n226\n6.750\nHE CONTINUED HIS PRETENDED SEARCH AND TO GIVE ...\nHe continued his pretended search, and to give...\n9.80\nTrue\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\ntest_wh_model(whisper.load_model('large-v2'))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 07:39&lt;00:00]\n    \n    \n\n\nWER: 6.07%\nWER (w/o hallucinations): 3.19%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said while on her lap ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n606\n2.610\nNone\nWE SUFFER STIFLING PAINS\nWe suffer stifling pains.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n607\n7.040\nNone\nSATURDAY AUGUST FIFTEENTH THE SEA UNBROKEN ALL...\nSaturday, August 15th. The sea unbroken all ro...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n608\n3.070\nNone\nTHE HORIZON SEEMS EXTREMELY DISTANT\nThe horizon seems extremely distant.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n609\n9.985\nNone\nALL MY DANGER AND SUFFERINGS WERE NEEDED TO ST...\nAll my danger and sufferings were needed to st...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHans Sturznott\n0.666667\n0.666667\n0.833333\n0.166667\nFalse\n\n\n95\n8.800\nNone\nTHOUGHT THE FIR TREE AND BELIEVED IT ALL BECAU...\nthought the fir tree, and believed it all, bec...\n4.285714\n0.810811\n0.810811\n0.189189\nTrue\n\n\n902\n7.370\nNone\nI HAD A NAME I BELIEVE IN MY YOUNG DAYS BUT I ...\nI had a name, I believe, in my young days, but...\n7.476190\n0.882022\n0.882022\n0.117978\nTrue\n\n\n610\n7.370\nNone\nYOU SEEM ANXIOUS MY UNCLE I SAID SEEING HIM CO...\n\"'You seem anxious, my uncle,' I said, seeing ...\n7.823529\n0.886667\n0.886667\n0.113333\nTrue\n\n\n438\n6.665\nNone\nAS TO HIS AGE AND ALSO THE NAME OF HIS MASTER ...\nAs to his age, and also the name of his master...\n8.631579\n0.896175\n0.896175\n0.103825\nTrue\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\nQuantized Whisper models\n\ndef test_model(modelfile, N=1000):\n    vqmodel = RQBottleneckTransformer.load_model(local_filename=modelfile).cuda()\n    stats = WERStats()\n    for snd, gt_text in progress_bar(librispeech_data('/data/LibriSpeech/test-clean'), total=N):\n        stoks = vqmodel.encode_audio(snd.cuda())\n        text = vqmodel.decode_text(stoks[0])[0].text\n        diff = stats.push_sample(snd, gt_text, text)\n        last_diff = diff.alignments[0][-1]\n        stats.push(hallucination = last_diff.type == 'insert' and last_diff.hyp_end_idx - last_diff.hyp_start_idx &gt; 3)\n    stats = stats.df().sort_values('wer')\n    print(f\"WER: {stats.wer.mean()*100:.2f}%\")\n    print(f\"WER (w/o hallucinations): {stats[~stats['hallucination']].wer.mean()*100:.2f}%\")\n    return stats\n\n\ntest_model(None) # the old stoks model from early 2023\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:10&lt;00:00]\n    \n    \n\n\nWER: 16.06%\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n207\n4.075\nSEVERAL HUNDRED FREE STATE MEN PROMPTLY RESPON...\nseveral hundred free state men promptly respon...\n0.000000\n\n\n209\n5.295\nTHE LEADERS OF THE CONSPIRACY BECAME DISTRUSTF...\nThe leaders of the conspiracy became distrustf...\n0.000000\n\n\n709\n2.440\nTHE THREE MODES OF MANAGEMENT\nThe three modes of management.\n0.000000\n\n\n708\n13.020\nTHE PAIN PRODUCED BY AN ACT OF HASTY AND ANGRY...\nThe pain produced by an act of hasty and angry...\n0.000000\n\n\n705\n5.250\nTHEY ARE CHIEFLY FORMED FROM COMBINATIONS OF T...\nThey are chiefly formed from combinations of t...\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCOSA was my man's servant.\n1.000000\n\n\n144\n4.680\nAND BESIDES SUPPOSE THEE DOES LEARN MEDICINE\nand be sides, supposed to be lost, Lord medicine.\n1.000000\n\n\n907\n4.195\nMADAME QUINSON BESIDES CAN ANSWER YOUR ENQUIRIES\nMadam Gwen-Saun, besides Ken Sir Ian Corrie's.\n1.142857\n\n\n187\n2.230\nNO ITS NOT TOO SOON\nKnow what's sought to assume.\n1.200000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephano's Nerdos.\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\ntest_model('vq-ce9.2.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:12&lt;00:00]\n    \n    \n\n\nWER: 8.80%\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n0\n8.230\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n\n\n283\n1.420\nDIRECTION\ndirection.\n0.000000\n\n\n282\n2.385\nI DIDN'T PREACH WITHOUT DIRECTION\nI didn't preach without direction.\n0.000000\n\n\n624\n3.975\nI SHUDDER AS I RECALL THESE MONSTERS TO MY REM...\nI shudder as I recall these monsters to my rem...\n0.000000\n\n\n279\n10.490\nWE CAN ALL BE SERVANTS OF GOD WHEREVER OUR LOT...\nWe can all be servants of God, wherever our lo...\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n820\n2.155\nTHE FORMER BOOLOOROO GROANED\nthe former Boula Rook round.\n0.750000\n\n\n918\n3.000\nTHAT IS TRUE BADAUDERIE\nThat is true, bad, old-gree.\n0.750000\n\n\n105\n6.555\nIF IT ONLY WERE NOT SO DARK HERE AND SO TERRIB...\nIf... ... ... ... ... ... ... ... ... ... ... ...\n0.916667\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nJose was my man's servant.\n1.000000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefanos de los\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\ntest_model('vq-256c.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:13&lt;00:00]\n    \n    \n\n\nWER: 10.26%\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n789\n5.945\nAND THIS PLAN WAS ADOPTED TOO IN ORDER TO EXTR...\nAnd this plan was adopted too, in order to ext...\n0.000\n\n\n365\n5.780\nI WILL SHOW YOU WHAT A GOOD JOB I DID AND SHE ...\nI will show you what a good job I did. And she...\n0.000\n\n\n722\n10.720\nAS I SPOKE I MADE HIM A GRACIOUS BOW AND I THI...\nAs I spoke, I made him a gracious bow, and I t...\n0.000\n\n\n723\n7.840\nI HAVE COME TO YOUR SHORES MISTER PRESIDENT WI...\nI have come to your shores, Mr. President, wit...\n0.000\n\n\n362\n5.335\nSOMETIMES IT IS CALLED A CRAZY QUILT BECAUSE T...\nSometimes it is called a crazy quilt because t...\n0.000\n\n\n...\n...\n...\n...\n...\n\n\n106\n2.020\nSQUEAK SQUEAK\nSquick. Squick.\n1.000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephanos de Arlos.\n1.000\n\n\n288\n1.905\nI DELIGHT IN YOUR KITCHEN\nI'd like to introduce you in your kitchen.\n1.000\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCall say, was my man servant?\n1.000\n\n\n381\n4.880\nCONSEIL I CALLED A THIRD TIME CONSEIL APPEARED\nCan't say, at call the third time. Can't say a...\n1.125\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\ntest_model('vq-256c-cosine.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:10&lt;00:00]\n    \n    \n\n\nWER: 10.24%\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n710\n11.490\nTO SUPPOSE THAT THE OBJECT OF THIS WORK IS TO ...\nTo suppose that the object of this work is to ...\n0.000000\n\n\n629\n3.235\nTWO HOURS AFTERWARDS A TERRIBLE SHOCK AWOKE ME\nTwo hours afterwards, a terrible shock awoke me.\n0.000000\n\n\n640\n1.740\nPOOR ALICE\nPoor Alice.\n0.000000\n\n\n262\n2.435\nTHAT'S WHAT YOU'D LIKE TO BE DOING IS IT\nThat's what you'd like to be doing, is it?\n0.000000\n\n\n644\n3.105\nAND YESTERDAY THINGS WENT ON JUST AS USUAL\nAnd yesterday, things went on just as usual.\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n187\n2.230\nNO ITS NOT TOO SOON\nNo, it's not just here.\n0.800000\n\n\n115\n4.470\nWHO IS HUMPY DUMPY ASKED THE MICE\nWho is a MP? Don't be. Ask the mice.\n0.857143\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCross say, was my man servant.\n1.000000\n\n\n106\n2.020\nSQUEAK SQUEAK\nquick, quick.\n1.000000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephenos der los.\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\ntest_model('vq-2d-256c.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:11&lt;00:00]\n    \n    \n\n\nWER: 21.75%\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n709\n2.440\nTHE THREE MODES OF MANAGEMENT\nThe Three Modes of Management\n0.000000\n\n\n419\n2.415\nFATHOM SIX FEET\nFathom six feet.\n0.000000\n\n\n703\n4.775\nNATURE OF THE EFFECT PRODUCED BY EARLY IMPRESS...\nnature of the effect produced by early impress...\n0.000000\n\n\n693\n2.110\nI AM VERY GLAD\nI am very glad.\n0.000000\n\n\n686\n2.740\nNO MY LITTLE SON SHE SAID\nNo, my little son, she said.\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n627\n3.060\nTUESDAY AUGUST EIGHTEENTH\n2. Day August 8th\n1.000000\n\n\n820\n2.155\nTHE FORMER BOOLOOROO GROANED\nThe former Bill of Rook around.\n1.000000\n\n\n28\n5.530\nKESWICK MARCH TWENTY SECOND EIGHTEEN THIRTY SE...\nYes, we wish between second 1837. Did you reme...\n1.333333\n\n\n106\n2.020\nSQUEAK SQUEAK\nQuick, quick, quick.\n1.500000\n\n\n792\n1.810\nVENICE\nThen Next\n2.000000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\ntest_model('vq-2d-256c-cosine.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:11&lt;00:00]\n    \n    \n\n\nWER: 11.61%\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n686\n2.740\nNO MY LITTLE SON SHE SAID\nNo, my little son, she said.\n0.000000\n\n\n902\n7.370\nI HAD A NAME I BELIEVE IN MY YOUNG DAYS BUT I ...\nI had a name I believe in my young days, but I...\n0.000000\n\n\n904\n3.300\nYOU DO ME A GREAT HONOUR\nYou do me a great honor.\n0.000000\n\n\n228\n6.775\nAS HE HAD PROMISED TO PROTECT THE HOTEL THE RE...\nAs he had promised to protect the hotel, the r...\n0.000000\n\n\n521\n3.440\nSOON THE WHOLE BRIDGE WAS TREMBLING AND RESOUN...\nSoon the whole bridge was trembling and resoun...\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n918\n3.000\nTHAT IS TRUE BADAUDERIE\nThat is true, bad, old-gree.\n0.750000\n\n\n381\n4.880\nCONSEIL I CALLED A THIRD TIME CONSEIL APPEARED\nConse, at call to third town. Conse, appeared.\n0.750000\n\n\n115\n4.470\nWHO IS HUMPY DUMPY ASKED THE MICE\nWho eats umpi, don't pee? Ask the mice.\n0.857143\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephenau Stairlauce.\n1.000000\n\n\n106\n2.020\nSQUEAK SQUEAK\nSpeak. Speak. Speak.\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n# full crop\ntest_model('vq-2d-256c-cosine-padfix2.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:10&lt;00:00]\n    \n    \n\n\nWER: 16.13%\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n652\n3.475\nI AM SO VERY TIRED OF BEING ALL ALONE HERE\nI'm so very tired of being all alone here.\n0.000000\n\n\n906\n2.610\nAT YOUR SERVICE SIR\nAt your service, sir.\n0.000000\n\n\n904\n3.300\nYOU DO ME A GREAT HONOUR\nYou do me a great honor.\n0.000000\n\n\n902\n7.370\nI HAD A NAME I BELIEVE IN MY YOUNG DAYS BUT I ...\nI had a name I believe in my young days, but I...\n0.000000\n\n\n901\n2.755\nI NEVER HAD ANY FAMILY\nI never had any family.\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n448\n2.215\nWHO TOUCHES ME AM I IN BED\nLook at us, me, our young dad.\n1.000000\n\n\n934\n4.205\nI RESIDE IN THE MARAIS RUE DE DOUZE PORTES\nIrae's eye in the Ma'rae's crew did to support.\n1.111111\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStep 4, Zetelos.\n1.500000\n\n\n16\n1.695\nFAREWELL MADAM\nFair will, damn.\n1.500000\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCos they were my man's servant.\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n# no cropping\ntest_model('vq-2d-256c-cosine-padfix2.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:10&lt;00:00]\n    \n    \n\n\nWER: 11.17%\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n839\n2.275\nTHE CAPTAIN SHOOK HIS HEAD\nThe captain shook his head.\n0.000000\n\n\n408\n9.935\nNEMO BUILDS A FABULOUS FUTURISTIC SUBMARINE TH...\nNemo builds a fabulous futuristic submarine, t...\n0.000000\n\n\n405\n2.695\nIT'S ALMOST BEYOND CONJECTURE\nIt's almost beyond conjecture.\n0.000000\n\n\n404\n13.140\nTHOUSANDS OF HANDKERCHIEFS WERE WAVING ABOVE T...\nThousands of handkerchiefs were waving above t...\n0.000000\n\n\n790\n14.900\nBRIGHTER THAN EARLY DAWN'S MOST BRILLIANT DYE ...\nBrighter than early dawn's most brilliant dye ...\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephenos dellos\n1.000000\n\n\n592\n1.805\nHANS STIRS NOT\nHonsters nod.\n1.000000\n\n\n907\n4.195\nMADAME QUINSON BESIDES CAN ANSWER YOUR ENQUIRIES\nMadam Quinsong, besides Cinanza, you're in que...\n1.000000\n\n\n115\n4.470\nWHO IS HUMPY DUMPY ASKED THE MICE\nPhew, he's on P, don't pee. Ask the mice.\n1.142857\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCos they were my man's servant.\n1.500000\n\n\n\n\n1000 rows ￗ 4 columns\n\n\n\n\n# crop to 200 toks minimum\ntest_model('vq-2d-256c-cosine-padfix2.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:10&lt;00:00]\n    \n    \n\n\nWER: 12.56%\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n871\n2.920\nWHO BEGAN THE QUARREL WAS IT THE MORMONS\nWho began the quarrel? Was it the Mormons?\n0.000000\n\n\n938\n12.435\nHOW STRANGE IT SEEMED TO THE SAD WOMAN AS SHE ...\nHow strange it seemed to the sad woman, as she...\n0.000000\n\n\n937\n12.605\nHIS HOUSEKEEPER HAD THE MANAGEMENT OF EVERYTHI...\nHis housekeeper had the management of everythi...\n0.000000\n\n\n558\n15.720\nIT WAS STRANGE TOO THAT HE FOUND AN ARID PLEAS...\nIt was strange too, that he found an arid plea...\n0.000000\n\n\n305\n3.835\nTHE HEAD OF THE PATCHWORK GIRL WAS THE MOST CU...\nThe head of the patchwork girl was the most cu...\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephenos dellos\n1.000000\n\n\n907\n4.195\nMADAME QUINSON BESIDES CAN ANSWER YOUR ENQUIRIES\nMadam Quinsong, besides Cenanza, you're in que...\n1.000000\n\n\n106\n2.020\nSQUEAK SQUEAK\nQuick, quick.\n1.000000\n\n\n115\n4.470\nWHO IS HUMPY DUMPY ASKED THE MICE\nP-E-S-A-P, don't be... asked the mice.\n1.142857\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCos-A was my man's servant.\n1.250000\n\n\n\n\n1000 rows ￗ 4 columns\n\n\n\n\n# crop to audio length\ntest_model('vq-2d-512c-cosine-padfix-premlp-learnpos.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:09&lt;00:00]\n    \n    \n\n\nWER: 9.89%\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n570\n2.715\nBEWARE OF MAKING THAT MISTAKE\nBeware of making that mistake.\n0.000000\n\n\n260\n3.155\nWHO TAUGHT YOU TO SCRUB A FLOOR I SHOULD LIKE ...\nWho taught you to scrub a floor? I should like...\n0.000000\n\n\n800\n5.770\nOLD DANCES ARE SIMPLIFIED OF THEIR YEARNING BL...\nOld dances are simplified of their yearning, b...\n0.000000\n\n\n258\n2.260\nSPINNING INDEED\nSpinning Indeed.\n0.000000\n\n\n653\n3.815\nAND I DECLARE IT'S TOO BAD THAT IT IS\nAnd I declare it's too bad that it is.\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n934\n4.205\nI RESIDE IN THE MARAIS RUE DE DOUZE PORTES\nIries I'd in the Marfra Grudetus port.\n0.777778\n\n\n115\n4.470\nWHO IS HUMPY DUMPY ASKED THE MICE\nWho is a P-Don't Be? Ask the mice.\n0.857143\n\n\n448\n2.215\nWHO TOUCHES ME AM I IN BED\nPotatys me, and my embed.\n0.857143\n\n\n592\n1.805\nHANS STIRS NOT\nHan Stersnide\n1.000000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefanos de los\n1.500000\n\n\n\n\n1000 rows ￗ 4 columns\n\n\n\n\n# crop to audio length\ntest_model('vq-2d-512c-cosine-padfix-premlp-learnpos-5e.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:09&lt;00:00]\n    \n    \n\n\nWER: 9.51%\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n0\n8.230\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000\n\n\n607\n7.040\nSATURDAY AUGUST FIFTEENTH THE SEA UNBROKEN ALL...\nSaturday, August 15th. The sea unbroken all ro...\n0.000\n\n\n608\n3.070\nTHE HORIZON SEEMS EXTREMELY DISTANT\nThe horizon seems extremely distant.\n0.000\n\n\n615\n3.735\nTHEREFORE DON'T TALK TO ME ABOUT VIEWS AND PRO...\nTherefore, don't talk to me about views and pr...\n0.000\n\n\n616\n5.795\nI TAKE THIS AS MY ANSWER AND I LEAVE THE PROFE...\nI take this as my answer and I leave the profe...\n0.000\n\n\n...\n...\n...\n...\n...\n\n\n157\n3.830\nAND THEE WON'T GO WHY SHOULD I\nAnd, see you all next time!\n0.875\n\n\n381\n4.880\nCONSEIL I CALLED A THIRD TIME CONSEIL APPEARED\nCan say, at call the third time, can say appea...\n0.875\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCaus￩ was my man's servant.\n1.000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefanos dellos.\n1.000\n\n\n106\n2.020\nSQUEAK SQUEAK\nSweet, sweet.\n1.000\n\n\n\n\n1000 rows ￗ 4 columns\n\n\n\n\n# crop to audio length\ntest_model('vq-2d-512c-cosine32-padfix-premlp-learnpos-5e.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:08&lt;00:00]\n    \n    \n\n\nWER: 9.84%\n\n\n\n\n\n\n\n\n\nsecs\ngt_text\ntext\nwer\n\n\n\n\n310\n4.040\nSHE POURED INTO THE DISH A QUANTITY FROM EACH ...\nShe poured into the dish a quantity from each ...\n0.0\n\n\n387\n2.735\nA ROUTE SLIGHTLY LESS DIRECT THAT'S ALL\na route slightly less direct, that's all.\n0.0\n\n\n385\n4.530\nANYHOW WE'LL LEAVE INSTRUCTIONS TO SHIP THE WH...\nAnyhow, we'll leave instructions to ship the w...\n0.0\n\n\n742\n4.730\nWE SAT WITH THE OFFICERS SOME LITTLE TIME AFTE...\nWe sat with the officers some little time afte...\n0.0\n\n\n383\n9.300\nPACK AS MUCH INTO MY TRUNK AS YOU CAN MY TRAVE...\nPack as much into my trunk as you can. My trav...\n0.0\n\n\n...\n...\n...\n...\n...\n\n\n559\n13.895\nTHE SENTENCE OF SAINT JAMES WHICH SAYS THAT HE...\nThank you.\n1.0\n\n\n775\n5.545\nTHE PECULIAR CIRCUMSTANCES OF THE COLONY ARE W...\nThank you.\n1.0\n\n\n106\n2.020\nSQUEAK SQUEAK\nQuick, quick.\n1.0\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephanos de los\n1.0\n\n\n491\n4.805\nTHE PARLIAMENT AND THE SCOTS LAID THEIR PROPOS...\nThank you.\n1.0\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:41&lt;00:00]\n    \n    \n\n\nWER: 7.51%\n\n\n\n\n\n\n\n\n\nsecs\ngt_text\ntext\nwer\n\n\n\n\n862\n6.720\nTO THE FERVENT LATTER DAY SAINT A TEMPLE IS NO...\nTo the fervent Latter-day Saint, a temple is n...\n0.000000\n\n\n436\n6.380\nSHE WAS A LARGE HOMELY WOMAN THEY WERE COMMON ...\nShe was a large, homely woman. They were commo...\n0.000000\n\n\n437\n5.425\nSUBSTANTIALLY THIS WAS JACOB'S UNVARNISHED DES...\nSubstantially, this was Jacob's unvarnished de...\n0.000000\n\n\n438\n6.665\nAS TO HIS AGE AND ALSO THE NAME OF HIS MASTER ...\nAs to his age and also the name of his master,...\n0.000000\n\n\n439\n3.020\nOF STARTING I DIDN'T KNOW THE WAY TO COME\nof starting. I didn't know the way to come.\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n480\n12.510\nTHIS WAS DONE FOR THE EVENT TOOK PLACE AT A TI...\nThis was done for the event took place.\n0.783784\n\n\n713\n17.945\nTHE MOTHER AS SOON AS THE CHAISE IS SO FAR TUR...\nThe Mother. As soon as the chase\n0.869565\n\n\n454\n7.720\nAMONG OTHER THINGS ON WHICH SHE CAST HER EYES ...\nAmong other things...\n0.869565\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefano Staedt-Los\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:33&lt;00:00]\n    \n    \n\n\nWER: 7.49%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n714\n8.010\nNone\nSO YOU WILL BE A GOOD GIRL I KNOW AND NOT MAKE...\nSo you will be a good girl, I know, and not ma...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n365\n5.780\nNone\nI WILL SHOW YOU WHAT A GOOD JOB I DID AND SHE ...\nI will show you what a good job I did, and she...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n608\n3.070\nNone\nTHE HORIZON SEEMS EXTREMELY DISTANT\nThe horizon seems extremely distant.\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n362\n5.335\nNone\nSOMETIMES IT IS CALLED A CRAZY QUILT BECAUSE T...\nSometimes it is called a crazy quilt because t...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n361\n6.045\nNone\nA BED QUILT MADE OF PATCHES OF DIFFERENT KINDS...\nA bed quilt made of patches of different kinds...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n480\n12.510\nNone\nTHIS WAS DONE FOR THE EVENT TOOK PLACE AT A TI...\nThis was done for the event took place.\n0.783784\n0.783784\n0.783784\n0.216216\n\n\n454\n7.720\nNone\nAMONG OTHER THINGS ON WHICH SHE CAST HER EYES ...\nAmong other things...\n0.869565\n0.869565\n0.869565\n0.130435\n\n\n713\n17.945\nNone\nTHE MOTHER AS SOON AS THE CHAISE IS SO FAR TUR...\nThe Mother. As soon as the chase\n0.869565\n0.869565\n0.888199\n0.111801\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Los\n1.500000\n1.000000\n1.000000\n0.000000\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\ntest_model(\"vq-base.en-2d-1024c-cosine32-padfix-premlp-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:31&lt;00:00]\n    \n    \n\n\nWER: 10.44%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n669\n8.540\nNone\nAT THE FARTHER END OF THE LARGEST HALL A TABLE...\nAt the farther end of the largest hall, a tabl...\n0.0\n0.0\n0.0\n1.0\n\n\n349\n2.130\nNone\nTHE WOMAN SEEMED THOUGHTFUL\nThe woman seemed thoughtful.\n0.0\n0.0\n0.0\n1.0\n\n\n572\n4.090\nNone\nHE IS CALLED AS YOU KNOW THE APOSTLE OF THE IN...\nHe is called, as you know, the apostle of the ...\n0.0\n0.0\n0.0\n1.0\n\n\n347\n3.665\nNone\nOJO HAD NEVER EATEN SUCH A FINE MEAL IN ALL HI...\nOjo had never eaten such a fine meal in all hi...\n0.0\n0.0\n0.0\n1.0\n\n\n346\n3.705\nNone\nAND YOU MUST BE OJO THE UNLUCKY SHE ADDED\nAnd you must be Ojo the unlucky,\" she added.\n0.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n896\n18.540\nNone\nSILVIA WAS THE ADORATION OF FRANCE AND HER TAL...\nSylvia\n1.0\n1.0\n1.0\n0.0\n\n\n689\n5.995\nNone\nDELLA HAD A YOUNG SISTER NAMED MARIA AND A COU...\nDela.\n1.0\n1.0\n1.0\n0.0\n\n\n512\n27.525\nNone\nVALOR INDEED WAS VERY GENERALLY DIFFUSED OVER ...\nVala.\n1.0\n1.0\n1.0\n0.0\n\n\n897\n23.740\nNone\nSILVIA DID NOT THINK THAT HER GOOD CONDUCT WAS...\nSylvia.\n1.0\n1.0\n1.0\n0.0\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.5\n1.0\n1.0\n0.0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:45&lt;00:00]\n    \n    \n\n\nWER: 6.64%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n377\n3.910\nNone\nHE WENT HERE THERE AND EVERYWHERE IN PERFECT C...\nHe went here, there, and everywhere in perfect...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n376\n8.340\nNone\nNEVER DID HE OBJECT TO BUCKLING UP HIS SUITCAS...\nNever did he object to buckling up his suitcas...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n687\n8.500\nNone\nIF YOU DRESSED IN SILK AND GOLD FROM TOP TO TO...\nIf you dressed in silk and gold from top to to...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n688\n11.125\nNone\nTO SUCH PERSONS THESE INDIRECT MODES OF TRAINI...\nTo such persons, these indirect modes of train...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n918\n3.000\nNone\nTHAT IS TRUE BADAUDERIE\nThat is true bad-dulch-gree.\n0.750000\n0.500000\n0.625000\n0.375000\n\n\n46\n25.640\nNone\nA GOOD NEIGHBOUR OF THE BRONTES A CLEVER INTEL...\nA good neighbor of the Bronte's, a clever, int...\n0.797101\n0.785714\n0.836957\n0.163043\n\n\n221\n15.060\nNone\nIN THE SHOOTING OF SHERIFF JONES IN LAWRENCE A...\nIn the shooting of Sheriff's\n0.857143\n0.857143\n0.880952\n0.119048\n\n\n879\n17.840\nNone\nTHEY KNEW NO NORTH NO SOUTH NO EAST NO WEST TH...\nThey knew no North.\n0.925926\n0.925926\n0.925926\n0.074074\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedalus.\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n_9.plot.scatter('secs', 'wer', alpha=.2)\n\n&lt;Axes: xlabel='secs', ylabel='wer'&gt;\n\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-preconv-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:36&lt;00:00]\n    \n    \n\n\nWER: 6.34%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n696\n18.415\nNone\nFOR INSTANCE ONE DAY THE CHILDREN HAD BEEN PLA...\nFor instance, one day the children had been pl...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n370\n2.340\nNone\nBUT NOW NOTHING COULD HOLD ME BACK\nBut now nothing could hold me back.\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n369\n9.340\nNone\nI WANTED NOTHING MORE THAN TO SEE MY COUNTRY A...\nI wanted nothing more than to see my country a...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n368\n6.190\nNone\nEVEN SO I HAD JUST RETURNED FROM AN ARDUOUS JO...\nEven so, I had just returned from an arduous j...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n820\n2.155\nNone\nTHE FORMER BOOLOOROO GROANED\nthe former Boula-Ri-Growned.\n0.750000\n0.600000\n0.800000\n0.200000\n\n\n843\n2.110\nNone\nFINE GLORIOUS\nFind. Chlorious.\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHon Stir's Night.\n1.333333\n1.000000\n1.000000\n0.000000\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.500000\n1.000000\n1.000000\n0.000000\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-repro.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:30&lt;00:00]\n    \n    \n\n\nWER: 10.00%\nWER (w/o hallucinations): 10.00%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n696\n18.415\nNone\nFOR INSTANCE ONE DAY THE CHILDREN HAD BEEN PLA...\nFor instance, one day the children had been pl...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n594\n4.865\nNone\nI REFER TO THE THERMOMETER IT INDICATES THE FI...\nI refer to the thermometer, it indicates the f...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n738\n8.105\nNone\nTHEN THERE WERE THREE OR FOUR LEADING MEN OF T...\nThen there were three or four leading men of t...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n355\n2.885\nNone\nI'M AFRAID I DON'T KNOW MUCH ABOUT THE LAND OF OZ\nI'm afraid I don't know much about the land of...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n354\n9.840\nNone\nI THINK THE NEXT GLASS CAT THE MAGICIAN MAKES ...\nI think the next glass cat the magician makes ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n22.095\nNone\nTHIS MEANT THAT FOR AN ALLEGED MISDEMEANOR FOR...\nThis is the end of the video.\n0.949153\n0.949153\n0.978208\n0.021792\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n85\n2.610\nNone\nTHIS EVENING THEY ALL SAID\nThis is the end of the video.\n1.200000\n0.857143\n0.971429\n0.028571\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n418\n17.640\nNone\nFOR MANY THEN THIS BOOK HAS BEEN A SOURCE OF F...\nFor many then, this is the end of the video. F...\n2.923077\n0.942149\n0.989616\n0.010384\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-repro.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:28&lt;00:00]\n    \n    \n\n\nWER: 7.82%\nWER (w/o hallucinations): 7.82%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n709\n2.440\nNone\nTHE THREE MODES OF MANAGEMENT\nthe three modes of management.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n740\n2.715\nNone\nBUT I MEAN TO HAVE MY INNINGS BEFORE LONG\nBut I mean to have my innings before long.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n362\n5.335\nNone\nSOMETIMES IT IS CALLED A CRAZY QUILT BECAUSE T...\nSometimes it is called a crazy quilt because t...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n361\n6.045\nNone\nA BED QUILT MADE OF PATCHES OF DIFFERENT KINDS...\nA bed quilt made of patches of different kinds...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n605\n6.305\nNone\nA SUFFOCATING SMELL OF NITROGEN FILLS THE AIR ...\nA suffocating smell of nitrogen fills the air....\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n793\n14.580\nNone\nIN A SUNSET GLOWING OF CRIMSON AND GOLD SHE LI...\nIn a sunset\n0.906250\n0.906250\n0.906250\n0.093750\nFalse\n\n\n170\n8.740\nNone\nRUTH WAS GLAD TO HEAR THAT PHILIP HAD MADE A P...\nRuth was\n0.931034\n0.931034\n0.931034\n0.068966\nFalse\n\n\n818\n9.870\nNone\nI'LL GLADLY DO THAT PROMISED THE NEW BOOLOOROO...\nI'll\n0.933333\n0.933333\n0.933333\n0.066667\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCosse was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-repro-warm1000.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:36&lt;00:00]\n    \n    \n\n\nWER: 7.23%\nWER (w/o hallucinations): 7.23%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n760\n6.370\nNone\nTHERE CAME UPON ME A SUDDEN SHOCK WHEN I HEARD...\nThere came upon me a sudden shock when I heard...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n368\n6.190\nNone\nEVEN SO I HAD JUST RETURNED FROM AN ARDUOUS JO...\nEven so, I had just returned from an arduous j...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n586\n5.515\nNone\nTHERE'S A HEAVY STORM COMING ON I CRIED POINTI...\nThere's a heavy storm coming on, I cried, poin...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n366\n3.615\nNone\nCHAPTER THREE AS MASTER WISHES\nChapter 3 As Master Wishes\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n365\n5.780\nNone\nI WILL SHOW YOU WHAT A GOOD JOB I DID AND SHE ...\nI will show you what a good job I did, and she...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n694\n5.965\nNone\nI EXPECT YOU HAVE BEEN A VERY GOOD GIRL ANDELL...\nI\n0.933333\n0.933333\n0.933333\n0.066667\nFalse\n\n\n881\n13.950\nNone\nWE BELIEVE IN A LITERAL RESURRECTION AND AN AC...\nWe believe that we are the most important ones.\n0.944444\n0.944444\n0.987654\n0.012346\nFalse\n\n\n106\n2.020\nNone\nSQUEAK SQUEAK\nQuick, quick!\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano's dead loss.\n2.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-repro-warm1000-2.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:35&lt;00:00]\n    \n    \n\n\nWER: 6.47%\nWER (w/o hallucinations): 6.47%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n702\n14.175\nNone\nAND THIS METHOD OF TREATING THE CASE WAS MUCH ...\nAnd this method of treating the case was much ...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n703\n4.775\nNone\nNATURE OF THE EFFECT PRODUCED BY EARLY IMPRESS...\nNature of the Effect produced by Early Impress...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n377\n3.910\nNone\nHE WENT HERE THERE AND EVERYWHERE IN PERFECT C...\nHe went here, there, and everywhere in perfect...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n376\n8.340\nNone\nNEVER DID HE OBJECT TO BUCKLING UP HIS SUITCAS...\nNever did he object to buckling up his suitcas...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossé was my man's servant.\n1.0\n0.666667\n0.833333\n0.166667\nFalse\n\n\n322\n3.200\nNone\nI NOW USE THEM AS ORNAMENTAL STATUARY IN MY GA...\nand\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n652\n3.475\nNone\nI AM SO VERY TIRED OF BEING ALL ALONE HERE\nand\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n555\n5.815\nNone\nBUT THE DUSK DEEPENING IN THE SCHOOLROOM COVER...\nand\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStaphano's dead loss.\n2.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:33&lt;00:00]\n    \n    \n\n\nWER: 5.93%\nWER (w/o hallucinations): 5.93%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n781\n3.050\nNone\nWHEN DO YOU INTEND THAT THE JOHN BRIGHT SHALL ...\nWhen do you intend that the John Bright shall ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n388\n2.355\nNone\nWE'RE LEAVING ON THE ABRAHAM LINCOLN\nWe're leaving on the Abraham Lincoln.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n387\n2.735\nNone\nA ROUTE SLIGHTLY LESS DIRECT THAT'S ALL\na route slightly less direct. That's all.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n386\n5.915\nNone\nYES WE ARE CERTAINLY I REPLIED EVASIVELY BUT A...\nYes, we are. Certainly, I replied evasively, b...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n385\n4.530\nNone\nANYHOW WE'LL LEAVE INSTRUCTIONS TO SHIP THE WH...\nAnyhow, we'll leave instructions to ship the w...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n524\n3.195\nNone\nBROTHER MAC ARDLE BROTHER KEOGH\nBrother McCarle, Brother Kioff.\n0.600000\n0.600000\n0.800000\n0.200000\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHans-Stirrsnacht.\n0.666667\n0.666667\n0.833333\n0.166667\nFalse\n\n\n766\n2.540\nNone\nYOU PROPOSE TO KIDNAP ME I SAID\nYou proposed a kenatmi set.\n0.857143\n0.857143\n0.971429\n0.028571\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nSteffano Staedalus\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\nax = _14.plot.scatter('secs', 'wer', alpha=.2)\nax.set_ylim(0, 1.5)\n\n(0.0, 1.5)\n\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-60k.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:30&lt;00:00]\n    \n    \n\n\nWER: 9.34%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n646\n3.385\nNone\nI ALMOST THINK I CAN REMEMBER FEELING A LITTLE...\nI almost think I can remember feeling a little...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n862\n6.720\nNone\nTO THE FERVENT LATTER DAY SAINT A TEMPLE IS NO...\nTo the fervent Latter-day Saint, a temple is n...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n370\n2.340\nNone\nBUT NOW NOTHING COULD HOLD ME BACK\nBut now nothing could hold me back.\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n369\n9.340\nNone\nI WANTED NOTHING MORE THAN TO SEE MY COUNTRY A...\nI wanted nothing more than to see my country a...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n368\n6.190\nNone\nEVEN SO I HAD JUST RETURNED FROM AN ARDUOUS JO...\nEven so, I had just returned from an arduous j...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n61\n10.250\nNone\nIN WINTER WHEN THE SNOW LAY GLITTERING ON THE ...\nIn winter, when the snow lay glittering on the...\n1.791667\n0.651515\n0.666035\n0.333965\n\n\n468\n12.250\nNone\nI HAVE GREAT THINGS TO TELL YOU SENOR SAID DON...\nI have great things to tell you, Senor, sadona...\n1.861111\n0.676768\n0.712682\n0.287318\n\n\n558\n15.720\nNone\nIT WAS STRANGE TOO THAT HE FOUND AN ARID PLEAS...\nIt was strange, too, that he found an arid ple...\n2.317073\n0.698529\n0.698529\n0.301471\n\n\n770\n13.960\nNone\nWHAT WORLD WIDE INIQUITY SUCH A SPEECH AS THAT...\nWhat worldwide iniquity such a speech as that ...\n2.375000\n0.719697\n0.738740\n0.261260\n\n\n444\n12.475\nNone\nTHEY DREW THEIR SWORDS HID THEIR FACES IN THE ...\nThey drew their swords, hid their faces in the...\n4.200000\n0.807692\n0.807692\n0.192308\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-eqvad.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:38&lt;00:00]\n    \n    \n\n\nWER: 7.47%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n673\n12.130\nNone\nTHE PRINCESS CERTAINLY WAS BEAUTIFUL AND HE WO...\nThe princess certainly was beautiful, and he w...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n674\n2.295\nNone\nHE ONLY SHOOK HIS HEAD\nHe only shook his head.\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n355\n2.885\nNone\nI'M AFRAID I DON'T KNOW MUCH ABOUT THE LAND OF OZ\nI'm afraid I don't know much about the land of...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n353\n5.870\nNone\nTHE FIRST LOT WE TESTED ON OUR GLASS CAT WHICH...\nThe first lot we tested on our glass cat, whic...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Stettelos.\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHonsters, nod.\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n146\n3.260\nNone\nWHERE THEE AND THY FAMILY ARE KNOWN\nWhere's D and I-F where's D and I-F are known?\n1.428571\n0.714286\n0.836735\n0.163265\n\n\n996\n19.915\nNone\nEDISON HAD INSTALLED HIS HISTORIC FIRST GREAT ...\nEdison had installed his historic first-grade ...\n3.208333\n0.766169\n0.771041\n0.228959\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\nax = _8.plot.scatter('secs', 'wer', alpha=.2)\nax.set_ylim(0, 1.5)\n\n(0.0, 1.5)\n\n\n\n\n\n\nax = _15['secs'].hist()\nax.set_yscale('log')\n\n\n\n\n\nplt.plot(_15['secs'], 1/_15['gt_text'].str.split('\\w+').str.len(), '.')\n\n\n\n\n\n# the reproducibility got pretty low ;)\nfor i in range(4):\n    print(i)\n    test_model(f\"test-run-{i}.model\")\n    print()\n\n0\nWER: 6.37%\nWER (w/o hallucinations): 6.37%\n\n1\nWER: 10.69%\nWER (w/o hallucinations): 9.89%\n\n2\nWER: 12.34%\nWER (w/o hallucinations): 11.79%\n\n3\nWER: 15.83%\nWER (w/o hallucinations): 15.30%\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:33&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:28&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:34&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:31&lt;00:00]\n    \n    \n\n\n\ntest_model(\"test-run-warm1000.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:31&lt;00:00]\n    \n    \n\n\nWER: 8.81%\nWER (w/o hallucinations): 8.81%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n368\n6.190\nNone\nEVEN SO I HAD JUST RETURNED FROM AN ARDUOUS JO...\nEven so, I had just returned from an arduous j...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n691\n4.985\nNone\nTO GIVE AN IDEA OF THESE CONVERSATIONS I WILL ...\nTo give an idea of these conversations, I will...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n366\n3.615\nNone\nCHAPTER THREE AS MASTER WISHES\nChapter 3 As Master Wishes\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n365\n5.780\nNone\nI WILL SHOW YOU WHAT A GOOD JOB I DID AND SHE ...\nI will show you what a good job I did, and she...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n792\n1.810\nNone\nVENICE\nVINIS.\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n324\n2.700\nNone\nASKED THE VOICE IN SCORNFUL ACCENTS\nAsk the voice in the voice in the voice in the...\n1.500000\n0.750000\n0.875000\n0.125000\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano's dead loss.\n2.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n26\n16.735\nNone\nP S PRAY SIR EXCUSE ME FOR WRITING TO YOU A SE...\nP-S-P-S-P-S-P-S-P-S-P-S-P-S-P-S-P-S-P-S-P-S-P-...\n2.037037\n0.982143\n0.999339\n0.000661\nFalse\n\n\n106\n2.020\nNone\nSQUEAK SQUEAK\nIn the past, we have a question.\n3.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_model(\"test-run-1e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:34&lt;00:00]\n    \n    \n\n\nWER: 8.41%\nWER (w/o hallucinations): 8.05%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n655\n4.895\nNone\nI SHALL BE PUNISHED FOR IT NOW I SUPPOSE BY BE...\nI shall be punished for it now, I suppose, by ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n657\n3.640\nNone\nI AM VERY TIRED OF SWIMMING ABOUT HERE O MOUSE\nI am very tired of swimming about here, oh mouse.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n318\n5.115\nNone\nMOST PEOPLE TALK TOO MUCH SO IT IS A RELIEF TO...\nMost people talk too much, so it is a relief t...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n317\n7.920\nNone\nHE SELECTED A SMALL GOLD BOTTLE WITH A PEPPER ...\nHe selected a small gold bottle with a pepper ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n549\n10.575\nNone\nAT MOST BY AN ALMS GIVEN TO A BEGGAR WHOSE BLE...\nAt most, by an alms given to a beggar whose bl...\n1.000000\n0.500000\n0.500000\n0.500000\nTrue\n\n\n399\n6.365\nNone\nI WAS WELL SATISFIED WITH MY CABIN WHICH WAS L...\nI was well satisfied with my cabin, which was ...\n1.052632\n0.540541\n0.588905\n0.411095\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nSteffinor's Daedalus.\n1.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n659\n4.995\nNone\nWE WON'T TALK ABOUT HER ANY MORE IF YOU'D RATH...\nWe won't talk about her anymore if he'd rather...\n1.866667\n0.700000\n0.760000\n0.240000\nTrue\n\n\n95\n8.800\nNone\nTHOUGHT THE FIR TREE AND BELIEVED IT ALL BECAU...\nthought the fur tree, and believed it all, bec...\n4.619048\n0.829060\n0.837200\n0.162800\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n# but it got better after some hyperparam tuning\ntest_model(\"vqmodel-4e-6454-hyptuned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:30&lt;00:00]\n    \n    \n\n\nWER: 7.71%\nWER (w/o hallucinations): 7.71%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n403\n5.370\nNone\nDEPARTING FROM FIVE HUNDRED THOUSAND THROATS T...\nDeparting from 500,000 throats, three cheers b...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n922\n4.400\nNone\nBUT HOW DID SHE MANAGE TO RENDER IT SO FASHION...\nBut how did she manage to render it so fashion...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n629\n3.235\nNone\nTWO HOURS AFTERWARDS A TERRIBLE SHOCK AWOKE ME\nTwo hours afterwards, a terrible shock, awoke me.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n355\n2.885\nNone\nI'M AFRAID I DON'T KNOW MUCH ABOUT THE LAND OF OZ\nI'm afraid I don't know much about the land of...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n353\n5.870\nNone\nTHE FIRST LOT WE TESTED ON OUR GLASS CAT WHICH...\nThe first lot we tested on our glass cat, whic...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n849\n3.560\nNone\nI HAD A NOTION IT WAS YOU MATE AS SAVED ME FRO...\nI'll have a note.\n0.928571\n0.866667\n0.942857\n0.057143\nFalse\n\n\n741\n16.360\nNone\nOF WHAT MISSUS NEVERBEND HAD GONE THROUGH IN P...\nOf what Mrs. N N N N N N N N N N N N N N N N N...\n0.936170\n0.936170\n0.992021\n0.007979\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCasa was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Stetelos.\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHon Stur's Night.\n1.333333\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_model(\"vqmodel-4e-6454-hyptuned-small.en.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:41&lt;00:00]\n    \n    \n\n\nWER: 7.38%\nWER (w/o hallucinations): 7.38%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.00000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n350\n10.680\nNone\nAT ONE END STOOD A GREAT FIREPLACE IN WHICH A ...\nAt one end stood a great fireplace, in which a...\n0.00000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n349\n2.130\nNone\nTHE WOMAN SEEMED THOUGHTFUL\nThe woman seemed thoughtful.\n0.00000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n680\n6.450\nNone\nHE DARTED LIKE AN ARROW THROUGH ALL THE HALLS ...\nHe darted like an arrow through all the halls,...\n0.00000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n347\n3.665\nNone\nOJO HAD NEVER EATEN SUCH A FINE MEAL IN ALL HI...\nOjo had never eaten such a fine meal in all hi...\n0.00000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHonsters, Nod.\n1.00000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n792\n1.810\nNone\nVENICE\nVINUS.\n1.00000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.00000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStephenos dead loss.\n1.50000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n440\n15.770\nNone\nELEVEN O'CLOCK HAD STRUCK IT WAS A FINE CLEAR ...\nAt the time of the day, the morning of the day...\n4.12766\n0.960396\n0.993259\n0.006741\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_model(\"vqmodel-4e-hyptuned-16gpu.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:32&lt;00:00]\n    \n    \n\n\nWER: 6.01%\nWER (w/o hallucinations): 6.01%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n390\n1.975\nNone\nWE DON'T KNOW WHERE IT WILL TAKE US\nWe don't know where it will take us.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n708\n13.020\nNone\nTHE PAIN PRODUCED BY AN ACT OF HASTY AND ANGRY...\nThe pain produced by an act of hasty and angry...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n388\n2.355\nNone\nWE'RE LEAVING ON THE ABRAHAM LINCOLN\nWe're leaving on the Abraham Lincoln.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n387\n2.735\nNone\nA ROUTE SLIGHTLY LESS DIRECT THAT'S ALL\nA route slightly less direct, that's all.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n918\n3.000\nNone\nTHAT IS TRUE BADAUDERIE\nThat is true, bad old gree.\n0.750000\n0.500000\n0.625000\n0.375000\nFalse\n\n\n809\n8.875\nNone\nWHEN THE BLUESKINS SAW GHIP GHISIZZLE THEY RAI...\nThanks for watching!\n0.961538\n0.961538\n0.987179\n0.012821\nFalse\n\n\n643\n12.020\nNone\nALICE TOOK UP THE FAN AND GLOVES AND AS THE HA...\nThank you.\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCosse was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefanos de los\n1.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_model(\"vqmodel-4e-hyptuned-32gpu.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:32&lt;00:00]\n    \n    \n\n\nWER: 5.94%\nWER (w/o hallucinations): 5.94%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n757\n10.030\nNone\nTHEREFORE I FEEL MYSELF QUITE ABLE AS PRESIDEN...\nTherefore, I feel myself quite able, as Presid...\n0.00\n0.000000\n0.000000\n1.000000\nFalse\n\n\n628\n2.550\nNone\nDURING HIS WATCH I SLEPT\nDuring his watch, I slept.\n0.00\n0.000000\n0.000000\n1.000000\nFalse\n\n\n756\n4.735\nNone\nYOU HAVE COME TO US THREATENING US WITH ABSOLU...\nYou have come to us threatening us with absolu...\n0.00\n0.000000\n0.000000\n1.000000\nFalse\n\n\n377\n3.910\nNone\nHE WENT HERE THERE AND EVERYWHERE IN PERFECT C...\nHe went here, there, and everywhere in perfect...\n0.00\n0.000000\n0.000000\n1.000000\nFalse\n\n\n376\n8.340\nNone\nNEVER DID HE OBJECT TO BUCKLING UP HIS SUITCAS...\nNever did he object to buckling up his suitcas...\n0.00\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n918\n3.000\nNone\nTHAT IS TRUE BADAUDERIE\nThat is true bad-delt gree.\n0.75\n0.500000\n0.625000\n0.375000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.00\n0.666667\n0.833333\n0.166667\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHonsters Nied.\n1.00\n1.000000\n1.000000\n0.000000\nFalse\n\n\n819\n5.775\nNone\nSCUSE ME SAID TROT I NEGLECTED TO TELL YOU THA...\nThanks for watching.\n1.00\n1.000000\n1.000000\n0.000000\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.50\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_model(\"vqmodel-512c-4e-hyptuned-32gpu.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 08:52&lt;00:00]\n    \n    \n\n\nWER: 7.37%\nWER (w/o hallucinations): 7.37%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n715\n11.340\nNone\nTHE MOTHER IN MANAGING THE CASE IN THIS WAY RE...\nThe mother, in managing the case in this way, ...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n347\n3.665\nNone\nOJO HAD NEVER EATEN SUCH A FINE MEAL IN ALL HI...\nOjo had never eaten such a fine meal in all hi...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n860\n10.555\nNone\nIT IS NOTABLE THAT THE INDIAN TRIBES HAVE GENE...\nIt is notable that the Indian tribes have gene...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n608\n3.070\nNone\nTHE HORIZON SEEMS EXTREMELY DISTANT\nThe horizon seems extremely distant.\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n344\n4.275\nNone\nI AM MY DEAR AND ALL STRANGERS ARE WELCOME TO ...\nI am, my dear, and all strangers are welcome t...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCosay was my man's servant.\n1.0\n0.666667\n0.833333\n0.166667\nFalse\n\n\n260\n3.155\nNone\nWHO TAUGHT YOU TO SCRUB A FLOOR I SHOULD LIKE ...\n.\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHonster's Night.\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n792\n1.810\nNone\nVENICE\nVenus.\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefanos de los.\n1.5\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\ntest_model(\"vqmodel-512c-dim64-4e-hyptuned-32gpu.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:29&lt;00:00]\n    \n    \n\n\nWER: 7.13%\nWER (w/o hallucinations): 7.13%\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n680\n6.450\nNone\nHE DARTED LIKE AN ARROW THROUGH ALL THE HALLS ...\nHe darted like an arrow through all the halls,...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n682\n5.145\nNone\nAND ALL HIS BROTHERS AND SISTERS STOOD ROUND A...\nand all his brothers and sisters stood round a...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n684\n2.165\nNone\nANDERS FACE GREW RED\nAnders face grew red.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n685\n2.775\nNone\nBUT HIS MOTHER HUGGED HIM CLOSE\nBut his mother hugged him close.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n106\n2.020\nNone\nSQUEAK SQUEAK\nSpeak, speak.\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHonsters, Nied.\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n336\n4.835\nNone\nFOR A LONG TIME HE HAD WISHED TO EXPLORE THE B...\nFor a long time, you can see that the video is...\n1.333333\n0.800000\n0.933333\n0.066667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns"
  },
  {
    "objectID": "1b. voice activity detection.html",
    "href": "1b. voice activity detection.html",
    "title": "Perform Voice Activity Detection (VAD)",
    "section": "",
    "text": "from IPython.display import HTML\nimport pylab as plt\nWe use the voice activity detection model from WhisperX (but we don’t use their merging algorithm):\nTest just a few files:\nds = wds.WebDataset('/data2/libritts-r-raw-000000.tar').compose(wds.decode(wds.torch_audio))\nfor x in ds: break\nx\n\n{'__key__': './dev-clean/1272/128104/1272_128104_000001_000000',\n '__url__': '/data2/libritts-r-raw-000000.tar',\n 'normalized.txt': \"A 'JOLLY' ART CRITIC\",\n 'original.txt': \"A 'JOLLY' ART CRITIC\",\n 'wav': (tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0036, -0.0038, -0.0050]]),\n  24000)}\n# test it locally\ninput:str  = 'https://huggingface.co/datasets/collabora/librilight-webdataset/resolve/main/librilight-large-wo6454-flac-000002.tar'\noutput:str = input.rsplit(\"/\", 1)[1].replace('flac', 'vad') + \".gz\"\n\nds = load_dataset(input)\nvad_model = whisperx.vad.load_vad_model('cuda')\n\nwith wds.TarWriter(output) as sink:\n    for s in progress_bar(ds, total=10):\n        audio, sr = s['flac']\n        assert(sr == 16000)\n        sink.write({\n            \"__key__\": s['__key__'],\n            \"vad.npy\": np.array(segment_audio(vad_model, audio), dtype=np.float16)\n        })\n        \n!ls -lh {output}\n!tar tf {output}\n\nLightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../.cache/torch/whisperx-vad-segmentation.bin`\n\n\nModel was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n-rw-r--r-- 1 root root 7.5K Sep 21 08:51 librilight-large-wo6454-vad-000002.tar.gz\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_03_molesworth_64kb.vad.npy\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_04_molesworth_64kb.vad.npy\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_08_molesworth_64kb.vad.npy\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_09_molesworth_64kb.vad.npy\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_10_molesworth_64kb.vad.npy\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_11_molesworth_64kb.vad.npy\nlarge/10089/goodcheerstories_1511_librivox_64kb_mp3/goodcheerstories_13_dickinson_64kb.vad.npy\nlarge/10089/goodcheerstories_1511_librivox_64kb_mp3/goodcheerstories_30_dickinson_64kb.vad.npy\nlarge/10089/mothers_nursery_tales_1512_librivox_64kb_mp3/mothers_nursery_tales_16_pyle_64kb.vad.npy\nlarge/10089/mothers_nursery_tales_1512_librivox_64kb_mp3/mothers_nursery_tales_25_pyle_64kb.vad.npy\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:10&lt;00:00]"
  },
  {
    "objectID": "1b. voice activity detection.html#batch-processing",
    "href": "1b. voice activity detection.html#batch-processing",
    "title": "Perform Voice Activity Detection (VAD)",
    "section": "Batch processing",
    "text": "Batch processing\nLet’s put everything above together.\n\n# for reference, this was the performance on a single 4090:\nprocess_shard('https://huggingface.co/datasets/collabora/librilight-webdataset/resolve/main/librilight-small-flac-000000.tar', fix_dots=True)\n\nLightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../.cache/torch/whisperx-vad-segmentation.bin`\n\n\nModel was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n\n\n\n\n\n\n\n    \n      \n      100.00% [335/335 03:30&lt;00:00]\n    \n    \n\n\n\nfor x in wds.WebDataset('/data2/libritts-r-vad-000000.tar').decode(): break\nx['__key__'].split('/')\n\n['.', 'dev-clean', '1272', '128104', '1272_128104_000001_000000']\n\n\n\nplt.hist([x['vad.npy'].shape[0] for x in wds.WebDataset('/data2/libritts-r-vad-000000.tar').decode()])\n\n(array([1.6967e+04, 0.0000e+00, 6.4500e+02, 0.0000e+00, 0.0000e+00,\n        1.0800e+02, 0.0000e+00, 2.5000e+01, 0.0000e+00, 7.0000e+00]),\n array([1. , 1.4, 1.8, 2.2, 2.6, 3. , 3.4, 3.8, 4.2, 4.6, 5. ]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html",
    "href": "2b. whisper quantization (semantic token) model.html",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "",
    "text": "from whisperspeech import wh_transcribe\nimport IPython\n\n/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n  torchaudio.set_audio_backend(\"soundfile\")\n/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n  torchaudio.set_audio_backend(\"soundfile\")\ntorchvision is not available - cannot save figures"
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#prepare-the-dataset",
    "href": "2b. whisper quantization (semantic token) model.html#prepare-the-dataset",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "Prepare the dataset",
    "text": "Prepare the dataset\n\nshards = [str(x) for x in Path('/data/whisperspeech-wds/').glob('librilight-*.tar')]\n\n\nds = wds.WebDataset(shards, shardshuffle=True)\n\n\nds2 = ds.compose(\n    wds.decode(wds.torch_audio),\n    wds.select(lambda x: 'wav' in x or 'flac' in x or 'mp3' in x or 'ogg' in x), # skip samples without audio\n    wds.rename(audio=\"flac;mp3;wav;ogg\"),\n    merge_in(derived_dataset('/data/whisperspeech-processed-wds/', 'vad')),\n    wds.map_dict(**{\"vad.npy\":wh_transcribe.chunk_merger}),\n    wh_transcribe.split_to_chunks,\n    merge_in(derived_dataset('/data/whisperspeech-processed-wds/', 'base.en-txt')),\n    wds.shuffle(),\n    wds.select(lambda x: x['i'] != 0 and x['i'] != x['imax']),\n)\n\n\nvad_shards = [str(x) for x in Path('/data/whisperspeech-processed-wds/').glob('*-large-6454-vad-*.tar.gz')]\n\n\nds = wds.WebDataset(vad_shards).decode().map_dict(**{'vad.npy':wh_transcribe.chunk_merger})\n\n\nchunks = [len(x['vad.npy'][1:-1]) for x in progress_bar(ds, total='noinfer')]\n\n\n\n\n\n\n    \n      \n      100.00% [3411/3411 00:01&lt;00:00]\n    \n    \n\n\n\nsum(chunks)\n\n203078\n\n\n\nfor x in progress_bar(ds2, total=5):\n    IPython.display.display(IPython.display.Markdown(f\"## {x['__key__']} from {x['__url__']}\\n{x['txt']}\"))\n    IPython.display.display(IPython.display.Audio(x['samples'], rate=16000))\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 00:01&lt;00:00]\n    \n    \n\n\nlarge/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_006 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nPhysically I was incapable of complying with the command, and mentally I had not the slightest intention of departing. In an outhouse devoted to storing melees, sheepskins, and harness, an old man was sitting on the doorstep, compounding a mixture which I recognized as a sheep remedy.\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nlarge/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_009 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nThe following day I was the most surprised man in South Africa when I learned that my preparation was working a marvelous cure. I was invited to remain with the bore the balance of the season as an honoured guest. Day after day I tramped the hills, returning at night as wise and as rich as when I set out. There were unmistakable indications that gold should be found in the vicinity, but the stubborn fact remained that I could not find it.\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nlarge/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_001 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nI was one of the first prospectors in the Transvaal to search for gold and a precious dance it led me. At that time, but few Englishmen had ventured into the Boer country, and such was the jealousy with which they were regarded that it was impossible to secure any information which would assist in the search. Footsoir and weary, I tramped from farm to farm, content\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nlarge/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_032 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nDead, more than twenty years. In fact, before I was married and came to live here, for he was my husband’s father. Did you know him? Yes, but I was only a little girl at the time. Why have the clothes been kept?\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nlarge/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_004 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nFortunately, I had acquired some knowledge of sheep in Australia else I believe that I should have starved. When all else failed, I became a sheep doctor and then did a compound whose virtues would have done credit to the most widely advertised path and medicine nostrum.\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nds3 = ds2.compose(\n    add_masks,\n    tokenize_text,\n    wds.to_tuple('samples', 'mask', 'in_ttoks', 'out_ttoks')\n)\n\n\nfor x in ds3: break\nx\n\n(tensor([0.0043, 0.0102, 0.0163,  ..., 0.0000, 0.0000, 0.0000]),\n tensor([ True,  True,  True,  ..., False, False, False]),\n tensor([50257,  3152,   257, 44823,  3154,  1589,    11,   484,   673,  1144,\n           572,   503,   286,  2837,   290,   706,  2063,   281,  1711,   338,\n          1057,    11,   262, 39535, 21067,   373,   625,   262,  2318,   290,\n           287,  5897, 10150,    13,  1119,  2582, 40424,   510,   262, 27913,\n          4608,   284, 47251,   290,  1043,   257,  1588,  1426,   325,   286,\n          4684, 13384,  3492,   284, 17655,   511, 15892,    13, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256]),\n tensor([ 3152,   257, 44823,  3154,  1589,    11,   484,   673,  1144,   572,\n           503,   286,  2837,   290,   706,  2063,   281,  1711,   338,  1057,\n            11,   262, 39535, 21067,   373,   625,   262,  2318,   290,   287,\n          5897, 10150,    13,  1119,  2582, 40424,   510,   262, 27913,  4608,\n           284, 47251,   290,  1043,   257,  1588,  1426,   325,   286,  4684,\n         13384,  3492,   284, 17655,   511, 15892,    13, 50256,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100]))\n\n\n\nds3 = ds2.compose(\n    add_masks,\n    lambda x: tokenize_text(x, model='medium', language='en'),\n    wds.to_tuple('samples', 'mask', 'in_ttoks', 'out_ttoks')\n)\n\n\nfor x in ds3: break\nx\n\n(tensor([0.0013, 0.0010, 0.0011,  ..., 0.0000, 0.0000, 0.0000]),\n tensor([ True,  True,  True,  ..., False, False, False]),\n tensor([50258, 50259, 50359,    32,  1326,  1270,  3931,   382,   613,    11,\n         11672,   293, 37632, 13809,    11,   576,  1319,   264,  1851,   295,\n           264,  1002,    11,   293,  1939,   576,   572,   544,  1643,   281,\n         18071,   264,  1164,   295,  3687,    11,   420,  1497,   554,  1952,\n          6018,    11,   813,   264,  1974,  5010,   295,   721,    11,   689,\n           264,  7700,   366,  4054,   293,  7006,   293, 14154,   292,    13,\n          2188,  1359, 17431,  2212,   281,  3511,   328,  3780,   311,  3567,\n           294,   702,  1536,  6717,  1062,   362, 16424,   796,   666,   257,\n          5403, 14763,    13, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257]),\n tensor([50259, 50359,    32,  1326,  1270,  3931,   382,   613,    11, 11672,\n           293, 37632, 13809,    11,   576,  1319,   264,  1851,   295,   264,\n          1002,    11,   293,  1939,   576,   572,   544,  1643,   281, 18071,\n           264,  1164,   295,  3687,    11,   420,  1497,   554,  1952,  6018,\n            11,   813,   264,  1974,  5010,   295,   721,    11,   689,   264,\n          7700,   366,  4054,   293,  7006,   293, 14154,   292,    13,  2188,\n          1359, 17431,  2212,   281,  3511,   328,  3780,   311,  3567,   294,\n           702,  1536,  6717,  1062,   362, 16424,   796,   666,   257,  5403,\n         14763,    13, 50257,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100]))\n\n\n\ntrain_ds = load_dataset('librilight-wds/librilight-small-flac-000000-s0*.tar', 'librilight-preproc-wds/', samples=2500 * 32)\n\n\nval_ds = load_dataset('librilight-wds/librilight-small-flac-000000-s11.tar', 'librilight-preproc-wds/', samples=500)\n\n\nfor x in progress_bar(wds.WebLoader(train_ds, num_workers=16, batch_size=None), total='noinfer'): pass\n\n\n\n\n\n\n    \n      \n      [245/? 00:09&lt;?]\n    \n    \n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 for x in progress_bar(wds.WebLoader(train_ds, num_workers=16, batch_size=None), total='n     │\n│   2                                                                                              │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/site-packages/fastprogress/fastprogress.py:41 in __iter__              │\n│                                                                                                  │\n│    38 │   def __iter__(self):                                                                    │\n│    39 │   │   if self.total != 0: self.update(0)                                                 │\n│    40 │   │   try:                                                                               │\n│ ❱  41 │   │   │   for i,o in enumerate(self.gen):                                                │\n│    42 │   │   │   │   if self.total and i &gt;= self.total: break                                   │\n│    43 │   │   │   │   yield o                                                                    │\n│    44 │   │   │   │   self.update(i+1)                                                           │\n│                                                                                                  │\n│ /root/workspace/webdataset/webdataset/pipeline.py:64 in iterator                                 │\n│                                                                                                  │\n│    61 │   def iterator(self):                                                                    │\n│    62 │   │   \"\"\"Create an iterator through the entire dataset, using the given number of repe   │\n│    63 │   │   for i in range(self.repetitions):                                                  │\n│ ❱  64 │   │   │   for sample in self.iterator1():                                                │\n│    65 │   │   │   │   yield sample                                                               │\n│    66 │                                                                                          │\n│    67 │   def __iter__(self):                                                                    │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633 in __next__           │\n│                                                                                                  │\n│    630 │   │   │   if self._sampler_iter is None:                                                │\n│    631 │   │   │   │   # TODO(https://github.com/pytorch/pytorch/issues/76750)                   │\n│    632 │   │   │   │   self._reset()  # type: ignore[call-arg]                                   │\n│ ❱  633 │   │   │   data = self._next_data()                                                      │\n│    634 │   │   │   self._num_yielded += 1                                                        │\n│    635 │   │   │   if self._dataset_kind == _DatasetKind.Iterable and \\                          │\n│    636 │   │   │   │   │   self._IterableDataset_len_called is not None and \\                    │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328 in _next_data        │\n│                                                                                                  │\n│   1325 │   │   │   │   return self._process_data(data)                                           │\n│   1326 │   │   │                                                                                 │\n│   1327 │   │   │   assert not self._shutdown and self._tasks_outstanding &gt; 0                     │\n│ ❱ 1328 │   │   │   idx, data = self._get_data()                                                  │\n│   1329 │   │   │   self._tasks_outstanding -= 1                                                  │\n│   1330 │   │   │   if self._dataset_kind == _DatasetKind.Iterable:                               │\n│   1331 │   │   │   │   # Check for _IterableDatasetStopIteration                                 │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294 in _get_data         │\n│                                                                                                  │\n│   1291 │   │   │   # need to call `.task_done()` because we don't use `.join()`.                 │\n│   1292 │   │   else:                                                                             │\n│   1293 │   │   │   while True:                                                                   │\n│ ❱ 1294 │   │   │   │   success, data = self._try_get_data()                                      │\n│   1295 │   │   │   │   if success:                                                               │\n│   1296 │   │   │   │   │   return data                                                           │\n│   1297                                                                                           │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132 in _try_get_data     │\n│                                                                                                  │\n│   1129 │   │   # Returns a 2-tuple:                                                              │\n│   1130 │   │   #   (bool: whether successfully get data, any: data if successful else None)      │\n│   1131 │   │   try:                                                                              │\n│ ❱ 1132 │   │   │   data = self._data_queue.get(timeout=timeout)                                  │\n│   1133 │   │   │   return (True, data)                                                           │\n│   1134 │   │   except Exception as e:                                                            │\n│   1135 │   │   │   # At timeout and error, we manually check whether any worker has              │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/multiprocessing/queues.py:113 in get                                   │\n│                                                                                                  │\n│   110 │   │   │   try:                                                                           │\n│   111 │   │   │   │   if block:                                                                  │\n│   112 │   │   │   │   │   timeout = deadline - time.monotonic()                                  │\n│ ❱ 113 │   │   │   │   │   if not self._poll(timeout):                                            │\n│   114 │   │   │   │   │   │   raise Empty                                                        │\n│   115 │   │   │   │   elif not self._poll():                                                     │\n│   116 │   │   │   │   │   raise Empty                                                            │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/multiprocessing/connection.py:257 in poll                              │\n│                                                                                                  │\n│   254 │   │   \"\"\"Whether there is any input available to be read\"\"\"                              │\n│   255 │   │   self._check_closed()                                                               │\n│   256 │   │   self._check_readable()                                                             │\n│ ❱ 257 │   │   return self._poll(timeout)                                                         │\n│   258 │                                                                                          │\n│   259 │   def __enter__(self):                                                                   │\n│   260 │   │   return self                                                                        │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/multiprocessing/connection.py:424 in _poll                             │\n│                                                                                                  │\n│   421 │   │   return self._recv(size)                                                            │\n│   422 │                                                                                          │\n│   423 │   def _poll(self, timeout):                                                              │\n│ ❱ 424 │   │   r = wait([self], timeout)                                                          │\n│   425 │   │   return bool(r)                                                                     │\n│   426                                                                                            │\n│   427                                                                                            │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/multiprocessing/connection.py:931 in wait                              │\n│                                                                                                  │\n│   928 │   │   │   │   deadline = time.monotonic() + timeout                                      │\n│   929 │   │   │                                                                                  │\n│   930 │   │   │   while True:                                                                    │\n│ ❱ 931 │   │   │   │   ready = selector.select(timeout)                                           │\n│   932 │   │   │   │   if ready:                                                                  │\n│   933 │   │   │   │   │   return [key.fileobj for (key, events) in ready]                        │\n│   934 │   │   │   │   else:                                                                      │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/selectors.py:416 in select                                             │\n│                                                                                                  │\n│   413 │   │   │   timeout = math.ceil(timeout * 1e3)                                             │\n│   414 │   │   ready = []                                                                         │\n│   415 │   │   try:                                                                               │\n│ ❱ 416 │   │   │   fd_event_list = self._selector.poll(timeout)                                   │\n│   417 │   │   except InterruptedError:                                                           │\n│   418 │   │   │   return ready                                                                   │\n│   419 │   │   for fd, event in fd_event_list:                                                    │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nKeyboardInterrupt\n\n\n\n\nfor x in train_ds:\n    print(x[3])\n    break\n\ntensor([[  464,  7664,   286,  ...,  -100,  -100,  -100],\n        [ 2953,   717,   612,  ...,  -100,  -100,  -100],\n        [25383,   339,   587,  ...,  -100,  -100,  -100],\n        ...,\n        [  392,   340,   880,  ...,  -100,  -100,  -100],\n        [  464, 31526, 11416,  ...,  -100,  -100,  -100],\n        [ 2202,   262, 16720,  ...,  -100,  -100,  -100]])"
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_006-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "href": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_006-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_006 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar",
    "text": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_006 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nPhysically I was incapable of complying with the command, and mentally I had not the slightest intention of departing. In an outhouse devoted to storing melees, sheepskins, and harness, an old man was sitting on the doorstep, compounding a mixture which I recognized as a sheep remedy."
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_009-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "href": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_009-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_009 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar",
    "text": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_009 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nThe following day I was the most surprised man in South Africa when I learned that my preparation was working a marvelous cure. I was invited to remain with the bore the balance of the season as an honoured guest. Day after day I tramped the hills, returning at night as wise and as rich as when I set out. There were unmistakable indications that gold should be found in the vicinity, but the stubborn fact remained that I could not find it."
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_001-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "href": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_001-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_001 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar",
    "text": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_001 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nI was one of the first prospectors in the Transvaal to search for gold and a precious dance it led me. At that time, but few Englishmen had ventured into the Boer country, and such was the jealousy with which they were regarded that it was impossible to secure any information which would assist in the search. Footsoir and weary, I tramped from farm to farm, content"
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_032-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "href": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_032-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_032 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar",
    "text": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_032 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nDead, more than twenty years. In fact, before I was married and came to live here, for he was my husband’s father. Did you know him? Yes, but I was only a little girl at the time. Why have the clothes been kept?"
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_004-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "href": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_004-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_004 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar",
    "text": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_004 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nFortunately, I had acquired some knowledge of sheep in Australia else I believe that I should have starved. When all else failed, I became a sheep doctor and then did a compound whose virtues would have done credit to the most widely advertised path and medicine nostrum."
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#architectural-experiments",
    "href": "2b. whisper quantization (semantic token) model.html#architectural-experiments",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "Architectural experiments",
    "text": "Architectural experiments\n\n# with learned positional embeddings, no out_blocks\nvqmodel = RQBottleneckTransformer(codebook_dim=16, vq_codes=512, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=1, lr=3e-3, warmup_steps=1000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6290 1\n\n\n\n\n\n'Entropy: 8.71'\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n107.56952\n157.32113\n8.71\n05:24\n\n\n100000\n85.44750\n101.79171\n8.70\n10:37\n\n\n126688\n81.44776\n104.25017\n8.71\n13:27\n\n\n\n\n\n    \n      \n      62.94% [3959/6290 13:26&lt;07:54 #126688/201280 loss: 81.448 / 104.250]\n    \n    \n\n\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 11 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 12 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n# with learned positional embeddings, out_blocks before positional\nvqmodel = RQBottleneckTransformer(codebook_dim=16, vq_codes=512, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=1, lr=3e-3, warmup_steps=1000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6290 1\n\n\n\n\n\n'Entropy: 8.70'\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 22:57&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n23.45991\n42.24113\n8.80\n05:48\n\n\n100000\n16.19686\n23.67809\n8.78\n11:27\n\n\n150016\n11.99028\n17.22306\n8.74\n17:07\n\n\n200000\n11.68037\n16.67605\n8.70\n22:46\n\n\n201280\n11.92631\n16.65236\n8.70\n22:57\n\n\n\n\n\n    \n      \n      100.00% [6290/6290 22:57&lt;00:00 #201280/201280 loss: 11.926 / 16.652]\n    \n    \n\n\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6290 batches x 32 samples, 1307.9 hours) was reported to be 6290 (when accessing len(dataloader)), but 6291 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n# with learned positional embeddings, out_blocks before positional, mlp before vq\nvqmodel = RQBottleneckTransformer(codebook_dim=16, vq_codes=512, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=1, lr=3e-3, warmup_steps=1000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6290 1\n\n\n\n\n\n'Entropy: 8.57'\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 23:09&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n24.63220\n44.67238\n8.74\n05:53\n\n\n100000\n14.69983\n19.67298\n8.67\n11:35\n\n\n150016\n11.50774\n17.75203\n8.58\n17:16\n\n\n200000\n11.33895\n15.66892\n8.55\n22:58\n\n\n201280\n10.87422\n15.81362\n8.57\n23:09\n\n\n\n\n\n    \n      \n      100.00% [6290/6290 23:08&lt;00:00 #201280/201280 loss: 10.874 / 15.814]\n    \n    \n\n\n\n\n\n\n# with learned positional embeddings, out_blocks after positional, mlp before vq\nvqmodel = RQBottleneckTransformer(codebook_dim=16, vq_codes=512, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=1, lr=3e-3, warmup_steps=1000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6290 1\n\n\n\n\n\n'Entropy: 8.54'\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 23:11&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.37899\n27.54997\n8.65\n05:53\n\n\n100000\n13.13329\n17.32240\n8.60\n11:35\n\n\n150016\n10.83435\n13.55371\n8.56\n17:18\n\n\n200000\n9.69492\n12.35855\n8.51\n23:00\n\n\n201280\n10.54271\n12.43994\n8.54\n23:11\n\n\n\n\n\n    \n      \n      100.00% [6290/6290 23:11&lt;00:00 #201280/201280 loss: 10.543 / 12.440]\n    \n    \n\n\n\n\n\n\n# with learned positional embeddings, out_blocks after positional, mlp before vq\nvqmodel = RQBottleneckTransformer(codebook_dim=16, vq_codes=512, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-2d-512c-cosine-padfix-premlp-learnpos-5e.model')\n\nOneCycle: 6290 5\n\n\n\n\n\n'Entropy: 8.40'\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 1:55:58&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n24.24790\n47.61960\n8.62\n05:53\n\n\n100000\n14.35983\n18.50102\n8.55\n11:35\n\n\n150016\n12.35634\n16.84217\n8.56\n17:18\n\n\n200000\n11.74603\n16.10603\n8.52\n23:00\n\n\n250016\n10.85323\n14.83014\n8.49\n28:56\n\n\n300000\n10.78046\n14.04290\n8.47\n34:38\n\n\n350016\n10.05354\n12.98133\n8.40\n40:21\n\n\n400000\n9.59631\n13.78049\n8.50\n46:03\n\n\n450016\n9.22316\n12.76403\n8.40\n51:57\n\n\n500000\n9.38958\n11.96084\n8.46\n57:40\n\n\n550016\n8.36034\n12.59843\n8.35\n1:03:22\n\n\n600000\n9.39242\n11.55411\n8.43\n1:09:05\n\n\n650016\n8.30749\n10.80241\n8.42\n1:15:02\n\n\n700000\n8.20436\n10.39852\n8.48\n1:20:45\n\n\n750016\n8.21392\n10.36367\n8.41\n1:26:27\n\n\n800000\n7.73189\n11.21438\n8.48\n1:32:10\n\n\n850016\n7.64852\n10.93893\n8.47\n1:38:06\n\n\n900000\n7.72010\n10.49391\n8.39\n1:43:48\n\n\n950016\n7.58901\n9.85925\n8.42\n1:49:31\n\n\n1000000\n7.14871\n10.67987\n8.40\n1:55:14\n\n\n1006400\n6.73056\n10.67323\n8.40\n1:55:58\n\n\n\n\n\n    \n      \n      100.00% [6290/6290 23:12&lt;00:00 #201280/201280 loss: 6.731 / 10.673]\n    \n    \n\n\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 11 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 12 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6290 batches x 32 samples, 1307.9 hours) was reported to be 6290 (when accessing len(dataloader)), but 6291 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n# with learned positional embeddings, out_blocks after positional, mlp before vq\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model')\n\nOneCycle: 6290 5\n\n\n\n\n\n'Entropy: 11.07'\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 1:57:58&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n15.49718\n26.42581\n11.23\n06:00\n\n\n100000\n11.36006\n14.78076\n11.25\n11:48\n\n\n150016\n10.29752\n13.68974\n11.19\n17:36\n\n\n200000\n9.22019\n12.14817\n11.26\n23:24\n\n\n250016\n9.09067\n13.16928\n11.17\n29:26\n\n\n300000\n8.56113\n12.38342\n11.13\n35:14\n\n\n350016\n8.30965\n12.02589\n11.15\n41:02\n\n\n400000\n7.76135\n10.97900\n11.14\n46:50\n\n\n450016\n7.34585\n10.10667\n11.11\n52:53\n\n\n500000\n7.65255\n11.02440\n11.10\n58:41\n\n\n550016\n7.47726\n10.73619\n11.10\n1:04:29\n\n\n600000\n6.96974\n9.63206\n11.14\n1:10:17\n\n\n650016\n6.93395\n9.97940\n11.08\n1:16:19\n\n\n700000\n6.64507\n8.91945\n11.13\n1:22:07\n\n\n750016\n6.53036\n9.27800\n11.01\n1:27:55\n\n\n800000\n6.50427\n8.30845\n11.07\n1:33:44\n\n\n850016\n6.51113\n9.09502\n11.12\n1:39:48\n\n\n900000\n6.05660\n8.44461\n10.99\n1:45:36\n\n\n950016\n6.20974\n8.88156\n11.06\n1:51:25\n\n\n1000000\n5.95045\n8.69922\n11.08\n1:57:13\n\n\n1006400\n6.18939\n8.88604\n11.07\n1:57:58\n\n\n\n\n\n    \n      \n      100.00% [6290/6290 23:37&lt;00:00 #201280/201280 loss: 6.189 / 8.886]\n    \n    \n\n\n\n\n\n\n# base.en Whisper with learned positional embeddings, out_blocks after positional, mlp before vq\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model')\n\nOneCycle: 6280 5\n\n\n\n\n\n'Entropy: 10.86'\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:05:51&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.17899\n27.83681\n11.11\n09:23\n\n\n100000\n13.50658\n17.32206\n11.06\n18:34\n\n\n150016\n12.10491\n15.49411\n11.08\n27:47\n\n\n200000\n11.84169\n15.30570\n10.95\n36:58\n\n\n250016\n11.19514\n14.05272\n10.99\n46:23\n\n\n300000\n10.98578\n13.69234\n10.86\n55:34\n\n\n350016\n10.58517\n13.25610\n10.99\n1:04:46\n\n\n400000\n9.87159\n12.88844\n10.91\n1:13:57\n\n\n450016\n9.76353\n12.50161\n10.92\n1:23:22\n\n\n500000\n10.08099\n12.71940\n10.94\n1:32:33\n\n\n550016\n9.85388\n12.70232\n10.89\n1:41:45\n\n\n600000\n10.50843\n11.94505\n10.93\n1:50:57\n\n\n650016\n9.29321\n12.16166\n10.96\n2:00:20\n\n\n700000\n9.24717\n11.35387\n10.93\n2:09:32\n\n\n750016\n8.80798\n11.78821\n10.95\n2:18:43\n\n\n800000\n9.14499\n10.97496\n10.93\n2:27:55\n\n\n850016\n8.75328\n11.08632\n10.96\n2:37:21\n\n\n900000\n8.40084\n10.79851\n10.88\n2:46:33\n\n\n950016\n8.73481\n11.27116\n10.96\n2:55:45\n\n\n1000000\n8.55846\n11.28967\n10.86\n3:04:57\n\n\n1004800\n8.09170\n11.12924\n10.86\n3:05:51\n\n\n\n\n\n    \n      \n      100.00% [6280/6280 37:12&lt;00:00 #200960/200960 loss: 8.092 / 11.129]\n    \n    \n\n\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 11 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 12 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6280 batches x 32 samples, 1306.1 hours) was reported to be 6280 (when accessing len(dataloader)), but 6281 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n# base.en whisper with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset (removed 1st and last segments)\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 10.79'\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:09:42&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n19.44056\n22.67257\n11.13\n09:46\n\n\n100000\n13.55178\n14.58443\n11.26\n19:23\n\n\n150016\n11.96837\n13.18968\n11.09\n29:00\n\n\n200000\n11.43871\n12.44640\n11.05\n38:49\n\n\n250016\n11.28360\n11.70081\n11.10\n48:26\n\n\n300000\n10.83751\n11.31110\n11.09\n58:03\n\n\n350016\n10.69315\n11.17086\n11.12\n1:07:40\n\n\n400000\n9.98770\n10.92539\n11.05\n1:17:30\n\n\n450016\n9.83174\n10.69181\n11.05\n1:27:07\n\n\n500000\n9.77236\n10.48352\n11.14\n1:36:44\n\n\n550016\n9.66632\n10.36597\n11.09\n1:46:21\n\n\n600000\n9.40930\n10.08656\n11.02\n1:56:09\n\n\n650016\n9.44357\n9.92484\n11.04\n2:05:46\n\n\n700000\n8.96556\n9.79054\n11.06\n2:15:23\n\n\n750016\n8.83601\n9.65099\n11.01\n2:25:00\n\n\n800000\n8.66107\n9.39148\n11.12\n2:34:48\n\n\n850016\n8.44581\n9.40969\n11.00\n2:44:26\n\n\n900000\n8.56439\n9.22455\n11.05\n2:54:03\n\n\n950016\n8.52489\n9.30351\n11.03\n3:03:40\n\n\n981120\n8.84632\n9.33108\n10.79\n3:09:42\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 37:57&lt;00:00 #196224/196224 loss: 8.846 / 9.331]\n    \n    \n\n\n/tmp/ipykernel_90303/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_90303/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6132 batches x 32 samples, 1277.7 hours) was reported to be 6132 (when accessing len(dataloader)), but 6133 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n# base.en whisper with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=1024, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=12, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-1024c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 9.36'\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:08:14&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n21.66206\n27.27091\n9.59\n09:41\n\n\n100000\n15.25066\n16.20915\n9.53\n19:13\n\n\n150016\n13.21848\n14.25581\n9.54\n28:45\n\n\n200000\n11.82871\n13.98582\n9.49\n38:30\n\n\n250016\n11.85884\n13.12596\n9.42\n48:02\n\n\n300000\n11.54107\n12.60187\n9.43\n57:34\n\n\n350016\n11.45310\n12.29700\n9.46\n1:07:07\n\n\n400000\n11.08207\n11.98462\n9.38\n1:16:51\n\n\n450016\n10.65160\n11.61482\n9.44\n1:26:24\n\n\n500000\n10.69448\n11.57619\n9.34\n1:35:56\n\n\n550016\n10.25768\n11.15084\n9.38\n1:45:29\n\n\n600000\n9.86860\n10.86430\n9.48\n1:55:14\n\n\n650016\n9.90988\n10.71315\n9.44\n2:04:47\n\n\n700000\n9.53233\n10.52028\n9.42\n2:14:19\n\n\n750016\n9.89578\n10.26827\n9.36\n2:23:52\n\n\n800000\n9.15078\n10.15152\n9.42\n2:33:36\n\n\n850016\n9.16481\n9.96554\n9.34\n2:43:09\n\n\n900000\n9.14512\n9.90501\n9.40\n2:52:42\n\n\n950016\n9.18524\n9.92719\n9.36\n3:02:15\n\n\n981120\n8.97033\n9.95517\n9.36\n3:08:14\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 37:41&lt;00:00 #196224/196224 loss: 8.970 / 9.955]\n    \n    \n\n\n/tmp/ipykernel_90303/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_90303/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n# base.en whisper with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=64, q_depth=1, n_head=8, depth=1,\n                                  downsample=1, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-64c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 5.64'\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:09:51&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n76.17780\n192.67165\n5.82\n09:48\n\n\n100000\n27.85803\n31.11143\n5.71\n19:25\n\n\n150016\n19.38920\n22.02595\n5.75\n29:02\n\n\n200000\n16.75521\n18.75611\n5.68\n38:51\n\n\n250016\n16.22832\n17.68415\n5.60\n48:29\n\n\n300000\n15.28871\n16.20028\n5.68\n58:06\n\n\n350016\n14.91663\n16.24565\n5.63\n1:07:43\n\n\n400000\n14.08824\n15.30097\n5.64\n1:17:32\n\n\n450016\n13.53690\n15.08575\n5.61\n1:27:10\n\n\n500000\n13.62558\n14.45319\n5.65\n1:36:47\n\n\n550016\n12.45450\n13.74045\n5.66\n1:46:25\n\n\n600000\n12.25172\n14.05763\n5.68\n1:56:14\n\n\n650016\n12.76195\n13.71730\n5.69\n2:05:51\n\n\n700000\n12.19483\n13.02070\n5.61\n2:15:28\n\n\n750016\n11.83110\n12.79714\n5.62\n2:25:06\n\n\n800000\n12.23673\n12.70706\n5.73\n2:34:56\n\n\n850016\n11.69901\n12.50606\n5.64\n2:44:34\n\n\n900000\n12.03180\n12.29434\n5.71\n2:54:11\n\n\n950016\n12.06521\n12.22985\n5.67\n3:03:49\n\n\n981120\n13.17802\n12.70389\n5.64\n3:09:51\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 38:00&lt;00:00 #196224/196224 loss: 13.178 / 12.704]\n    \n    \n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6132 batches x 32 samples, 1277.7 hours) was reported to be 6132 (when accessing len(dataloader)), but 6133 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=512, q_depth=1, n_head=8, depth=1,\n                                  downsample=1, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=12, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-512c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 8.44'\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:10:13&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n21.94018\n27.54010\n8.70\n09:48\n\n\n100000\n15.30265\n16.38729\n8.72\n19:26\n\n\n150016\n13.55491\n14.22489\n8.67\n29:04\n\n\n200000\n12.27958\n13.59388\n8.53\n38:54\n\n\n250016\n11.48394\n12.79483\n8.59\n48:33\n\n\n300000\n11.45791\n12.34518\n8.52\n58:11\n\n\n350016\n11.51288\n11.73254\n8.54\n1:07:49\n\n\n400000\n11.04880\n11.61340\n8.44\n1:17:41\n\n\n450016\n10.74074\n11.15114\n8.51\n1:27:20\n\n\n500000\n10.22759\n11.11760\n8.52\n1:36:59\n\n\n550016\n10.23485\n10.82111\n8.45\n1:46:38\n\n\n600000\n9.62602\n10.52901\n8.48\n1:56:30\n\n\n650016\n9.54247\n10.39591\n8.40\n2:06:08\n\n\n700000\n9.27610\n10.17579\n8.41\n2:15:47\n\n\n750016\n9.39848\n10.03072\n8.46\n2:25:25\n\n\n800000\n8.95939\n9.87603\n8.49\n2:35:15\n\n\n850016\n9.08446\n9.74571\n8.47\n2:44:54\n\n\n900000\n8.76172\n9.79162\n8.43\n2:54:32\n\n\n950016\n9.12931\n9.58630\n8.47\n3:04:10\n\n\n981120\n9.33700\n9.72177\n8.44\n3:10:13\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 38:02&lt;00:00 #196224/196224 loss: 9.337 / 9.722]\n    \n    \n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=64, vq_codes=512, q_depth=1, n_head=8, depth=1,\n                                  downsample=1, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 8.55'\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 38:00&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n24.54137\n31.36435\n8.57\n09:47\n\n\n100000\n15.90889\n17.09020\n8.58\n19:26\n\n\n150016\n13.30405\n13.95759\n8.51\n29:05\n\n\n196224\n14.19891\n12.88708\n8.55\n38:00\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 38:00&lt;00:00 #196224/196224 loss: 14.199 / 12.887]\n    \n    \n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 11.28'\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 37:54&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n17.26417\n22.29299\n11.24\n09:45\n\n\n100000\n12.41381\n14.22859\n11.25\n19:22\n\n\n150016\n11.16801\n11.97096\n11.21\n29:00\n\n\n196224\n10.49819\n10.57301\n11.28\n37:54\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 37:54&lt;00:00 #196224/196224 loss: 10.498 / 10.573]\n    \n    \n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-preconv-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 10.75'\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:11:21&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.85334\n22.89696\n10.80\n09:51\n\n\n100000\n13.86454\n16.37101\n10.73\n19:33\n\n\n150016\n12.85605\n13.55042\n10.70\n29:15\n\n\n200000\n11.59676\n12.87997\n10.70\n39:09\n\n\n250016\n11.12804\n12.39809\n10.76\n48:52\n\n\n300000\n11.10460\n11.67927\n10.78\n58:33\n\n\n350016\n11.11719\n11.55583\n10.77\n1:08:16\n\n\n400000\n10.57183\n11.07552\n10.69\n1:18:09\n\n\n450016\n10.49243\n10.82820\n10.79\n1:27:51\n\n\n500000\n10.20853\n10.77793\n10.81\n1:37:33\n\n\n550016\n10.11812\n10.54805\n10.73\n1:47:15\n\n\n600000\n9.56493\n10.22062\n10.77\n1:57:10\n\n\n650016\n9.40594\n10.19217\n10.68\n2:06:52\n\n\n700000\n9.17259\n9.85726\n10.74\n2:16:34\n\n\n750016\n9.18224\n9.74915\n10.68\n2:26:17\n\n\n800000\n8.92105\n9.47104\n10.70\n2:36:09\n\n\n850016\n8.61280\n9.39290\n10.71\n2:45:51\n\n\n900000\n8.43418\n9.33166\n10.72\n2:55:33\n\n\n950016\n8.57911\n9.33823\n10.71\n3:05:16\n\n\n981120\n8.63924\n9.37749\n10.75\n3:11:21\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 38:16&lt;00:00 #196224/196224 loss: 8.639 / 9.377]\n    \n    \n\n\n/tmp/ipykernel_100642/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_100642/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset, mean downsampling\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 10.87'\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:09:50&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n17.48580\n22.87051\n10.93\n09:49\n\n\n100000\n13.30088\n14.67394\n11.07\n19:26\n\n\n150016\n12.26683\n12.99752\n10.98\n29:04\n\n\n200000\n11.53840\n12.33599\n10.96\n38:53\n\n\n250016\n10.86994\n12.00824\n11.01\n48:30\n\n\n300000\n10.59976\n11.63654\n11.01\n58:08\n\n\n350016\n10.76181\n11.29659\n10.93\n1:07:45\n\n\n400000\n9.99428\n10.90412\n10.98\n1:17:35\n\n\n450016\n9.78972\n10.65274\n10.92\n1:27:13\n\n\n500000\n9.70262\n10.54080\n10.93\n1:36:50\n\n\n550016\n9.86663\n10.32896\n10.96\n1:46:28\n\n\n600000\n9.41082\n10.16734\n10.97\n1:56:16\n\n\n650016\n9.54473\n9.94173\n10.96\n2:05:53\n\n\n700000\n9.06406\n9.71947\n10.93\n2:15:30\n\n\n750016\n9.10101\n9.46919\n10.93\n2:25:08\n\n\n800000\n8.60536\n9.40041\n10.94\n2:34:56\n\n\n850016\n8.50216\n9.23997\n10.89\n2:44:34\n\n\n900000\n8.29970\n9.23626\n10.90\n2:54:11\n\n\n950016\n8.52151\n9.20892\n10.93\n3:03:48\n\n\n981120\n8.69804\n9.14721\n10.87\n3:09:50\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 37:58&lt;00:00 #196224/196224 loss: 8.698 / 9.147]\n    \n    \n\n\n/tmp/ipykernel_129075/774804256.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_129075/774804256.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6132 batches x 32 samples, 1277.7 hours) was reported to be 6132 (when accessing len(dataloader)), but 6133 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset, mean downsampling\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\nvqmodel.ensure_whisper()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=16, visual_class=RQVisual)\n\n\n\n\n'Entropy: 10.91'\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50008\n15.93577\n18.26651\n10.88\n31:51\n\n\n71736\n14.07252\n15.22314\n10.91\n57:51\n\n\n\n\n\n    \n      \n      35.23% [5124/14546 57:50&lt;1:46:20 #14348/203648 loss: 14.073 / 15.223]\n    \n    \n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset, mean downsampling\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\nvqmodel.ensure_whisper()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=8, visual_class=RQVisual)\n#vqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned.model')\n\n\n\n\n'Entropy: 10.75'\n\n\n\n\n\n\n\n    \n      \n      20.00% [1/5 30:53&lt;2:03:32]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50008\n17.99252\n21.13446\n10.86\n07:13\n\n\n100002\n14.73851\n15.26074\n10.74\n14:30\n\n\n150010\n12.67679\n13.50757\n10.61\n22:25\n\n\n200004\n11.98636\n12.63929\n10.72\n30:13\n\n\n248374\n12.14378\n12.26164\n10.75\n37:45\n\n\n\n\n\n    \n      \n      22.25% [3236/14546 06:51&lt;23:57 #49675/203648 loss: 12.144 / 12.262]\n    \n    \n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset, mean downsampling, eqvad\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-eqvad.model')\n\nOneCycle: 9933 5\n\n\n\n\n\n'Entropy: 9.83'\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 5:07:42&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.06458\n19.45549\n10.27\n09:48\n\n\n100000\n13.27705\n13.06077\n10.36\n19:27\n\n\n150016\n11.91958\n12.15395\n10.17\n29:05\n\n\n200000\n11.59404\n11.67862\n10.28\n38:44\n\n\n250016\n11.44242\n11.32514\n10.16\n48:22\n\n\n300000\n10.80200\n11.16721\n10.17\n58:01\n\n\n350016\n10.78535\n10.94168\n10.32\n1:07:53\n\n\n400000\n10.66275\n10.93297\n10.21\n1:17:32\n\n\n450016\n11.32866\n10.82697\n10.23\n1:27:11\n\n\n500000\n10.40007\n10.87806\n10.05\n1:36:50\n\n\n550016\n10.74838\n10.63030\n10.02\n1:46:30\n\n\n600000\n10.57567\n10.58560\n9.97\n1:56:08\n\n\n650016\n10.26159\n10.44148\n10.19\n2:06:01\n\n\n700000\n10.08803\n10.51371\n10.12\n2:15:40\n\n\n750016\n10.02600\n10.39278\n9.97\n2:25:19\n\n\n800000\n10.27624\n10.39350\n10.06\n2:34:58\n\n\n850016\n10.19159\n10.25763\n9.81\n2:44:37\n\n\n900000\n10.08171\n10.23527\n10.00\n2:54:16\n\n\n950016\n9.88339\n10.25396\n9.92\n3:03:55\n\n\n1000000\n9.62146\n10.11803\n10.06\n3:13:46\n\n\n1050016\n9.46334\n10.04561\n9.84\n3:23:25\n\n\n1100000\n9.51465\n10.11484\n9.79\n3:33:04\n\n\n1150016\n9.50131\n9.95828\n9.79\n3:42:43\n\n\n1200000\n9.53149\n9.94314\n9.89\n3:52:22\n\n\n1250016\n9.33688\n9.85693\n9.80\n4:02:01\n\n\n1300000\n9.26627\n9.81014\n9.75\n4:11:53\n\n\n1350016\n9.37144\n9.76661\n9.77\n4:21:32\n\n\n1400000\n9.06240\n9.80434\n9.76\n4:31:11\n\n\n1450016\n9.10573\n9.80284\n9.77\n4:40:50\n\n\n1500000\n9.01136\n9.71748\n9.74\n4:50:29\n\n\n1550016\n9.15775\n9.71512\n9.85\n5:00:08\n\n\n1589280\n9.26362\n9.71802\n9.83\n5:07:42\n\n\n\n\n\n    \n      \n      100.00% [9933/9933 1:01:29&lt;00:00 #317856/317856 loss: 9.264 / 9.718]\n    \n    \n\n\n/tmp/ipykernel_133489/774804256.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_133489/774804256.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 9933 batches x 32 samples, 1275.0 hours) was reported to be 9933 (when accessing len(dataloader)), but 9934 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\n# downsample conv\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n#vqmodel.save_model('vq-base.en-512c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 10.70'\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 38:13&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.56527\n21.86226\n10.70\n09:50\n\n\n100000\n14.16297\n14.83381\n10.66\n19:32\n\n\n150016\n11.57994\n12.28649\n10.68\n29:14\n\n\n196224\n10.27239\n10.96855\n10.70\n38:13\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 38:13&lt;00:00 #196224/196224 loss: 10.272 / 10.969]\n    \n    \n\n\n/tmp/ipykernel_100642/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_100642/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6132 batches x 32 samples, 1277.7 hours) was reported to be 6132 (when accessing len(dataloader)), but 6133 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=64, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n#vqmodel.save_model('vq-base.en-512c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 10.14'\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n19.88679\n26.18120\n10.21\n09:49\n\n\n100000\n14.04911\n15.88962\n10.19\n19:26\n\n\n107520\n13.98125\n15.41472\n10.14\n20:55\n\n\n\n\n\n    \n      \n      54.79% [3360/6132 20:54&lt;17:14 #107520/196224 loss: 13.981 / 15.415]\n    \n    \n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=2,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n#vqmodel.save_model('vq-base.en-512c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 11.10'\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 40:03&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.68695\n25.23358\n11.06\n10:18\n\n\n100000\n13.17344\n14.20349\n11.11\n20:28\n\n\n150016\n10.66736\n11.51643\n11.02\n30:39\n\n\n196224\n9.68099\n10.36363\n11.10\n40:03\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 40:03&lt;00:00 #196224/196224 loss: 9.681 / 10.364]\n    \n    \n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=64, q_depth=2, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n#vqmodel.save_model('vq-base.en-512c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 5.65'\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 37:35&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n82.99027\n173.42301\n5.91\n09:42\n\n\n100000\n31.85972\n36.78515\n5.81\n19:14\n\n\n150016\n23.16688\n25.48340\n5.76\n28:46\n\n\n196224\n20.68511\n23.00216\n5.65\n37:36\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 37:35&lt;00:00 #196224/196224 loss: 20.685 / 23.002]\n    \n    \n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)"
  },
  {
    "objectID": "7. Pipeline.html",
    "href": "7. Pipeline.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\nPipeline\n\n Pipeline (t2s_ref=None, s2a_ref=None)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "1. acoustic token extraction.html",
    "href": "1. acoustic token extraction.html",
    "title": "Acoustic token extraction",
    "section": "",
    "text": "# unpacked small.tar should go here:\ndatadir = Path('/mnt/')\n# you can download it downloaded from\n# https://github.com/facebookresearch/libri-light/blob/main/data_preparation/README.md\n\n\nsource\n\nload\n\n load (fname, newsr=24000)\n\nLoad an audio file to the GPU and resample to newsr.\n\nsource\n\n\nload_model\n\n load_model ()\n\nLoad the pretrained EnCodec model\n\nsource\n\n\nextract_Atoks\n\n extract_Atoks (model, audio)\n\nExtract EnCodec tokens for the given audio tensor (or file path) using the given model (see load_model).\n\nsource\n\n\nextract_acoustic\n\n extract_acoustic (srcdir:pathlib.Path, outdir:pathlib.Path)\n\nConvert audio files to .encodec files with tensors of tokens\n\n\n\n\nType\nDetails\n\n\n\n\nsrcdir\nPath\nsource dir, should contain *.flac files\n\n\noutdir\nPath\noutput dir, will get the *.encodec files\n\n\n\n\n# process all files for speaker 1401\nmodel = load_model()\nextract_acoustic(model, datadir/'small/1401', datadir/'acoustic-1401')\n\n\n\n\n\n\n    \n      \n      100.00% [131/131 05:38&lt;00:00]\n    \n    \n\n\n\n!du -hs {datadir}/acoustic-1401/\n\n78M /mnt/acoustic-1401/"
  },
  {
    "objectID": "5b. text to semantic token modeling.html",
    "href": "5b. text to semantic token modeling.html",
    "title": "Text to semantic tokens model",
    "section": "",
    "text": "from whisperspeech.wer_metrics import *\nimport torchaudio\nfrom fastprogress import master_bar"
  },
  {
    "objectID": "5b. text to semantic token modeling.html#codebook-dim-and-vq-codes",
    "href": "5b. text to semantic token modeling.html#codebook-dim-and-vq-codes",
    "title": "Text to semantic tokens model",
    "section": "Codebook dim and VQ codes",
    "text": "Codebook dim and VQ codes\n\n# make sure it works at all\ntrain_ds, val_ds = load_datasets('t2s-6454/*.tar.gz', 67000)\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-4e-hyptuned-32gpu.model').cuda()\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=8,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n# model.save_model('t2s-chr-micro-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-large-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [8/8 20:37&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n2.56502\n2.57105\n01:55\n\n\n100032\n2.09122\n2.19948\n03:50\n\n\n150016\n1.52316\n1.76259\n05:47\n\n\n200000\n1.46774\n1.54980\n07:42\n\n\n250048\n1.32339\n1.45032\n09:37\n\n\n300032\n1.33820\n1.40221\n11:33\n\n\n350016\n1.24444\n1.37554\n13:28\n\n\n400000\n1.20127\n1.35640\n15:23\n\n\n450048\n1.17260\n1.36000\n17:19\n\n\n500032\n1.12745\n1.35252\n19:14\n\n\n535552\n1.20389\n1.35048\n20:37\n\n\n\n\n\n    \n      \n      100.00% [1046/1046 02:35&lt;00:00 #66944/67000 loss: 1.204 / 1.350]\n    \n    \n\n\n\n\n\n\nvqmodel.vq_codes\n\n512\n\n\n\nfor x in val_ds: break\nx\n\n[tensor([[ 84, 104, 101,  ...,   0,   0,   0],\n         [116, 104, 114,  ...,   0,   0,   0],\n         [ 84, 104, 101,  ...,   0,   0,   0],\n         ...,\n         [ 77, 121,  32,  ...,   0,   0,   0],\n         [ 70, 111, 108,  ...,   0,   0,   0],\n         [ 85, 110, 100,  ...,   0,   0,   0]]),\n tensor([[513, 273, 195,  ..., 513, 513, 513],\n         [513, 303,  89,  ..., 513, 513, 513],\n         [513,  46, 462,  ..., 513, 513, 513],\n         ...,\n         [513, 454, 173,  ..., 513, 513, 513],\n         [513, 454, 273,  ..., 513, 513, 513],\n         [513, 332, 332,  ..., 513, 513, 513]]),\n tensor([[273, 195, 100,  ..., 513, 513, 513],\n         [303,  89, 351,  ..., 513, 513, 513],\n         [ 46, 462, 453,  ..., 513, 513, 513],\n         ...,\n         [454, 173, 173,  ..., 513, 513, 513],\n         [454, 273, 198,  ..., 513, 513, 513],\n         [332, 332,  88,  ..., 513, 513, 513]]),\n tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n\n\n\n# make sure it works at all\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c/*.tar.gz', 67000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=8,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n# model.save_model('t2s-chr-micro-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-large-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [8/8 16:32&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.82633\n2.22741\n01:32\n\n\n100032\n1.60664\n1.93017\n03:05\n\n\n150016\n1.31743\n1.83070\n04:37\n\n\n200000\n1.47108\n1.79051\n06:10\n\n\n250048\n1.34203\n1.68920\n07:42\n\n\n300032\n1.35330\n1.65576\n09:15\n\n\n350016\n1.36777\n1.62137\n10:48\n\n\n400000\n1.28777\n1.59741\n12:20\n\n\n450048\n1.21631\n1.57967\n13:53\n\n\n500032\n1.20893\n1.57008\n15:26\n\n\n535552\n1.34431\n1.56491\n16:32\n\n\n\n\n\n    \n      \n      100.00% [1046/1046 02:04&lt;00:00 #66944/67000 loss: 1.344 / 1.565]\n    \n    \n\n\n\n\n\n\nvqmodel.vq_codes\n\n512\n\n\n\n# make sure it works at all\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c-dim64/*.tar.gz', 67000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=8,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\nmodel.save_model('t2s-chr-tiny-base.en-2d-512c-dim64-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [8/8 16:39&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.47909\n1.92589\n01:33\n\n\n100032\n1.27453\n1.60590\n03:06\n\n\n150016\n1.21110\n1.39237\n04:39\n\n\n200000\n0.94160\n1.33038\n06:12\n\n\n250048\n1.01031\n1.22544\n07:46\n\n\n300032\n0.94135\n1.15930\n09:19\n\n\n350016\n0.81337\n1.04483\n10:53\n\n\n400000\n0.75797\n0.99746\n12:26\n\n\n450048\n0.74541\n0.96900\n13:59\n\n\n500032\n0.78007\n0.94995\n15:32\n\n\n535552\n0.77188\n0.94316\n16:39\n\n\n\n\n\n    \n      \n      100.00% [1046/1046 02:05&lt;00:00 #66944/67000 loss: 0.772 / 0.943]\n    \n    \n\n\n\n\n\n\n# make sure it works at all\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c-dim64/*.tar.gz', 250000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\nmodel.save_model('t2s-chr-base-base.en-2d-512c-dim64-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 1:02:33&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.54400\n1.56100\n03:07\n\n\n100032\n1.39910\n1.37488\n06:15\n\n\n150016\n1.39666\n1.29138\n09:22\n\n\n200000\n1.25551\n1.25673\n12:30\n\n\n250048\n1.32616\n1.23933\n15:37\n\n\n300032\n1.26494\n1.21040\n18:45\n\n\n350016\n1.24198\n1.19714\n21:52\n\n\n400000\n1.20484\n1.18515\n25:00\n\n\n450048\n1.23590\n1.16427\n28:08\n\n\n500032\n1.16193\n1.13852\n31:15\n\n\n550016\n1.11333\n1.11318\n34:23\n\n\n600000\n1.13005\n1.08927\n37:31\n\n\n650048\n1.06830\n1.07150\n40:39\n\n\n700032\n1.14128\n1.05110\n43:46\n\n\n750016\n1.10899\n1.03504\n46:54\n\n\n800000\n0.99225\n1.01920\n50:02\n\n\n850048\n1.02860\n1.00818\n53:10\n\n\n900032\n0.98420\n1.00081\n56:18\n\n\n950016\n0.97809\n0.99681\n59:26\n\n\n999936\n0.97344\n0.99474\n1:02:33\n\n\n\n\n\n    \n      \n      100.00% [3906/3906 15:38&lt;00:00 #249984/250000 loss: 0.973 / 0.995]\n    \n    \n\n\n\n\n\n\n# make sure it works at all\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c-dim64/*-6454-*.tar.gz', 250000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('micro', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=False)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n# model.save_model('t2s-chr-base-base.en-2d-512c-dim64-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 08:38&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.75974\n2.00099\n00:25\n\n\n100032\n1.28389\n1.75231\n00:51\n\n\n150016\n1.26908\n1.60221\n01:17\n\n\n200000\n0.95525\n1.16684\n01:43\n\n\n250048\n0.96412\n1.08697\n02:10\n\n\n300032\n0.83162\n1.04516\n02:35\n\n\n350016\n0.79592\n1.03185\n03:01\n\n\n400000\n0.84238\n0.99971\n03:27\n\n\n450048\n0.87118\n0.98406\n03:53\n\n\n500032\n0.86152\n0.96493\n04:19\n\n\n550016\n0.78976\n0.95754\n04:45\n\n\n600000\n0.74924\n0.95597\n05:10\n\n\n650048\n0.77826\n0.93904\n05:36\n\n\n700032\n0.81133\n0.93131\n06:02\n\n\n750016\n0.84455\n0.92535\n06:28\n\n\n800000\n0.73336\n0.92830\n06:53\n\n\n850048\n0.70844\n0.92848\n07:20\n\n\n900032\n0.72498\n0.91084\n07:46\n\n\n950016\n0.74317\n0.91438\n08:12\n\n\n999936\n0.73405\n0.90956\n08:38\n\n\n\n\n\n    \n      \n      100.00% [3906/3906 02:10&lt;00:00 #249984/250000 loss: 0.734 / 0.910]\n    \n    \n\n\n\n\n\n\n# make sure it works at all\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c-dim64/*-6454-*.tar.gz', 250000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('micro', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 08:41&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.66787\n2.02706\n00:26\n\n\n100032\n1.40207\n1.76437\n00:51\n\n\n150016\n1.27476\n1.62537\n01:17\n\n\n200000\n1.04374\n1.26855\n01:43\n\n\n250048\n0.98632\n1.15448\n02:10\n\n\n300032\n0.86521\n1.10903\n02:35\n\n\n350016\n0.85357\n1.09162\n03:02\n\n\n400000\n0.87830\n1.06209\n03:28\n\n\n450048\n0.85805\n1.04703\n03:54\n\n\n500032\n0.89811\n1.02455\n04:21\n\n\n550016\n0.79925\n1.01411\n04:47\n\n\n600000\n0.73336\n1.01863\n05:13\n\n\n650048\n0.82640\n0.98806\n05:39\n\n\n700032\n0.77968\n0.98760\n06:04\n\n\n750016\n0.89192\n0.97200\n06:31\n\n\n800000\n0.82949\n0.95988\n06:57\n\n\n850048\n0.79494\n0.95736\n07:23\n\n\n900032\n0.77900\n0.95254\n07:49\n\n\n950016\n0.86420\n0.94808\n08:15\n\n\n999936\n0.78041\n0.94627\n08:41\n\n\n\n\n\n    \n      \n      100.00% [3906/3906 02:10&lt;00:00 #249984/250000 loss: 0.780 / 0.946]\n    \n    \n\n\n\n\n\n\n# cps += in decoder\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c-dim64/*-6454-*.tar.gz', 250000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('micro', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 08:43&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.66341\n2.03399\n00:26\n\n\n100032\n1.39218\n1.79365\n00:52\n\n\n150016\n1.25304\n1.62505\n01:18\n\n\n200000\n1.01684\n1.18102\n01:44\n\n\n250048\n0.92334\n1.09278\n02:11\n\n\n300032\n0.87596\n1.06255\n02:36\n\n\n350016\n0.79978\n1.04593\n03:03\n\n\n400000\n0.83765\n1.01986\n03:29\n\n\n450048\n0.86234\n1.00090\n03:55\n\n\n500032\n0.83526\n0.98805\n04:21\n\n\n550016\n0.79300\n0.99132\n04:47\n\n\n600000\n0.75818\n0.99056\n05:14\n\n\n650048\n0.81305\n0.96675\n05:40\n\n\n700032\n0.81213\n0.96176\n06:06\n\n\n750016\n0.81502\n0.95014\n06:32\n\n\n800000\n0.78007\n0.93658\n06:58\n\n\n850048\n0.70850\n0.93692\n07:24\n\n\n900032\n0.81334\n0.92831\n07:50\n\n\n950016\n0.80085\n0.92518\n08:17\n\n\n999936\n0.79627\n0.92422\n08:43\n\n\n\n\n\n    \n      \n      100.00% [3906/3906 02:10&lt;00:00 #249984/250000 loss: 0.796 / 0.924]\n    \n    \n\n\n\n\n\n\n# make sure it works at all\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c-dim64/*-6454-*.tar.gz', 250000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0/2, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\nmodel.save_model('t2s-chr-base-base.en-2d-512c-dim64-cps-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 1:02:41&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.53023\n1.95611\n03:08\n\n\n100032\n1.29101\n1.72470\n06:16\n\n\n150016\n1.27883\n1.58126\n09:24\n\n\n200000\n1.04288\n1.43338\n12:32\n\n\n250048\n0.85578\n1.04747\n15:40\n\n\n300032\n0.78075\n0.93366\n18:49\n\n\n350016\n0.70612\n0.89653\n21:57\n\n\n400000\n0.67291\n0.86028\n25:04\n\n\n450048\n0.74054\n0.83844\n28:12\n\n\n500032\n0.72978\n0.82141\n31:21\n\n\n550016\n0.69412\n0.81737\n34:29\n\n\n600000\n0.65242\n0.81077\n37:36\n\n\n650048\n0.64136\n0.79844\n40:45\n\n\n700032\n0.70199\n0.78766\n43:53\n\n\n750016\n0.65563\n0.78030\n47:01\n\n\n800000\n0.61551\n0.77682\n50:09\n\n\n850048\n0.61524\n0.77560\n53:18\n\n\n900032\n0.64484\n0.76186\n56:25\n\n\n950016\n0.60820\n0.76180\n59:34\n\n\n999936\n0.67128\n0.75855\n1:02:41\n\n\n\n\n\n    \n      \n      100.00% [3906/3906 15:40&lt;00:00 #249984/250000 loss: 0.671 / 0.759]\n    \n    \n\n\n\n\n\n\n# make sure it works at all, deocder cps\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c-dim64/*-6454-*.tar.gz', 250000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0/2, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\nmodel.save_model('t2s-chr-base-base.en-2d-512c-dim64-deccps-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 1:02:42&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.64473\n2.00029\n03:08\n\n\n100032\n1.39717\n1.78492\n06:16\n\n\n150016\n1.34219\n1.63625\n09:24\n\n\n200000\n1.24191\n1.48726\n12:32\n\n\n250048\n1.14523\n1.36233\n15:40\n\n\n300032\n1.04355\n1.24797\n18:48\n\n\n350016\n0.86282\n1.20460\n21:56\n\n\n400000\n0.92359\n1.12239\n25:04\n\n\n450048\n0.89628\n1.07208\n28:13\n\n\n500032\n0.86888\n1.01718\n31:21\n\n\n550016\n0.76435\n1.00148\n34:29\n\n\n600000\n0.67413\n0.99886\n37:37\n\n\n650048\n0.74025\n0.95487\n40:46\n\n\n700032\n0.74133\n0.94135\n43:54\n\n\n750016\n0.77500\n0.92194\n47:02\n\n\n800000\n0.77986\n0.90922\n50:10\n\n\n850048\n0.65531\n0.90641\n53:18\n\n\n900032\n0.78476\n0.89178\n56:26\n\n\n950016\n0.75005\n0.88819\n59:35\n\n\n999936\n0.79055\n0.88686\n1:02:42\n\n\n\n\n\n    \n      \n      100.00% [3906/3906 15:40&lt;00:00 #249984/250000 loss: 0.791 / 0.887]\n    \n    \n\n\n\n\n\n\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c-dim64/*-6454-*.tar.gz', 250000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('base', dataset=train_ds, frozen_embeddings_model='vqmodel-256c-dim64-4e-hyptuned-32gpu.model', tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.25, embedding_projector_lr_scale=5, cps_input=True)).cuda()\n# model.load_frozen_semantic_embeddings(vqmodel)\nmodel.load_checkpoint('t2s_up_wds-epoch=0-step=17189-val_loss=0.68.ckpt')\nmodel.save_model('t2s-chr-base-base.en-2d-512c-dim64-deccps.model')\n\n\n# make sure it works at all, deocder cps\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('t2s-dim64-6454/*.tar.gz', 250000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0/2, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\nmodel.save_model('t2s-chr-base-base.en-2d-512c-dim64-deccps-mixed6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 1:02:41&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.60500\n1.71495\n03:08\n\n\n100032\n1.42996\n1.49575\n06:15\n\n\n150016\n1.30471\n1.35728\n09:23\n\n\n200000\n1.23745\n1.28199\n12:31\n\n\n250048\n1.19447\n1.22556\n15:39\n\n\n300032\n1.17491\n1.16905\n18:47\n\n\n350016\n1.10648\n1.11482\n21:55\n\n\n400000\n0.96267\n1.04749\n25:03\n\n\n450048\n0.91607\n0.96573\n28:11\n\n\n500032\n0.86155\n0.90699\n31:19\n\n\n550016\n0.82900\n0.86232\n34:28\n\n\n600000\n0.81113\n0.83169\n37:36\n\n\n650048\n0.79252\n0.80817\n40:44\n\n\n700032\n0.76063\n0.79134\n43:52\n\n\n750016\n0.73414\n0.77928\n47:01\n\n\n800000\n0.75583\n0.76989\n50:09\n\n\n850048\n0.70969\n0.76218\n53:17\n\n\n900032\n0.74606\n0.75786\n56:25\n\n\n950016\n0.73967\n0.75527\n59:33\n\n\n999936\n0.73023\n0.75428\n1:02:41\n\n\n\n\n\n    \n      \n      100.00% [3906/3906 15:40&lt;00:00 #249984/250000 loss: 0.730 / 0.754]\n    \n    \n\n\n\n\n\n\n# make sure it works at all, deocder cps\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('t2s-dim64-6454/*.tar.gz', 250000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5, cps_input=True)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\nmodel.save_model('t2s-chr-base-base.en-2d-512c-dim64-deccps-mixed6454-2.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 1:02:38&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.57490\n1.65323\n03:08\n\n\n100032\n1.30007\n1.41616\n06:16\n\n\n150016\n1.27452\n1.29610\n09:24\n\n\n200000\n1.21859\n1.23891\n12:32\n\n\n250048\n1.13482\n1.20451\n15:40\n\n\n300032\n1.12068\n1.18082\n18:48\n\n\n350016\n1.09567\n1.16008\n21:55\n\n\n400000\n1.11592\n1.11699\n25:03\n\n\n450048\n1.02158\n1.06097\n28:11\n\n\n500032\n0.96963\n1.01791\n31:20\n\n\n550016\n0.94567\n0.97895\n34:28\n\n\n600000\n0.98367\n0.96788\n37:36\n\n\n650048\n0.97887\n1.08697\n40:44\n\n\n700032\n1.01399\n1.05165\n43:52\n\n\n750016\n0.96321\n1.03418\n47:00\n\n\n800000\n1.01943\n1.03342\n50:07\n\n\n850048\n0.94176\n1.02338\n53:16\n\n\n900032\n0.97142\n1.01605\n56:24\n\n\n950016\n1.01311\n1.01310\n59:31\n\n\n999936\n0.96521\n1.01233\n1:02:38\n\n\n\n\n\n    \n      \n      100.00% [3906/3906 15:39&lt;00:00 #249984/250000 loss: 0.965 / 1.012]\n    \n    \n\n\n\n\n\n\n# make sure it works at all\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c-dim64/*.tar.gz', 250000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5)).cuda()\nmodel.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0/2, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\nmodel.save_model('t2s-chr-base-base.en-2d-512c-dim64-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 1:02:36&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.69897\n1.60454\n03:08\n\n\n100032\n1.48860\n1.41279\n06:15\n\n\n150016\n1.29894\n1.29969\n09:23\n\n\n200000\n1.18683\n1.20707\n12:31\n\n\n250048\n1.00874\n1.06997\n15:39\n\n\n300032\n0.96573\n0.97376\n18:47\n\n\n350016\n0.93865\n0.93088\n21:55\n\n\n400000\n0.84999\n0.90498\n25:02\n\n\n450048\n0.87169\n0.89323\n28:10\n\n\n500032\n0.88411\n0.87570\n31:19\n\n\n550016\n0.84026\n0.85508\n34:26\n\n\n600000\n0.88852\n0.84814\n37:34\n\n\n650048\n0.80170\n0.83732\n40:42\n\n\n700032\n0.80375\n0.83114\n43:50\n\n\n750016\n0.81585\n0.83003\n46:58\n\n\n800000\n0.75375\n0.81828\n50:06\n\n\n850048\n0.84435\n0.81684\n53:14\n\n\n900032\n0.79457\n0.81153\n56:21\n\n\n950016\n0.86009\n0.81152\n59:29\n\n\n999936\n0.78353\n0.80970\n1:02:36\n\n\n\n\n\n    \n      \n      100.00% [3906/3906 15:39&lt;00:00 #249984/250000 loss: 0.784 / 0.810]\n    \n    \n\n\n\n\n\n\n# make sure it works at all\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c-dim64/*.tar.gz', 67000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=vqmodel.codebook_dim, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5)).cuda()\n# model.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=8,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n# model.save_model('t2s-chr-micro-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-large-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [8/8 16:34&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.42096\n1.76318\n01:32\n\n\n100032\n1.27539\n1.56825\n03:05\n\n\n150016\n1.18232\n1.37608\n04:38\n\n\n200000\n1.02870\n1.29385\n06:10\n\n\n250048\n0.76373\n1.06234\n07:43\n\n\n300032\n0.80656\n0.96563\n09:16\n\n\n350016\n0.81366\n0.92360\n10:49\n\n\n400000\n0.71119\n0.89729\n12:22\n\n\n450048\n0.68225\n0.86979\n13:55\n\n\n500032\n0.69954\n0.85986\n15:28\n\n\n535552\n0.68638\n0.85534\n16:34\n\n\n\n\n\n    \n      \n      100.00% [1046/1046 02:04&lt;00:00 #66944/67000 loss: 0.686 / 0.855]\n    \n    \n\n\n\n\n\n\n# make sure it works at all\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-256c-dim64-4e-hyptuned-32gpu.model').cuda()\ntrain_ds, val_ds = load_datasets('whisperspeech-t2s-512c-dim64/*.tar.gz', 67000, vq_codes=vqmodel.vq_codes+1)\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, tunables=Tunables(weight_decay=1e-3, encoder_depth_ratio=.5, embedding_projector_lr_scale=5)).cuda()\n# model.load_frozen_semantic_embeddings(vqmodel)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=8,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n# model.save_model('t2s-chr-micro-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-large-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      62.50% [5/8 10:22&lt;06:13]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n1.40568\n1.90047\n01:33\n\n\n100032\n1.11408\n1.45658\n03:06\n\n\n150016\n1.02597\n1.29036\n04:39\n\n\n200000\n0.84436\n1.09712\n06:11\n\n\n250048\n0.81835\n0.99984\n07:44\n\n\n300032\n0.74737\n0.96436\n09:16\n\n\n350016\n0.72640\n0.92548\n10:50\n\n\n\n\n\n    \n      \n      79.35% [830/1046 01:37&lt;00:25 #48480/67000 loss: 0.719 / 0.905]\n    \n    \n\n\n\n# make sure it works at all\ntrain_ds, val_ds = load_datasets('txts-large-6454.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('micro', dataset=train_ds, ttoks_codes=256, tunables=Tunables(encoder_depth_ratio=0.75)).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n# model.save_model('t2s-chr-micro-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-large-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 09:55&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n2.78801\n2.96649\n00:38\n\n\n100032\n2.17206\n2.31484\n01:16\n\n\n150016\n1.72292\n1.83490\n01:54\n\n\n200000\n1.61193\n1.71674\n02:33\n\n\n250048\n1.56491\n1.66836\n03:11\n\n\n300032\n1.52783\n1.63776\n03:49\n\n\n350016\n1.50633\n1.61980\n04:27\n\n\n400000\n1.50440\n1.59825\n05:05\n\n\n450048\n1.49942\n1.58279\n05:43\n\n\n500032\n1.46938\n1.56660\n06:21\n\n\n550016\n1.47412\n1.55321\n06:59\n\n\n600000\n1.47976\n1.54060\n07:37\n\n\n650048\n1.45918\n1.53168\n08:15\n\n\n700032\n1.44378\n1.52434\n08:53\n\n\n750016\n1.43097\n1.52031\n09:32\n\n\n780544\n1.43909\n1.51978\n09:55\n\n\n\n\n\n    \n      \n      100.00% [3049/3049 02:28&lt;00:00 #195137/195093 loss: 1.439 / 1.520]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# make sure it works at all\ntrain_ds, val_ds = load_datasets('txts-large-6454.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('micro', dataset=train_ds, ttoks_codes=256, tunables=Tunables(encoder_depth_ratio=0.75)).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=50000, run_valid_every_iters=10000)\n# model.save_model('t2s-chr-micro-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-large-6454.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 10:16&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50048\n2.77018\n3.21193\n00:39\n\n\n100032\n2.19978\n2.33585\n01:19\n\n\n150016\n1.72773\n1.81870\n01:58\n\n\n200000\n1.61463\n1.72023\n02:38\n\n\n250048\n1.58084\n1.67646\n03:17\n\n\n300032\n1.53818\n1.64994\n03:57\n\n\n350016\n1.52391\n1.62680\n04:36\n\n\n400000\n1.49011\n1.60559\n05:16\n\n\n450048\n1.50526\n1.58791\n05:55\n\n\n500032\n1.48537\n1.57416\n06:34\n\n\n550016\n1.50840\n1.56029\n07:14\n\n\n600000\n1.44101\n1.54977\n07:54\n\n\n650048\n1.44604\n1.53820\n08:33\n\n\n700032\n1.45949\n1.53106\n09:12\n\n\n750016\n1.43625\n1.52777\n09:52\n\n\n780544\n1.47563\n1.52655\n10:16\n\n\n\n\n\n    \n      \n      100.00% [3049/3049 02:34&lt;00:00 #195137/195093 loss: 1.476 / 1.527]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# make sure it works at all\ntrain_ds, val_ds = load_datasets('txts-large-6454.feather', 'stoks-2d-512c-cosine-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('tiny', dataset=train_ds).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=64, lr=model.tunables.lr0, epochs=1,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=25000, run_valid_every_iters=5000)\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 06:33&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n25024\n2.60773\n3.69719\n00:50\n\n\n50048\n2.17093\n2.28062\n01:40\n\n\n75008\n1.84834\n1.99388\n02:31\n\n\n100032\n1.69955\n1.83580\n03:21\n\n\n125056\n1.58153\n1.71103\n04:12\n\n\n150016\n1.52247\n1.63647\n05:02\n\n\n175040\n1.46376\n1.56754\n05:52\n\n\n195136\n1.42067\n1.54652\n06:33\n\n\n\n\n\n    \n      \n      100.00% [3049/3049 06:33&lt;00:00 #195137/195093 loss: 1.421 / 1.547]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\nvqmodel = RQBottleneckTransformer.load_model(local_filename='vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model').cuda()\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=32).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-tiny-eot0.1x.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 07:24&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.00655\n2.57978\n03:57\n\n\n187296\n1.56021\n1.91384\n07:24\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 07:24&lt;00:00 #187296/187286 loss: 1.560 / 1.914]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=32).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-tiny-eot10x.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 07:25&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.05328\n2.59797\n03:58\n\n\n187296\n1.68969\n2.10436\n07:26\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 07:25&lt;00:00 #187296/187286 loss: 1.690 / 2.104]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=32).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-tiny-dropeot.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 07:26&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n1.98257\n2.55949\n03:58\n\n\n187296\n1.69418\n2.06103\n07:26\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 07:25&lt;00:00 #187296/187286 loss: 1.694 / 2.061]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=32).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-tiny-dropeot1.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 07:25&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.05267\n3.04738\n03:57\n\n\n187296\n1.73854\n2.19192\n07:25\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 07:25&lt;00:00 #187296/187286 loss: 1.739 / 2.192]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=32).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=3,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-tiny-dropeot1-3e.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [3/3 22:20&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.16998\n3.32899\n03:58\n\n\n200000\n1.61419\n2.16362\n07:56\n\n\n300000\n1.46889\n1.88969\n11:55\n\n\n400000\n1.43118\n1.82983\n15:54\n\n\n500000\n1.38416\n1.74227\n19:52\n\n\n561888\n1.31517\n1.72425\n22:20\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 07:27&lt;00:00 #187296/187286 loss: 1.315 / 1.724]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=32).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=5,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-base-dropeot1-5e.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 1:07:09&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.15739\n3.14531\n07:10\n\n\n200000\n1.61380\n2.14494\n14:20\n\n\n300000\n1.44079\n1.92889\n21:29\n\n\n400000\n1.39237\n1.84527\n28:40\n\n\n500000\n1.31629\n1.86375\n35:50\n\n\n600000\n1.30173\n1.73855\n43:00\n\n\n700000\n1.32894\n1.71813\n50:10\n\n\n800000\n1.21819\n1.68263\n57:21\n\n\n900000\n1.22189\n1.66920\n1:04:31\n\n\n936480\n1.22943\n1.66867\n1:07:09\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 13:27&lt;00:00 #187296/187286 loss: 1.229 / 1.669]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n\n\n\n\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=32, tunables=Tunables(eot_dropout_p=.5)).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=5,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-base-dropeot_p.5-5e.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 1:07:12&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.17942\n2.77374\n07:09\n\n\n200000\n1.79937\n2.20654\n14:19\n\n\n300000\n1.55872\n1.98851\n21:29\n\n\n400000\n1.48880\n1.85471\n28:39\n\n\n500000\n1.35137\n1.78626\n35:49\n\n\n600000\n1.35452\n1.73572\n43:00\n\n\n700000\n1.28091\n1.70269\n50:11\n\n\n800000\n1.21449\n1.68077\n57:23\n\n\n900000\n1.27294\n1.66469\n1:04:33\n\n\n936480\n1.23449\n1.66274\n1:07:12\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 13:29&lt;00:00 #187296/187286 loss: 1.234 / 1.663]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=32).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=10,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-base-dropeot1-10e.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 2:14:15&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.17033\n2.89845\n07:10\n\n\n200000\n1.50829\n2.02612\n14:20\n\n\n300000\n1.43682\n1.84901\n21:30\n\n\n400000\n1.35099\n1.87154\n28:40\n\n\n500000\n1.39165\n1.76240\n35:50\n\n\n600000\n1.31132\n1.79887\n43:00\n\n\n700000\n1.33037\n1.74390\n50:11\n\n\n800000\n1.27363\n1.81697\n57:22\n\n\n900000\n1.29424\n1.71701\n1:04:32\n\n\n1000000\n1.22769\n1.72730\n1:11:41\n\n\n1100000\n1.29504\n1.70143\n1:18:50\n\n\n1200000\n1.19364\n1.68924\n1:26:01\n\n\n1300000\n1.23772\n1.68347\n1:33:11\n\n\n1400000\n1.21491\n1.65472\n1:40:22\n\n\n1500000\n1.12864\n1.63846\n1:47:32\n\n\n1600000\n1.14099\n1.63741\n1:54:41\n\n\n1700000\n1.11053\n1.62853\n2:01:51\n\n\n1800000\n1.10278\n1.63842\n2:09:02\n\n\n1872960\n1.07250\n1.63690\n2:14:15\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 13:25&lt;00:00 #187296/187286 loss: 1.073 / 1.637]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=32).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=20,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-base-dropeot1-20e.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [20/20 4:28:43&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.12544\n2.91373\n07:10\n\n\n200000\n1.66652\n2.18800\n14:20\n\n\n300000\n1.49255\n1.94797\n21:30\n\n\n400000\n1.49216\n2.17191\n28:40\n\n\n500000\n1.42051\n2.13272\n35:51\n\n\n600000\n1.43777\n2.19251\n43:01\n\n\n700000\n1.39764\n1.91985\n50:12\n\n\n800000\n1.38636\n1.88027\n57:24\n\n\n900000\n1.36741\n1.77583\n1:04:34\n\n\n1000000\n1.34063\n1.78621\n1:11:44\n\n\n1100000\n1.36154\n2.18804\n1:18:54\n\n\n1200000\n1.31143\n2.22532\n1:26:04\n\n\n1300000\n1.35298\n2.09253\n1:33:14\n\n\n1400000\n1.30664\n1.89705\n1:40:24\n\n\n1500000\n1.24203\n1.74635\n1:47:34\n\n\n1600000\n1.31825\n2.27607\n1:54:44\n\n\n1700000\n1.31140\n1.72148\n2:01:54\n\n\n1800000\n1.33538\n1.92881\n2:09:04\n\n\n1900000\n1.27322\n1.70205\n2:16:15\n\n\n2000000\n1.28421\n1.80239\n2:23:24\n\n\n2100000\n1.25328\n1.70730\n2:30:35\n\n\n2200000\n1.32125\n1.69055\n2:37:46\n\n\n2300000\n1.26999\n1.70065\n2:44:56\n\n\n2400000\n1.25610\n1.67959\n2:52:07\n\n\n2500000\n1.23619\n1.68657\n2:59:18\n\n\n2600000\n1.27713\n1.65734\n3:06:29\n\n\n2700000\n1.22025\n1.66189\n3:13:40\n\n\n2800000\n1.19720\n1.64429\n3:20:50\n\n\n2900000\n1.18185\n1.66007\n3:28:01\n\n\n3000000\n1.14963\n1.64336\n3:35:12\n\n\n3100000\n1.15113\n1.65405\n3:42:22\n\n\n3200000\n1.14006\n1.64569\n3:49:33\n\n\n3300000\n1.11431\n1.65047\n3:56:43\n\n\n3400000\n1.17996\n1.65538\n4:03:54\n\n\n3500000\n1.09533\n1.65644\n4:11:05\n\n\n3600000\n1.14862\n1.66063\n4:18:16\n\n\n3700000\n1.08553\n1.65924\n4:25:25\n\n\n3745920\n1.11090\n1.65944\n4:28:43\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 13:25&lt;00:00 #187296/187286 loss: 1.111 / 1.659]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n\n\n\n\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=32).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-tiny-nodropeot.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 07:25&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n1.79375\n2.43253\n03:57\n\n\n187296\n1.50454\n1.88259\n07:25\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 07:25&lt;00:00 #187296/187286 loss: 1.505 / 1.883]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\nvqmodel = RQBottleneckTransformer.load_model(local_filename='vq-base.en-2d-4096c-cosine32-padfix-premlp-preconv-learnpos-5e-cleaned.model')\n\n\nvqmodel.rq.layers[0]._codebook.embed.shape\n\ntorch.Size([1, 4097, 32])\n\n\n\n# cleaned data\nvqmodel = RQBottleneckTransformer.load_model(local_filename='vq-base.en-2d-4096c-cosine32-padfix-premlp-preconv-learnpos-5e-cleaned.model')\ntrain_ds, val_ds = load_datasets('txts-large-6454.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-preconv-learnpos-5e-cleaned.feather')\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=32, tunables=Tunables(eot_dropout_p=0)).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-tiny-cleaned.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 07:39&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n3.74254\n3.83815\n03:57\n\n\n194048\n3.29812\n3.46448\n07:39\n\n\n\n\n\n    \n      \n      100.00% [6064/6064 07:39&lt;00:00 #194048/194040 loss: 3.298 / 3.464]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# cleaned data\nvqmodel = RQBottleneckTransformer.load_model(local_filename='vq-base.en-2d-4096c-cosine32-padfix-premlp-preconv-learnpos-5e-cleaned.model')\ntrain_ds, val_ds = load_datasets('txts-large-6454.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-preconv-learnpos-5e-cleaned.feather')\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=32, tunables=Tunables(eot_dropout_p=0)).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=5,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-base-cleaned.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 1:09:42&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n3.68559\n4.04428\n07:10\n\n\n200000\n3.01551\n3.21883\n14:22\n\n\n300000\n3.24499\n3.11739\n21:33\n\n\n400000\n2.96113\n3.07379\n28:43\n\n\n500000\n2.97186\n3.04313\n35:54\n\n\n600000\n2.97368\n3.01292\n43:06\n\n\n700000\n2.78741\n2.99100\n50:17\n\n\n800000\n2.62833\n2.96299\n57:29\n\n\n900000\n2.91650\n2.94995\n1:04:40\n\n\n970240\n2.87281\n2.94701\n1:09:42\n\n\n\n\n\n    \n      \n      100.00% [6064/6064 13:55&lt;00:00 #194048/194040 loss: 2.873 / 2.947]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# cleaned data\nvqmodel = RQBottleneckTransformer.load_model(local_filename='vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\ntrain_ds, val_ds = load_datasets('txts-large-6454.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-cleaned.feather')\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=32, tunables=Tunables(eot_dropout_p=0)).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-tiny-nopreconv-cleaned.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 07:40&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.06768\n2.32964\n03:58\n\n\n194048\n1.70456\n1.86111\n07:41\n\n\n\n\n\n    \n      \n      100.00% [6064/6064 07:40&lt;00:00 #194048/194040 loss: 1.705 / 1.861]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# cleaned data\nvqmodel = RQBottleneckTransformer.load_model(local_filename='vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\ntrain_ds, val_ds = load_datasets('txts-large-6454.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-cleaned.feather')\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=32, tunables=Tunables(eot_dropout_p=0)).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=5,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-base-nopreconv-cleaned.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 1:09:45&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.11192\n2.53749\n07:11\n\n\n200000\n1.55551\n1.72807\n14:22\n\n\n300000\n1.45162\n1.62413\n21:32\n\n\n400000\n1.41284\n1.56698\n28:44\n\n\n500000\n1.37127\n1.52728\n35:56\n\n\n600000\n1.31131\n1.48980\n43:08\n\n\n700000\n1.32659\n1.46523\n50:19\n\n\n800000\n1.23844\n1.43754\n57:31\n\n\n900000\n1.21430\n1.42486\n1:04:42\n\n\n970240\n1.21257\n1.42089\n1:09:45\n\n\n\n\n\n    \n      \n      100.00% [6064/6064 13:56&lt;00:00 #194048/194040 loss: 1.213 / 1.421]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# cleaned data\nvq_name = \"base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned\"\nvqmodel = RQBottleneckTransformer.load_model(local_filename=f'vq-{vq_name}.model')\ntrain_ds, val_ds = load_datasets('txts-large-6454-eqvad.feather', f'stoks-{vq_name}.feather')\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=32, tunables=Tunables(eot_dropout_p=0)).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=5,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-base-premean-cleaned-eqvad.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 1:53:19&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n1.39675\n1.01765\n07:08\n\n\n200000\n0.99473\n0.70335\n14:15\n\n\n300000\n0.98095\n0.65620\n21:23\n\n\n400000\n0.91941\n0.63601\n28:31\n\n\n500000\n0.89299\n0.62238\n35:38\n\n\n600000\n0.91047\n0.61567\n42:45\n\n\n700000\n0.84376\n0.60385\n49:54\n\n\n800000\n0.83167\n0.59419\n57:02\n\n\n900000\n0.85216\n0.58662\n1:04:11\n\n\n1000000\n0.80380\n0.57944\n1:11:20\n\n\n1100000\n0.75631\n0.57185\n1:18:28\n\n\n1200000\n0.85797\n0.56436\n1:25:36\n\n\n1300000\n0.77849\n0.55731\n1:32:45\n\n\n1400000\n0.78297\n0.55378\n1:39:54\n\n\n1500000\n0.77391\n0.55065\n1:47:02\n\n\n1588160\n0.79618\n0.54950\n1:53:19\n\n\n\n\n\n    \n      \n      100.00% [9926/9926 22:40&lt;00:00 #317632/317625 loss: 0.796 / 0.549]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n\n\n\n\n\n\n# cleaned data\nvqmodel = RQBottleneckTransformer.load_model(local_filename='vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model')\ntrain_ds, val_ds = load_datasets('txts-large-6454.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('tiny', dataset=train_ds, ttoks_codes=256, stoks_width=32, tunables=Tunables(eot_dropout_p=0)).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-tiny-nocleanvq.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n26976\n2.87742\n2.94360\n01:05\n\n\n\n\n\n    \n      \n      13.89% [842/6064 01:05&lt;06:46 #26944/194040 loss: 2.843 / 2.944]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\n\n\n\n\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=32).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=5,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-chr-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-large-6454-cleaned.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 1:07:10&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.05248\n2.78820\n07:10\n\n\n200000\n1.52021\n1.95092\n14:20\n\n\n300000\n1.45599\n1.82633\n21:30\n\n\n400000\n1.39701\n1.77782\n28:40\n\n\n500000\n1.33500\n1.72368\n35:49\n\n\n600000\n1.33470\n1.69579\n43:00\n\n\n700000\n1.26856\n1.66998\n50:11\n\n\n800000\n1.23700\n1.64218\n57:22\n\n\n900000\n1.24926\n1.62649\n1:04:33\n\n\n936480\n1.19744\n1.62555\n1:07:10\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 13:27&lt;00:00 #187296/187286 loss: 1.197 / 1.626]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n\n\n\n\n\n\n# reuse the VQ stoks padding token\ntrain_ds, val_ds = load_datasets('txts-large-6454-cleaned.feather', 'stoks-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.feather')\nmodel = make_model('base', dataset=train_ds, ttoks_codes=256, stoks_width=32).cuda()\nwith torch.no_grad(): model.decoder.embedding.weight[:] = vqmodel.rq.layers[0]._codebook.embed[0]\nmodel.decoder.embedding.lr_scale = 0\nfor m in [model.decoder.emb_to_hidden, model.decoder.hidden_to_emb]:\n    m.lr_scale = 5\n    std = model.tunables.init_std\n    torch.nn.init.trunc_normal_(m.weight, std=std, a=-3*std, b=3*std)\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=5,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=25000)\nmodel.save_model('t2s-chr-eot10x-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-large-6454-cleaned.model')\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 1:06:52&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.18693\n2.79553\n07:09\n\n\n200000\n1.72759\n2.21211\n14:17\n\n\n300000\n1.58026\n2.02650\n21:25\n\n\n400000\n1.50245\n1.91226\n28:32\n\n\n500000\n1.46347\n1.84637\n35:40\n\n\n600000\n1.41378\n1.78844\n42:49\n\n\n700000\n1.36574\n1.75137\n49:59\n\n\n800000\n1.32156\n1.73190\n57:07\n\n\n900000\n1.30705\n1.71549\n1:04:16\n\n\n936480\n1.25838\n1.71270\n1:06:52\n\n\n\n\n\n    \n      \n      100.00% [5853/5853 13:22&lt;00:00 #187296/187286 loss: 1.258 / 1.713]\n    \n    \n\n\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:63: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/root/workspace/spear-tts-pytorch/whisperspeech/train.py:66: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(500, self.total_steps)"
  },
  {
    "objectID": "5b. text to semantic token modeling.html#optimize-the-sampling-parameters",
    "href": "5b. text to semantic token modeling.html#optimize-the-sampling-parameters",
    "title": "Text to semantic tokens model",
    "section": "Optimize the sampling parameters",
    "text": "Optimize the sampling parameters\nFew things to keep in mind:\n\ndecoding with quantized Whisper is not the ultimate goal (and quantized Whisper itself has an average WER of &gt; 6%)\nthe dots on the plots don’t show perfect samples (WER=0%) because of logscale, the green line average line does\n\n\nvqmodel = vq_stoks.RQBottleneckTransformer.load_model(local_filename='vqmodel-512c-dim64-4e-hyptuned-32gpu.model').cuda()\nt2s = TSARTransformer.load_model(local_filename='t2s-chr-base-base.en-2d-512c-dim64-deccps.model').cuda()\n\n\ndef search_infer_hyp(T_range=(.1,10), top_k_range=(1,128)):\n    stats = WERStats()\n    for snd, gt_text in progress_bar(librispeech_data('/data/LibriSpeech/test-clean'), total=1000):\n        T = 10**rand(*np.log10(T_range))\n        top_k = round(10**rand(*np.log10([top_k_range[0]-.5,top_k_range[1]+.5])))\n        stoks = t2s.generate(gt_text.lower(), show_progress_bar=False, top_k=top_k, T=T)\n        text = vqmodel.decode_text(stoks)[0].text\n        stats.push_sample(snd, gt_text, text)\n        stats.push(top_k = top_k, T = T)\n    stats = stats.df().sort_values('wer')\n    ax = stats.plot.scatter('top_k', 'wer', logx=True, logy=True, alpha=.5)\n    ax.plot(stats.groupby('top_k')['wer'].mean(), c='green', alpha=.3)\n    stats.plot.scatter('T', 'wer', logx=True, logy=True, alpha=.5)\n    return stats\n\n\nsearch_infer_hyp()\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 11:34&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\ntop_k\nT\n\n\n\n\n315\n8.510\nNone\nNO ONE SAW HIM DO THIS FOR ALL WERE LOOKING AT...\nNo one saw him do this, for all were looking a...\n0.000000\n0.000000\n0.000000\n1.000000\n1\n0.108978\n\n\n445\n5.645\nNone\nFINALLY THE ONE PARTY WENT OFF EXULTING AND TH...\nFinally the one party went off exulting, and t...\n0.000000\n0.000000\n0.000000\n1.000000\n15\n0.504203\n\n\n451\n2.665\nNone\nSHE FOUND THE DOOR BUT IT WAS LOCKED OUTSIDE\nShe found the door but it was locked outside.\n0.000000\n0.000000\n0.000000\n1.000000\n6\n3.195216\n\n\n127\n6.270\nNone\nTHE GOLDEN STAR OF TINSEL WAS STILL ON THE TOP...\nThe golden star of tinsel was still on the top...\n0.000000\n0.000000\n0.000000\n1.000000\n95\n0.233646\n\n\n711\n2.690\nNone\nBY REASON AND AFFECTION\nby reason and affection.\n0.000000\n0.000000\n0.000000\n1.000000\n2\n0.599798\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n482\n7.605\nNone\nHE PASSED THROUGH HENLEY SAINT ALBANS AND CAME...\nHe's a successful cook and he's not going to b...\n9.647059\n0.987952\n0.998583\n0.001417\n63\n6.728842\n\n\n387\n2.735\nNone\nA ROUTE SLIGHTLY LESS DIRECT THAT'S ALL\nI am right, right, right, right, right, right,...\n14.125000\n1.000000\n1.000000\n0.000000\n49\n9.033729\n\n\n89\n4.000\nNone\nCRIED THE YOUNG LADIES AND THEY QUICKLY PUT OU...\nGrandma, you're a real man. You're a significa...\n14.545455\n1.000000\n1.000000\n0.000000\n51\n5.997146\n\n\n598\n2.900\nNone\nEACH OF US IS LASHED TO SOME PART OF THE RAFT\nHe's not going to die on gas. He's not going t...\n14.636364\n0.987730\n0.997769\n0.002231\n113\n7.302803\n\n\n226\n6.750\nNone\nHE CONTINUED HIS PRETENDED SEARCH AND TO GIVE ...\nHe can see that he can see that he can see tha...\n14.800000\n0.995516\n0.999701\n0.000299\n1\n0.294595\n\n\n\n\n1000 rows × 10 columns\n\n\n\n\n\n\n\n\n\n\nstats = search_infer_hyp(T_range=(.5,1))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 10:59&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\ntop_k\nT\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said while on her lap,...\n0.000000\n0.000000\n0.000000\n1.000000\n49\n0.933901\n\n\n268\n2.825\nNone\nSHE'S GOING TO PUT THE IRONING THINGS AWAY\nShe's going to put the ironing things away.\n0.000000\n0.000000\n0.000000\n1.000000\n3\n0.848717\n\n\n267\n3.595\nNone\nCOLD IS IT MY DARLING BLESS YOUR SWEET FACE\nCold is it my darling bless your sweet face?\n0.000000\n0.000000\n0.000000\n1.000000\n3\n0.691180\n\n\n620\n4.730\nNone\nTHE SHADOW OF THE RAFT WAS CLEARLY OUTLINED UP...\nThe shadow of the raft was clearly outlined up...\n0.000000\n0.000000\n0.000000\n1.000000\n8\n0.729119\n\n\n623\n7.000\nNone\nTHESE THOUGHTS AGITATED ME ALL DAY AND MY IMAG...\nThese thoughts agitated me all day, and my ima...\n0.000000\n0.000000\n0.000000\n1.000000\n113\n0.880830\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n393\n5.980\nNone\nI LEFT INSTRUCTIONS FOR SHIPPING MY CONTAINERS...\nAnd if you are not, you can see that the video...\n7.625000\n0.983871\n0.997984\n0.002016\n1\n0.519047\n\n\n322\n3.200\nNone\nI NOW USE THEM AS ORNAMENTAL STATUARY IN MY GA...\nNow you know, I'm not going to be able to do t...\n9.200000\n0.989247\n0.998925\n0.001075\n1\n0.907901\n\n\n676\n4.280\nNone\nTHAT IS A VERY FINE CAP YOU HAVE HE SAID\nThat is a very, very, very, very, very, very, ...\n10.900000\n0.964602\n0.985841\n0.014159\n1\n0.594969\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nhands, stirs, stirs, stirs, stirs, stirs, stir...\n24.666667\n0.986667\n0.995556\n0.004444\n29\n0.569005\n\n\n693\n2.110\nNone\nI AM VERY GLAD\nI am very happy. I am very happy. I am very ha...\n28.250000\n0.974138\n0.980603\n0.019397\n19\n0.974444\n\n\n\n\n1000 rows × 10 columns\n\n\n\n\n\n\n\n\n\n\nstats = search_infer_hyp(T_range=(.5,1), top_k_range=(3,128))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 10:40&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nstats = search_infer_hyp(T_range=(.7,1), top_k_range=(3,64))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 10:45&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nstats = search_infer_hyp(T_range=(.7,1), top_k_range=(3,10))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 10:52&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nstats = search_infer_hyp(T_range=(.7,1), top_k_range=(8,8))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 10:46&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\nstats.plot.scatter('secs', 'wer', logx=True, logy=True, alpha=.5)\n\n&lt;Axes: xlabel='secs', ylabel='wer'&gt;"
  },
  {
    "objectID": "4b. multi-language semantic to acoustic token modeling.html",
    "href": "4b. multi-language semantic to acoustic token modeling.html",
    "title": "Semantic to acoustic token modeling",
    "section": "",
    "text": "import pylab as plt\nfrom IPython.display import Audio, HTML, display"
  },
  {
    "objectID": "4b. multi-language semantic to acoustic token modeling.html#model",
    "href": "4b. multi-language semantic to acoustic token modeling.html#model",
    "title": "Semantic to acoustic token modeling",
    "section": "Model",
    "text": "Model\n\nsource\n\nCMLMVisual\n\n CMLMVisual (model, masterbar, total_steps)\n\nVisualize training progress\n\nsource\n\n\napply_rotary_pos_emb\n\n apply_rotary_pos_emb (q, k, cos, sin, query_subsampling:int=1,\n                       key_subsampling:int=1)\n\n\nsource\n\n\nrotate_half\n\n rotate_half (x)\n\n\nsource\n\n\nRotary\n\n Rotary (dim, base=10000)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nMultiHeadAttention\n\n MultiHeadAttention (n_state:int, n_head:int, qk_scale:float=1,\n                     rope:bool=False, cross=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nResidualAttentionBlock\n\n ResidualAttentionBlock (n_state:int, n_head:int,\n                         cross_attention:bool=False, rope:bool=False,\n                         qk_scale:float=1, ffn_mult:int=4)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nfemb = FlexEmbeddings(2, 3, 1).half()\nwith torch.no_grad():\n    femb.main.weight[:] = 0\n    femb.main.weight[:,:2] = torch.eye(2)\n    femb.special.weight[:] = torch.tensor([0,0,1])\nfemb.main.weight, femb.special.weight\n\n(Parameter containing:\n tensor([[1., 0., 0.],\n         [0., 1., 0.]], dtype=torch.float16, requires_grad=True),\n Parameter containing:\n tensor([[0., 0., 1.]], dtype=torch.float16, requires_grad=True))\n\n\n\nembs = femb(torch.tensor([[0,2,1,0]]))\nembs\n\ntensor([[[1., 0., 0.],\n         [0., 0., 1.],\n         [0., 1., 0.],\n         [1., 0., 0.]]], dtype=torch.float16, grad_fn=&lt;IndexPutBackward0&gt;)\n\n\n\nembs += femb(torch.tensor([[0]]))\n\n\nfemb.unembed(embs.float())\n\ntorch.Size([1, 4, 2])\ntorch.Size([1, 4, 1])\ntorch.Size([1, 4, 3])\n\n\ntensor([[[2., 0., 0.],\n         [1., 0., 1.],\n         [1., 1., 0.],\n         [2., 0., 0.]]], grad_fn=&lt;CatBackward0&gt;)\n\n\n\namodel = EncodecModel.encodec_model_24khz()\n\n\namodel.quantizer.vq.layers[0].codebook\n\ntensor([[ 5.3395, 13.1336, -3.3514,  ...,  2.2543, -4.5506,  3.7425],\n        [ 2.5562, 13.8098, -5.7393,  ...,  0.4362, -2.5406,  1.5548],\n        [ 3.9551, 12.0306, -6.5480,  ...,  1.6861, -5.3334,  1.3966],\n        ...,\n        [ 2.3868, 11.8062, -3.8374,  ..., -0.3132, -3.2393,  1.8929],\n        [ 1.1349, 11.0860, -2.8491,  ..., -0.6624, -1.4591,  1.9885],\n        [ 3.7719, 12.2859, -3.8640,  ...,  1.1728, -3.3949,  3.3238]])\n\n\n\nsource\n\n\nSADelARTransformer\n\n SADelARTransformer (depth=3, ctx_n=2250, stoks_len=750, stoks_codes=4097,\n                     stoks_width=None, spk_width=None, atoks_width=None,\n                     n_head=3, head_width=64, ffn_mult=4, quantizers=8,\n                     speaker_map={'1': 0}, tunables=Tunables(init_std=9,\n                     embeddings_std=0.2, embeddings_lr_scale=10,\n                     output_mult=5.6, query_mult=0.3,\n                     encoder_depth_ratio=0.25, linear_heads=False,\n                     rope=True, lr0=0.003, clip_gradient_norm=2,\n                     weight_decay=0.001, warmup_steps=2000, random=False))\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nTunables\n\n Tunables (init_std:float=9, embeddings_std:float=0.2,\n           embeddings_lr_scale:float=10, output_mult:float=5.6,\n           query_mult:float=0.3, encoder_depth_ratio:float=0.25,\n           linear_heads:bool=False, rope:bool=True, lr0:float=0.003,\n           clip_gradient_norm:float=2, weight_decay:float=0.001,\n           warmup_steps:float=2000, random:bool=False)\n\n\nsource\n\n\nrand\n\n rand (start, end)\n\n\nsource\n\n\nDelSumDecoder\n\n DelSumDecoder (depth=6, n_head=6, head_width=64, atoks_width=None,\n                qk_scale=1, ffn_mult=4, length=2250, codes=1024,\n                quantizers=8, linear_heads=True, rope=False,\n                pos_embs=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nAutomatic pdb calling has been turned ON\n\n\n\n# baseline\ntrain_ds = load_dataset('libri')\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model').cuda()\ntrain(f\"s2a-new\", model, train_ds, train_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n\n\n\n    \n      \n      12.50% [2/16 00:00&lt;00:03 #64/512 loss: 4.636 / nan]\n    \n    \n\n\n\n# baseline\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='old-models/vqmodel-512c-dim64-4e-hyptuned-32gpu.model', dataset=small_ds).cuda()\ntrain(f\"s2a-new\", model, small_ds, small_val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n\n\n\n\nAccuracies:acc_0acc_127.1%19.6%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 03:31&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n20000\n3.26710\n3.48323\n03:31\n\n\n\n\n\n    \n      \n      100.00% [625/625 03:31&lt;00:00 #20000/20000 loss: 3.267 / 3.483]\n    \n    \n\n\nfound NaN: small/3488/jungle_tw_0908_librivox_64kb_mp3/thejungle_04_sinclair_64kb_087\nfound NaN: small/2628/turmoil_0908_librivox_64kb_mp3/turmoil_23_tarkington_64kb_053\nfound NaN: small/3488/jungle_tw_0908_librivox_64kb_mp3/thejungle_07_sinclair_64kb_078\n\n\n\n\n\n\n# baseline\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:25.4%18.5%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 09:45&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.43991\n3.64125\n03:15\n\n\n100000\n3.26612\n3.39851\n06:13\n\n\n150016\n3.15735\n3.30149\n09:10\n\n\n160000\n3.16834\n3.29321\n09:45\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 09:45&lt;00:00 #160000/160000 loss: 3.168 / 3.293]\n    \n    \n\n\n\n\n\n\n# crossattn\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:25.6%18.7%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:56&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.42783\n3.63384\n04:32\n\n\n100000\n3.24147\n3.39155\n08:49\n\n\n150016\n3.15337\n3.29065\n13:05\n\n\n160000\n3.17545\n3.28113\n13:56\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:56&lt;00:00 #160000/160000 loss: 3.175 / 3.281]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:26.6%21.7%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 09:54&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.30626\n3.51887\n03:17\n\n\n100000\n3.10474\n3.23221\n06:18\n\n\n150016\n3.00842\n3.13535\n09:18\n\n\n160000\n3.01038\n3.12537\n09:54\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 09:53&lt;00:00 #160000/160000 loss: 3.010 / 3.125]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:28.4%23.3%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:58&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.31771\n3.37854\n04:22\n\n\n100000\n3.11682\n3.12633\n08:44\n\n\n150016\n3.02503\n3.01944\n13:05\n\n\n160000\n3.01330\n3.01079\n13:58\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:57&lt;00:00 #160000/160000 loss: 3.013 / 3.011]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards\ntrain_ds, val_ds = load_datasets('s2a-small/*.tar.gz', samples=180000)\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_133.7%27.5%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 15:48&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.06899\n3.21095\n04:24\n\n\n100000\n2.84990\n2.93317\n08:47\n\n\n150016\n2.77312\n2.81134\n13:10\n\n\n180000\n2.73082\n2.77786\n15:48\n\n\n\n\n\n    \n      \n      100.00% [5625/5625 15:48&lt;00:00 #180000/180000 loss: 2.731 / 2.778]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards\ntrain_ds, val_ds = load_datasets('s2a-small/*.tar.gz', samples=180000)\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_1acc_start_0acc_start_133.7%27.5%37.9%33.2%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 15:49&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.10401\n3.20805\n04:24\n\n\n100000\n2.83302\n2.92958\n08:48\n\n\n150016\n2.75915\n2.80805\n13:12\n\n\n180000\n2.74166\n2.77508\n15:49\n\n\n\n\n\n    \n      \n      100.00% [5625/5625 15:49&lt;00:00 #180000/180000 loss: 2.742 / 2.775]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards\ntrain_ds, val_ds = load_datasets('s2a-small/*.tar.gz', samples=180000)\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_1acc_start_0acc_start_130.1%23.4%31.8%26.8%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 15:51&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.04470\n3.38566\n04:23\n\n\n100000\n2.93321\n3.15460\n08:48\n\n\n150016\n2.76893\n3.11252\n13:13\n\n\n180000\n2.71715\n3.10554\n15:51\n\n\n\n\n\n    \n      \n      100.00% [5625/5625 15:51&lt;00:00 #180000/180000 loss: 2.717 / 3.106]\n    \n    \n\n\n\n\n\n\n# crossattn, emb all, 400+ shards\nmodel = make_model('micro', quantizers=1, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_027.5%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 12:33&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.08482\n3.07243\n03:55\n\n\n100000\n2.85717\n2.89056\n07:51\n\n\n150016\n2.79573\n2.80195\n11:46\n\n\n160000\n2.79021\n2.79402\n12:33\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 12:33&lt;00:00 #160000/160000 loss: 2.790 / 2.794]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards, linear heads\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_128.4%23.2%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:58&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.33196\n3.38296\n04:22\n\n\n100000\n3.10307\n3.11948\n08:44\n\n\n150016\n3.02954\n3.01842\n13:06\n\n\n160000\n2.97770\n3.01012\n13:58\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:57&lt;00:00 #160000/160000 loss: 2.978 / 3.010]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards, linear heads\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_128.2%23.1%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:50&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.30979\n3.40961\n04:20\n\n\n100000\n3.10014\n3.13427\n08:39\n\n\n150016\n3.06509\n3.03229\n12:59\n\n\n160000\n3.05000\n3.02330\n13:51\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:50&lt;00:00 #160000/160000 loss: 3.050 / 3.023]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards, linear heads, EmbProj lr=10\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_128.0%22.9%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:49&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.36395\n3.41816\n04:20\n\n\n100000\n3.10823\n3.15187\n08:39\n\n\n150016\n3.02961\n3.04794\n12:58\n\n\n160000\n3.05585\n3.03891\n13:50\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:49&lt;00:00 #160000/160000 loss: 3.056 / 3.039]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards, linear heads, EmbProj lr=5\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_127.9%22.8%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:48&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.35115\n3.42734\n04:18\n\n\n100000\n3.14926\n3.16296\n08:37\n\n\n150016\n3.03723\n3.05682\n12:56\n\n\n160000\n3.05514\n3.04801\n13:48\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:48&lt;00:00 #160000/160000 loss: 3.055 / 3.048]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards, linear heads\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=1e-3, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_127.3%22.0%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:45&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.43349\n3.51735\n04:18\n\n\n100000\n3.20685\n3.21835\n08:36\n\n\n150016\n3.14143\n3.11906\n12:54\n\n\n160000\n3.12234\n3.11243\n13:45\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:45&lt;00:00 #160000/160000 loss: 3.122 / 3.112]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 4Q, 400+ shards, linear heads\ntrain_ds, val_ds = load_datasets('s2a-6454-4q/*.tar.gz', samples=160000)\nmodel = make_model('micro', quantizers=4, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_1acc_2acc_327.2%22.5%20.8%18.6%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 16:38&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.73575\n3.80450\n05:12\n\n\n100000\n3.52139\n3.51007\n10:24\n\n\n150016\n3.38680\n3.39870\n15:36\n\n\n160000\n3.43426\n3.39022\n16:38\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 16:38&lt;00:00 #160000/160000 loss: 3.434 / 3.390]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q\nmodel = make_model('tiny', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\nmodel.save_model('s2a-tiny-wds-cross.model')\n\n384 32 4097\n\n\n\n\n\n\nAccuracies:30.0%25.3%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 30:15&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.19818\n3.27087\n09:27\n\n\n100000\n2.94915\n2.98427\n18:55\n\n\n150016\n2.85464\n2.87508\n28:22\n\n\n160000\n2.82709\n2.86586\n30:15\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 30:15&lt;00:00 #160000/160000 loss: 2.827 / 2.866]\n    \n    \n\n\n\n\n\n\ntrain_ds, val_ds = load_datasets('whisperspeech-s2a-512c-dim64/*.tar.gz', samples=67000, stoks_pad_token=512)\nmodel = make_model('tiny', quantizers=2, frozen_embeddings_model='vqmodel-256c-dim64-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=3, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\nmodel.save_model('s2a-512c-dim64-tiny-wds-cross-6454.model')\n\n384 64 513\n\n\n\n\n\n\nAccuracies:acc_0acc_126.8%22.4%\n\n\n\n\n\n\n\n    \n      \n      100.00% [3/3 38:18&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.09247\n3.59736\n09:29\n\n\n100000\n2.92941\n3.28088\n18:58\n\n\n150016\n2.87256\n3.16761\n28:27\n\n\n200000\n2.77072\n3.07312\n37:56\n\n\n200928\n2.80928\n3.07167\n38:18\n\n\n\n\n\n    \n      \n      100.00% [2093/2093 12:46&lt;00:00 #66976/67000 loss: 2.809 / 3.072]\n    \n    \n\n\n\n\n\n\ntrain_ds, val_ds = load_datasets('whisperspeech-s2a-512c/*.tar.gz', samples=67000, stoks_pad_token=512)\nmodel = make_model('tiny', quantizers=2, frozen_embeddings_model='vqmodel-256c-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=3, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\nmodel.save_model('s2a-512c-tiny-wds-cross-6454.model')\n\n384 32 513\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00&lt;?]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n64\n5.18270\n0.00000\n00:01\n\n\n\n\n\n    \n      \n      0.10% [2/2093 00:00&lt;15:47 #22/67000 loss: 4.619 / nan]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 4Q, 400+ shards, linear heads\ntrain_ds, val_ds = load_datasets('s2a-6454-4q/*.tar.gz', samples=160000)\nmodel = make_model('tiny', quantizers=4, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\nmodel.save_model('s2a-tiny-4q-wds-cross.model')\n\n384 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_1acc_2acc_329.2%25.0%23.6%21.4%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 33:08&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.55823\n3.63592\n10:22\n\n\n100000\n3.24530\n3.29306\n20:43\n\n\n150016\n3.12748\n3.16482\n31:04\n\n\n160000\n3.15143\n3.15668\n33:08\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 33:08&lt;00:00 #160000/160000 loss: 3.151 / 3.157]\n    \n    \n\n\n\n\n\n\n# no xenc, emb 2Q\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:10.5%8.9%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 12:23&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n4.38294\n4.82997\n03:52\n\n\n100000\n4.36976\n4.85217\n07:45\n\n\n150016\n4.48648\n4.69173\n11:37\n\n\n160000\n4.53327\n4.65840\n12:23\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 12:23&lt;00:00 #160000/160000 loss: 4.533 / 4.658]\n    \n    \n\n\n\n\n\n\n# no xenc, emb 1Q\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:19.6%13.3%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 12:19&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.74297\n4.20712\n03:51\n\n\n100000\n3.65669\n4.04836\n07:42\n\n\n150016\n3.72150\n3.91123\n11:33\n\n\n160000\n3.74524\n3.89218\n12:19\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 12:19&lt;00:00 #160000/160000 loss: 3.745 / 3.892]\n    \n    \n\n\n\n\n\n\n# cross, 1q, don't embed Q1 ;)\nmodel = make_model('micro', quantizers=1, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:14.6%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 12:13&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.34288\n4.00282\n03:49\n\n\n100000\n3.27247\n3.88112\n07:38\n\n\n150016\n3.39283\n3.73514\n11:27\n\n\n160000\n3.46310\n3.71837\n12:13\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 12:13&lt;00:00 #160000/160000 loss: 3.463 / 3.718]\n    \n    \n\n\n\n\n\n\n# cross, 1q\nmodel = make_model('micro', quantizers=1, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:22.6%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 12:17&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n2.86980\n3.44263\n03:50\n\n\n100000\n2.82082\n3.44939\n07:41\n\n\n150016\n3.00885\n3.20670\n11:32\n\n\n160000\n3.04030\n3.16246\n12:18\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 12:17&lt;00:00 #160000/160000 loss: 3.040 / 3.162]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q (replace, not add)\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:17.9%11.5%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:38&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.67311\n4.19860\n04:16\n\n\n100000\n3.67140\n4.10176\n08:31\n\n\n150016\n3.73631\n3.94144\n12:47\n\n\n160000\n3.73755\n3.91028\n13:38\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:38&lt;00:00 #160000/160000 loss: 3.738 / 3.910]\n    \n    \n\n\n\n\n\n\ns2a = make_model('base', quantizers=2, frozen_embeddings_model='vqmodel-512c-dim64-4e-hyptuned-32gpu.model', dataset=train_ds)\ns2a.load_checkpoint('vital-dust-70.ckpt')\ns2a.save_model('s2a-base-wds-cross-4e.model')\n\n512 64 513\n\n\n\ns2a = make_model('base', quantizers=2, spk_width=192, frozen_embeddings_model='old-models/vqmodel-512c-dim64-4e-hyptuned-32gpu.model')\ns2a.load_checkpoint('../s2a_delar_mup_wds-epoch=2-step=10591-val_loss=0.00.ckpt')\ns2a.save_model('s2a-base-spkemb-gutenberg.model')\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:2                                                                                    │\n│                                                                                                  │\n│   1 s2a = make_model('base', quantizers=2, spk_width=192, frozen_embeddings_model='old-model     │\n│ ❱ 2 s2a.load_checkpoint('../s2a_delar_mup_wds-epoch=2-step=10591-val_loss=0.00.ckpt')            │\n│   3 s2a.save_model('s2a-base-spkemb-gutenberg.model')                                            │\n│   4                                                                                              │\n│                                                                                                  │\n│ in load_checkpoint:276                                                                           │\n│                                                                                                  │\n│   273 │   │   assert 'pytorch-lightning_version' in spec, 'not a valid PyTorch Lightning check   │\n│   274 │   │   state_dict = {k.replace('model.', ''):v                                            │\n│   275 │   │   │   │   │     for k,v in spec['state_dict'].items()}                               │\n│ ❱ 276 │   │   self.load_state_dict(state_dict)                                                   │\n│   277 │   │   return self                                                                        │\n│   278 │                                                                                          │\n│   279 │   def save_model(self, fname):                                                           │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2041 in load_state_dict       │\n│                                                                                                  │\n│   2038 │   │   │   │   │   │   ', '.join('\"{}\"'.format(k) for k in missing_keys)))               │\n│   2039 │   │                                                                                     │\n│   2040 │   │   if len(error_msgs) &gt; 0:                                                           │\n│ ❱ 2041 │   │   │   raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(     │\n│   2042 │   │   │   │   │   │   │      self.__class__.__name__, \"\\n\\t\".join(error_msgs)))         │\n│   2043 │   │   return _IncompatibleKeys(missing_keys, unexpected_keys)                           │\n│   2044                                                                                           │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nRuntimeError: Error(s) in loading state_dict for SADelARTransformer:\n        Missing key(s) in state_dict: \"emb_to_hidden.weight\", \"emb_to_hidden.bias\". \n        size mismatch for semantic_embedding.weight: copying a param with shape torch.Size([4097, 512]) from \ncheckpoint, the shape in current model is torch.Size([513, 64])."
  },
  {
    "objectID": "B2. Training (Lightning).html",
    "href": "B2. Training (Lightning).html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "def test_fun(a:str=None, to:int = 2, toggle:bool=True):\n    assert(a is not None)\n    print(a, to, toggle)\nparse_and_call(\"test\", test_fun, [\"--to\", \"4\"], dict(a=[]), log_to_wandb=False)\n\nArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=&lt;class 'fastcore.script._HelpFormatter'&gt;, conflict_handler='error', add_help=True)\n\n\n\nfrom fastcore.script import anno_parser\ndef test_fun(a:str=None, to:int = 2, toggle:bool=True):\n    assert(a is not None)\n    print(a, to, toggle)\ntest_fun(\"a\")\nanno_parser(test_fun).parse_args([])\n\na 2 True\n\n\nNamespace(a=None, to=2, toggle=False, pdb=False, xtra=None)\n\n\n\ndef test_fun2(a:str, to:int = 2):\n    assert(a is not None)\n    print(a, to)\n\nparse_and_call(\"test\", test_fun2, [\"qwe\"], log_to_wandb=False)\n\nqwe 2"
  },
  {
    "objectID": "dataset preparation.html",
    "href": "dataset preparation.html",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "",
    "text": "WhisperSpeech is trained on heavily preprocessed speech data generated from several models:"
  },
  {
    "objectID": "dataset preparation.html#who-is-who-a-high-level-overview",
    "href": "dataset preparation.html#who-is-who-a-high-level-overview",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Who is who? A high-level overview",
    "text": "Who is who? A high-level overview\nTo get these 3 data representations we have to run the audio data through several models. The first two steps are always the same, the rest depend on the model we want to run.\n\nWe start by downloading the speech audio files into a sharded webdataset (e.g. A3. Download Project Guttenberg audiobooks).\nWe released webdatasetified versions of two important public domain speech datasets – LibriLight and Project Gutenberg Audiobooks.\nAll subsequent steps rely on voice activity detection (VAD) so we always generate segment lists for all audio files (see 1B. Voice activity detection for source code).\nThe results of this step were also released on HuggingFace – LibriLight and Project Gutenberg Audiobooks.\n\nThe next steps depend on which model we want to train of fine-tune.\n\nTo re-train the quantized Whisper model we need to transcribe the audio with base.en (2A. Whisper quantization dataset preparation). A model pretrained on 60k hours of LibriLight is available from HuggingFace whisper-vq-stoks-v2.model.\nTo train the text to semantic token model we need to transcribe the audio with Whisper small.en and extract the semantic tokens (5A. T2S dataset preparation).\nTo train the semantic to acoustic model we need to extract the semantic tokens and compress the audio with Encodec for the semantic to acoustic model (4A. S2A dataset preparation).\n\nThese three steps are all independent since they require different chunking of speech data. For quantizing Whisper and S2A training we greedily merge the VAD segments into (at most) 30 second chunks to improve training performance (more uniform chunks mean less computation time is spent on padding). For T2S we randomly truncate when merging the VAD segments so the model also learns how to work with shorter texts."
  },
  {
    "objectID": "dataset preparation.html#why-webdataset",
    "href": "dataset preparation.html#why-webdataset",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Why WebDataset?",
    "text": "Why WebDataset?\nAll WhisperSpeech training and preproc code got reorganized around webdatasets. Webdatasets are just simple tar files that store all our data samples (files) but they are great for working with very large datasets. Inside these tar files we can store multiple files per sample in any format we want (e.g. the speech mp3/flac/wav files, the text transcripts, tokens in numpy arrays). For example from the data used to train the S2A model we have:\n$ tar tf whisperspeech-s2a-512c-dim64/librilight-small-000.tar.gz |head -6\nsmall/1874/shortlifelincoln_0809_librivox_64kb_mp3/shortlifeoflincoln_10_nicolay_64kb_021.atoks.npy\nsmall/1874/shortlifelincoln_0809_librivox_64kb_mp3/shortlifeoflincoln_10_nicolay_64kb_021.stoks.npy\nsmall/28/amateur_cracksman_librivox_64kb_mp3/amateur_cracksman_04_hornung_64kb_004.atoks.npy\nsmall/28/amateur_cracksman_librivox_64kb_mp3/amateur_cracksman_04_hornung_64kb_004.stoks.npy\nsmall/1874/shortlifelincoln_0809_librivox_64kb_mp3/shortlifeoflincoln_10_nicolay_64kb_052.atoks.npy\nsmall/1874/shortlifelincoln_0809_librivox_64kb_mp3/shortlifeoflincoln_10_nicolay_64kb_052.stoks.npy\nThe name of the file is the same as the file name of the original dataset sample and the extensions tell us what kind of value they hold and in which format.\nFurthermore we can split the whole dataset into fixed-size tar files called shards and load them on demand without unpacking. It turns out that this is exactly what we need for both AI training and data preprocessing:\n\nfor training we start a multiple CPU workers in parallel, open different shards in each, stream the data sequentially from disk (fast), decode it independently and them shuffle the samples we receive from each worker to create varied training batches\nfor preprocessing we independently send each shard to a worker and save all the results in a new webdataset shard\n\nReading samples sequentialy allows us to simply compress the whole file with gzip and offers best performance even on spinning or network disks.\n\n\n\n\n\n\nNote\n\n\n\nFor the Juwels cluster there is another crucial benefit. There is a pretty low limit on the total number of files on network disks (inodes to be precise) so there is a strong preference to keep data in a few large files. The network file system performance is also better if we don’t have to open too many files.\n\n\nKeeping each shard around 5GB seems to work great (the processed shards will likely be a lot smaller but it’s a lot easier to keep a 1-to-1 shard mapping). For the almost 4TB LibriLight dataset this translates to 625 files.\nWe found it quite useful to also keep all the data in some splits. This is data dependent but for LibriLight we followed the original split (small, medium, large) but also extracted the 6454 speaker from the large split because it is was the largest single speaker dataset and it allowed us to use it during development without downloading the full 4TB.\n\n\n\n\n\n\nCaution\n\n\n\nThe sample file names should not have dots in them, otherwise the WebDataset code gets confused which files go together into one sample. This can be worked around later but it’s easiest if we just do .replace('.', '_') when storing the initial raw dataset."
  },
  {
    "objectID": "dataset preparation.html#joins-on-webdatasets",
    "href": "dataset preparation.html#joins-on-webdatasets",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Joins on WebDatasets",
    "text": "Joins on WebDatasets\nOne novel functionality we developed for this project is the capability to join multiple preprocessed webdatasets. This mechanism relies on keeping a constant ordering of samples in a shard and ensuring 1-to-1 correspondence between the input and output shards during preprocessing.\nExample usage:\nds = wds.WebDataset([str(x) for x in Path('librilight/').glob('*.tar')]).compose( # load all audio shards\n    wds.decode(wds.torch_audio), # decode the audio data\n    vq_stoks.merge_in( # merge another WebDataset\n        # for each audio (`raw`) shard, find the path and name of a corresponding `vad` shard\n        vq_stoks.derived_dataset('librilight-processed/', 'vad')\n    ),\n)\nderived_dataset creates for us a helper function that returns an opened derived dataset given the original shard file name:\ndef derived_dataset(path, kind):\n    def deriver(url):\n        url = str(Path(path)/(Path(url).name.replace(\"raw\", kind) + \".gz\"))\n        return wds.WebDataset(wds.SimpleShardList([url])).decode()\n    return deriver\nThis feature is experimental and the API may change as we develop more experience with this merging style."
  },
  {
    "objectID": "dataset preparation.html#examples-of-preprocessing-runs",
    "href": "dataset preparation.html#examples-of-preprocessing-runs",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Examples of preprocessing runs",
    "text": "Examples of preprocessing runs\nAn example of running a preprocessing step locally on a single file:\nmkdir -p guttenberg-preproc && cd guttenberg-preproc\npython -m whisperspeech.vad ../guttenberg-audiobooks/guttenberg-audiobooks-raw-000010.tar\nThis will generate a file named guttenberg-audiobooks-vad-000000.tar.gz in the guttenberg-preproc directory.\nOn the cluster we can run multiple jobs in parallel (24 in this case), each processing one input shard. Since each job is pretty short (around 30 minutes) it’s easier for the scheduler to squeeze these between longer and higher-priority jobs.\nmkdir -p whisperspeech-s2a-512c-dim64 && cd whisperspeech-s2a-512c-dim64\nfind ../librilight/ -name 'librilight-small-*.tar'| ~/clapa1/run-batch 24 \\\n    'python -m whisperspeech.prepare_s2a_dataset $FILE ../librilight-preproc\n            --vq_model ~/clapa1/scratch/vqmodel-512c-dim64-4e-hyptuned-32gpu.model\n            --batch_size 8'\nThe prepare_s2a_dataset script is taking raw audio data from the input file, automatically finding corresponding shards with VAD results in ../librilight-preproc and writing the results to the whisperspeech-s2a-512c-dim64 directory."
  },
  {
    "objectID": "dataset preparation.html#voice-activity-detection",
    "href": "dataset preparation.html#voice-activity-detection",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Voice activity detection",
    "text": "Voice activity detection\nCode: 1B. Voice activity detection\nRight now we are using the VAD model from WhisperX that is enough to avoid cutting audio in the middle of a word which would hurt automated transcriptions quite a lot. For more fancy datasets with multiple speakers we could use pyannote for it’s detection of multiple people speaking at once and diarization capability.\nWe later merge the VAD segments into longer chunks for more efficient training (less padding == higher efficiency). The code and histogram plots can be found in 2A. Whisper quantization dataset preparation"
  },
  {
    "objectID": "dataset preparation.html#transcription",
    "href": "dataset preparation.html#transcription",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Transcription",
    "text": "Transcription\nCode: 5A. T2S dataset preparation\nFor training the TTS model (T2S) we are using running batches of chunked speech segments though FasterWhisper. We use the small.en model since there seems to be little benefit from using the larger models on English speech. For multilingual TTS we would probably want to switch to large-v2.\n\n\n\n\n\n\nNote\n\n\n\nRight now we extract both semantic tokens and transcriptions in one go. Doing the transcriptions is very time consuming are the result is unlikely to change. OTOH we may want to regenerate the semantic tokens if we train different quantized Whisper models. Because of that we may want to split this into two separate steps and only merge the results just before we generate the training dataset."
  },
  {
    "objectID": "dataset preparation.html#acoustic-token-extraction",
    "href": "dataset preparation.html#acoustic-token-extraction",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Acoustic token extraction",
    "text": "Acoustic token extraction\nCode: 4A. S2A dataset preparation\nThis is basically the same as T2S above but with Encodec instead of Whisper."
  },
  {
    "objectID": "dataset preparation.html#temporary-training-dataset",
    "href": "dataset preparation.html#temporary-training-dataset",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Temporary training dataset",
    "text": "Temporary training dataset\nCode: NFY\nWe noticed that it is beneficial to reshard the dataset just before training the models. This allows us:\n\nmake sensible train/validation splits,\nshuffle the samples (which stay in their original ordering to the last moment to enable effortless joins).\n\nWe are developing a script to tackle both of these problems at the same time and generate a configurable number of output shards (you want to have more shards than you have data loader threads * GPUs, otherwise you may end up with the same sample repeated several times in a single batch)."
  },
  {
    "objectID": "6. Quality-boosting vocoder.html",
    "href": "6. Quality-boosting vocoder.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\nVocoder\n\n Vocoder (repo_id='charactr/vocos-encodec-24khz')\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "B1. Training.html",
    "href": "B1. Training.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\nSimpleVisual\n\n SimpleVisual (model, masterbar, total_steps)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nvalidate\n\n validate (model, val, half=True, bs=16, drop_last=False, dl_workers=8,\n           device='cuda')\n\n\nsource\n\n\ntrain\n\n train (checkpoint_path, model, train, val, half=True, bs=16, lr=0.0001,\n        drop_last=False, weight_decay=0.1, warmup_steps=10000, epochs=10,\n        clip_gradient_norm=None, dl_workers=8, visual_class=&lt;class\n        '__main__.SimpleVisual'&gt;, profiler=None,\n        run_valid_every_iters=8000, table_row_every_iters=80000,\n        chkpt_every_iters=None, device='cuda', trainable_params=None)"
  },
  {
    "objectID": "A. Neural modules.html",
    "href": "A. Neural modules.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\ninit_transformer\n\n init_transformer (m)\n\n\nsource\n\n\nQueryHead\n\n QueryHead (in_features:int, out_features:int, bias:bool=True,\n            device=None, dtype=None)\n\nApplies a linear transformation to the incoming data: :math:y = xA^T + b\nThis module supports :ref:TensorFloat32&lt;tf32_on_ampere&gt;.\nOn certain ROCm devices, when using float16 inputs this module will use :ref:different precision&lt;fp16_on_mi200&gt; for backward.\nArgs: in_features: size of each input sample out_features: size of each output sample bias: If set to False, the layer will not learn an additive bias. Default: True\nShape: - Input: :math:(*, H_{in}) where :math:* means any number of dimensions including none and :math:H_{in} = \\text{in\\_features}. - Output: :math:(*, H_{out}) where all but the last dimension are the same shape as the input and :math:H_{out} = \\text{out\\_features}.\nAttributes: weight: the learnable weights of the module of shape :math:(\\text{out\\_features}, \\text{in\\_features}). The values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}), where :math:k = \\frac{1}{\\text{in\\_features}} bias: the learnable bias of the module of shape :math:(\\text{out\\_features}). If :attr:bias is True, the values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where :math:k = \\frac{1}{\\text{in\\_features}}\nExamples::\n&gt;&gt;&gt; m = nn.Linear(20, 30)\n&gt;&gt;&gt; input = torch.randn(128, 20)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([128, 30])\n\nsource\n\n\nLinearHead\n\n LinearHead (in_features:int, out_features:int, bias:bool=True,\n             device=None, dtype=None)\n\nApplies a linear transformation to the incoming data: :math:y = xA^T + b\nThis module supports :ref:TensorFloat32&lt;tf32_on_ampere&gt;.\nOn certain ROCm devices, when using float16 inputs this module will use :ref:different precision&lt;fp16_on_mi200&gt; for backward.\nArgs: in_features: size of each input sample out_features: size of each output sample bias: If set to False, the layer will not learn an additive bias. Default: True\nShape: - Input: :math:(*, H_{in}) where :math:* means any number of dimensions including none and :math:H_{in} = \\text{in\\_features}. - Output: :math:(*, H_{out}) where all but the last dimension are the same shape as the input and :math:H_{out} = \\text{out\\_features}.\nAttributes: weight: the learnable weights of the module of shape :math:(\\text{out\\_features}, \\text{in\\_features}). The values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}), where :math:k = \\frac{1}{\\text{in\\_features}} bias: the learnable bias of the module of shape :math:(\\text{out\\_features}). If :attr:bias is True, the values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where :math:k = \\frac{1}{\\text{in\\_features}}\nExamples::\n&gt;&gt;&gt; m = nn.Linear(20, 30)\n&gt;&gt;&gt; input = torch.randn(128, 20)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([128, 30])\n\nsource\n\n\nLayerNorm\n\n LayerNorm (normalized_shape:Union[int,List[int],torch.Size],\n            eps:float=1e-05, elementwise_affine:bool=True, bias:bool=True,\n            device=None, dtype=None)\n\nApplies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;__\n.. math:: y = * + \nThe mean and standard-deviation are calculated over the last D dimensions, where D is the dimension of :attr:normalized_shape. For example, if :attr:normalized_shape is (3, 5) (a 2-dimensional shape), the mean and standard-deviation are computed over the last 2 dimensions of the input (i.e. input.mean((-2, -1))). :math:\\gamma and :math:\\beta are learnable affine transform parameters of :attr:normalized_shape if :attr:elementwise_affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).\n.. note:: Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the :attr:affine option, Layer Normalization applies per-element scale and bias with :attr:elementwise_affine.\nThis layer uses statistics computed from input data in both training and evaluation modes.\nArgs: normalized_shape (int or list or torch.Size): input shape from an expected input of size\n    .. math::\n        [* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]\n            \\times \\ldots \\times \\text{normalized\\_shape}[-1]]\n\n    If a single integer is used, it is treated as a singleton list, and this module will\n    normalize over the last dimension which is expected to be of that specific size.\neps: a value added to the denominator for numerical stability. Default: 1e-5\nelementwise_affine: a boolean value that when set to ``True``, this module\n    has learnable per-element affine parameters initialized to ones (for weights)\n    and zeros (for biases). Default: ``True``.\nbias: If set to ``False``, the layer will not learn an additive bias (only relevant if\n    :attr:`elementwise_affine` is ``True``). Default: ``True``.\nAttributes: weight: the learnable weights of the module of shape :math:\\text{normalized\\_shape} when :attr:elementwise_affine is set to True. The values are initialized to 1. bias: the learnable bias of the module of shape :math:\\text{normalized\\_shape} when :attr:elementwise_affine is set to True. The values are initialized to 0.\nShape: - Input: :math:(N, *) - Output: :math:(N, *) (same shape as input)\nExamples::\n&gt;&gt;&gt; # NLP Example\n&gt;&gt;&gt; batch, sentence_length, embedding_dim = 20, 5, 10\n&gt;&gt;&gt; embedding = torch.randn(batch, sentence_length, embedding_dim)\n&gt;&gt;&gt; layer_norm = nn.LayerNorm(embedding_dim)\n&gt;&gt;&gt; # Activate module\n&gt;&gt;&gt; layer_norm(embedding)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Image Example\n&gt;&gt;&gt; N, C, H, W = 20, 5, 10, 10\n&gt;&gt;&gt; input = torch.randn(N, C, H, W)\n&gt;&gt;&gt; # Normalize over the last three dimensions (i.e. the channel and spatial dimensions)\n&gt;&gt;&gt; # as shown in the image below\n&gt;&gt;&gt; layer_norm = nn.LayerNorm([C, H, W])\n&gt;&gt;&gt; output = layer_norm(input)\n.. image:: ../_static/img/nn/layer_norm.jpg :scale: 50 %\n\nsource\n\n\nsinusoids\n\n sinusoids (length, channels, max_timescale=10000)\n\nReturns sinusoids for positional embedding\n\nsource\n\n\nMultiHeadAttention\n\n MultiHeadAttention (n_state:int, n_head:int, qk_scale:float=1,\n                     rope:bool=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nResidualAttentionBlock\n\n ResidualAttentionBlock (n_state:int, n_head:int,\n                         cross_attention:bool=False, rope:bool=False,\n                         qk_scale:float=1, ffn_mult:int=4)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nEncoder\n\n Encoder (depth=6, width=384, n_head=6, length=1500, codes=1024,\n          qk_scale=1, pos_embs=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nDecoder\n\n Decoder (depth=6, width=384, n_head=6, length=1500, codes=1024,\n          qk_scale=1, pos_embs=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nSumDecoder\n\n SumDecoder (depth=6, width=384, n_head=6, length=9000, codes=1024,\n             qk_scale=1, pos_embs=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "4b. semantic to acoustic token modeling.html",
    "href": "4b. semantic to acoustic token modeling.html",
    "title": "Semantic to acoustic token modeling",
    "section": "",
    "text": "from IPython.display import Audio, HTML, display"
  },
  {
    "objectID": "4b. semantic to acoustic token modeling.html#model",
    "href": "4b. semantic to acoustic token modeling.html#model",
    "title": "Semantic to acoustic token modeling",
    "section": "Model",
    "text": "Model\n\nsource\n\nCMLMVisual\n\n CMLMVisual (model, masterbar, total_steps)\n\nVisualize training progress\n\nsource\n\n\napply_rotary_pos_emb\n\n apply_rotary_pos_emb (q, k, cos, sin)\n\n\nsource\n\n\nrotate_half\n\n rotate_half (x)\n\n\nsource\n\n\nRotary\n\n Rotary (dim, base=10000)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nMultiHeadAttention\n\n MultiHeadAttention (n_state:int, n_head:int, qk_scale:float=1,\n                     rope:bool=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nResidualAttentionBlock\n\n ResidualAttentionBlock (n_state:int, n_head:int,\n                         cross_attention:bool=False, rope:bool=False,\n                         qk_scale:float=1, ffn_mult:int=4)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nSADelARTransformer\n\n SADelARTransformer (depth=3, ctx_n=2250, stoks_len=750, stoks_codes=4097,\n                     stoks_width=None, spk_width=None, n_head=3,\n                     head_width=64, ffn_mult=4, quantizers=8,\n                     speaker_map={'1': 0}, tunables=Tunables(init_std=9,\n                     embeddings_std=0.2, embeddings_lr_scale=10,\n                     output_mult=5.6, query_mult=0.3,\n                     encoder_depth_ratio=0.25, linear_heads=False,\n                     rope=True, lr0=0.003, clip_gradient_norm=2,\n                     weight_decay=0.001, warmup_steps=2000, random=False))\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nTunables\n\n Tunables (init_std:float=9, embeddings_std:float=0.2,\n           embeddings_lr_scale:float=10, output_mult:float=5.6,\n           query_mult:float=0.3, encoder_depth_ratio:float=0.25,\n           linear_heads:bool=False, rope:bool=True, lr0:float=0.003,\n           clip_gradient_norm:float=2, weight_decay:float=0.001,\n           warmup_steps:float=2000, random:bool=False)\n\n\nsource\n\n\nrand\n\n rand (start, end)\n\n\nsource\n\n\nEmbeddingProjector\n\n EmbeddingProjector (in_features:int, out_features:int, bias:bool=True,\n                     device=None, dtype=None)\n\nApplies a linear transformation to the incoming data: :math:y = xA^T + b\nThis module supports :ref:TensorFloat32&lt;tf32_on_ampere&gt;.\nOn certain ROCm devices, when using float16 inputs this module will use :ref:different precision&lt;fp16_on_mi200&gt; for backward.\nArgs: in_features: size of each input sample out_features: size of each output sample bias: If set to False, the layer will not learn an additive bias. Default: True\nShape: - Input: :math:(*, H_{in}) where :math:* means any number of dimensions including none and :math:H_{in} = \\text{in\\_features}. - Output: :math:(*, H_{out}) where all but the last dimension are the same shape as the input and :math:H_{out} = \\text{out\\_features}.\nAttributes: weight: the learnable weights of the module of shape :math:(\\text{out\\_features}, \\text{in\\_features}). The values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}), where :math:k = \\frac{1}{\\text{in\\_features}} bias: the learnable bias of the module of shape :math:(\\text{out\\_features}). If :attr:bias is True, the values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where :math:k = \\frac{1}{\\text{in\\_features}}\nExamples::\n&gt;&gt;&gt; m = nn.Linear(20, 30)\n&gt;&gt;&gt; input = torch.randn(128, 20)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([128, 30])\n\nsource\n\n\nDelSumDecoder\n\n DelSumDecoder (depth=6, n_head=6, head_width=64, qk_scale=1, ffn_mult=4,\n                length=2250, codes=1024, quantizers=8, linear_heads=True,\n                rope=False, pos_embs=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n# baseline\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:25.4%18.5%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 09:45&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.43991\n3.64125\n03:15\n\n\n100000\n3.26612\n3.39851\n06:13\n\n\n150016\n3.15735\n3.30149\n09:10\n\n\n160000\n3.16834\n3.29321\n09:45\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 09:45&lt;00:00 #160000/160000 loss: 3.168 / 3.293]\n    \n    \n\n\n\n\n\n\n# crossattn\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:25.6%18.7%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:56&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.42783\n3.63384\n04:32\n\n\n100000\n3.24147\n3.39155\n08:49\n\n\n150016\n3.15337\n3.29065\n13:05\n\n\n160000\n3.17545\n3.28113\n13:56\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:56&lt;00:00 #160000/160000 loss: 3.175 / 3.281]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:26.6%21.7%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 09:54&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.30626\n3.51887\n03:17\n\n\n100000\n3.10474\n3.23221\n06:18\n\n\n150016\n3.00842\n3.13535\n09:18\n\n\n160000\n3.01038\n3.12537\n09:54\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 09:53&lt;00:00 #160000/160000 loss: 3.010 / 3.125]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:28.4%23.3%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:58&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.31771\n3.37854\n04:22\n\n\n100000\n3.11682\n3.12633\n08:44\n\n\n150016\n3.02503\n3.01944\n13:05\n\n\n160000\n3.01330\n3.01079\n13:58\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:57&lt;00:00 #160000/160000 loss: 3.013 / 3.011]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards\ntrain_ds, val_ds = load_datasets('s2a-small/*.tar.gz', samples=180000)\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_133.7%27.5%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 15:48&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.06899\n3.21095\n04:24\n\n\n100000\n2.84990\n2.93317\n08:47\n\n\n150016\n2.77312\n2.81134\n13:10\n\n\n180000\n2.73082\n2.77786\n15:48\n\n\n\n\n\n    \n      \n      100.00% [5625/5625 15:48&lt;00:00 #180000/180000 loss: 2.731 / 2.778]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards\ntrain_ds, val_ds = load_datasets('s2a-small/*.tar.gz', samples=180000)\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_1acc_start_0acc_start_133.7%27.5%37.9%33.2%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 15:49&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.10401\n3.20805\n04:24\n\n\n100000\n2.83302\n2.92958\n08:48\n\n\n150016\n2.75915\n2.80805\n13:12\n\n\n180000\n2.74166\n2.77508\n15:49\n\n\n\n\n\n    \n      \n      100.00% [5625/5625 15:49&lt;00:00 #180000/180000 loss: 2.742 / 2.775]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards\ntrain_ds, val_ds = load_datasets('s2a-small/*.tar.gz', samples=180000)\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_1acc_start_0acc_start_130.1%23.4%31.8%26.8%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 15:51&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.04470\n3.38566\n04:23\n\n\n100000\n2.93321\n3.15460\n08:48\n\n\n150016\n2.76893\n3.11252\n13:13\n\n\n180000\n2.71715\n3.10554\n15:51\n\n\n\n\n\n    \n      \n      100.00% [5625/5625 15:51&lt;00:00 #180000/180000 loss: 2.717 / 3.106]\n    \n    \n\n\n\n\n\n\n# crossattn, emb all, 400+ shards\nmodel = make_model('micro', quantizers=1, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_027.5%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 12:33&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.08482\n3.07243\n03:55\n\n\n100000\n2.85717\n2.89056\n07:51\n\n\n150016\n2.79573\n2.80195\n11:46\n\n\n160000\n2.79021\n2.79402\n12:33\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 12:33&lt;00:00 #160000/160000 loss: 2.790 / 2.794]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards, linear heads\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_128.4%23.2%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:58&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.33196\n3.38296\n04:22\n\n\n100000\n3.10307\n3.11948\n08:44\n\n\n150016\n3.02954\n3.01842\n13:06\n\n\n160000\n2.97770\n3.01012\n13:58\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:57&lt;00:00 #160000/160000 loss: 2.978 / 3.010]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards, linear heads\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_128.2%23.1%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:50&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.30979\n3.40961\n04:20\n\n\n100000\n3.10014\n3.13427\n08:39\n\n\n150016\n3.06509\n3.03229\n12:59\n\n\n160000\n3.05000\n3.02330\n13:51\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:50&lt;00:00 #160000/160000 loss: 3.050 / 3.023]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards, linear heads, EmbProj lr=10\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_128.0%22.9%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:49&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.36395\n3.41816\n04:20\n\n\n100000\n3.10823\n3.15187\n08:39\n\n\n150016\n3.02961\n3.04794\n12:58\n\n\n160000\n3.05585\n3.03891\n13:50\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:49&lt;00:00 #160000/160000 loss: 3.056 / 3.039]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards, linear heads, EmbProj lr=5\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_127.9%22.8%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:48&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.35115\n3.42734\n04:18\n\n\n100000\n3.14926\n3.16296\n08:37\n\n\n150016\n3.03723\n3.05682\n12:56\n\n\n160000\n3.05514\n3.04801\n13:48\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:48&lt;00:00 #160000/160000 loss: 3.055 / 3.048]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q, 400+ shards, linear heads\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=1e-3, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_127.3%22.0%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:45&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.43349\n3.51735\n04:18\n\n\n100000\n3.20685\n3.21835\n08:36\n\n\n150016\n3.14143\n3.11906\n12:54\n\n\n160000\n3.12234\n3.11243\n13:45\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:45&lt;00:00 #160000/160000 loss: 3.122 / 3.112]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 4Q, 400+ shards, linear heads\ntrain_ds, val_ds = load_datasets('s2a-6454-4q/*.tar.gz', samples=160000)\nmodel = make_model('micro', quantizers=4, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_1acc_2acc_327.2%22.5%20.8%18.6%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 16:38&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.73575\n3.80450\n05:12\n\n\n100000\n3.52139\n3.51007\n10:24\n\n\n150016\n3.38680\n3.39870\n15:36\n\n\n160000\n3.43426\n3.39022\n16:38\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 16:38&lt;00:00 #160000/160000 loss: 3.434 / 3.390]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q\nmodel = make_model('tiny', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\nmodel.save_model('s2a-tiny-wds-cross.model')\n\n384 32 4097\n\n\n\n\n\n\nAccuracies:30.0%25.3%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 30:15&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.19818\n3.27087\n09:27\n\n\n100000\n2.94915\n2.98427\n18:55\n\n\n150016\n2.85464\n2.87508\n28:22\n\n\n160000\n2.82709\n2.86586\n30:15\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 30:15&lt;00:00 #160000/160000 loss: 2.827 / 2.866]\n    \n    \n\n\n\n\n\n\ntrain_ds, val_ds = load_datasets('whisperspeech-s2a-512c-dim64/*.tar.gz', samples=67000, stoks_pad_token=512)\nmodel = make_model('tiny', quantizers=2, frozen_embeddings_model='vqmodel-256c-dim64-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=3, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\nmodel.save_model('s2a-512c-dim64-tiny-wds-cross-6454.model')\n\n384 64 513\n\n\n\n\n\n\nAccuracies:acc_0acc_126.8%22.4%\n\n\n\n\n\n\n\n    \n      \n      100.00% [3/3 38:18&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.09247\n3.59736\n09:29\n\n\n100000\n2.92941\n3.28088\n18:58\n\n\n150016\n2.87256\n3.16761\n28:27\n\n\n200000\n2.77072\n3.07312\n37:56\n\n\n200928\n2.80928\n3.07167\n38:18\n\n\n\n\n\n    \n      \n      100.00% [2093/2093 12:46&lt;00:00 #66976/67000 loss: 2.809 / 3.072]\n    \n    \n\n\n\n\n\n\ntrain_ds, val_ds = load_datasets('whisperspeech-s2a-512c/*.tar.gz', samples=67000, stoks_pad_token=512)\nmodel = make_model('tiny', quantizers=2, frozen_embeddings_model='vqmodel-256c-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=3, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\nmodel.save_model('s2a-512c-tiny-wds-cross-6454.model')\n\n384 32 513\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00&lt;?]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n64\n5.18270\n0.00000\n00:01\n\n\n\n\n\n    \n      \n      0.10% [2/2093 00:00&lt;15:47 #22/67000 loss: 4.619 / nan]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 4Q, 400+ shards, linear heads\ntrain_ds, val_ds = load_datasets('s2a-6454-4q/*.tar.gz', samples=160000)\nmodel = make_model('tiny', quantizers=4, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds, tunables=Tunables(linear_heads=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\nmodel.save_model('s2a-tiny-4q-wds-cross.model')\n\n384 32 4097\n\n\n\n\n\n\nAccuracies:acc_0acc_1acc_2acc_329.2%25.0%23.6%21.4%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 33:08&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.55823\n3.63592\n10:22\n\n\n100000\n3.24530\n3.29306\n20:43\n\n\n150016\n3.12748\n3.16482\n31:04\n\n\n160000\n3.15143\n3.15668\n33:08\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 33:08&lt;00:00 #160000/160000 loss: 3.151 / 3.157]\n    \n    \n\n\n\n\n\n\n# no xenc, emb 2Q\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:10.5%8.9%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 12:23&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n4.38294\n4.82997\n03:52\n\n\n100000\n4.36976\n4.85217\n07:45\n\n\n150016\n4.48648\n4.69173\n11:37\n\n\n160000\n4.53327\n4.65840\n12:23\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 12:23&lt;00:00 #160000/160000 loss: 4.533 / 4.658]\n    \n    \n\n\n\n\n\n\n# no xenc, emb 1Q\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:19.6%13.3%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 12:19&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.74297\n4.20712\n03:51\n\n\n100000\n3.65669\n4.04836\n07:42\n\n\n150016\n3.72150\n3.91123\n11:33\n\n\n160000\n3.74524\n3.89218\n12:19\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 12:19&lt;00:00 #160000/160000 loss: 3.745 / 3.892]\n    \n    \n\n\n\n\n\n\n# cross, 1q, don't embed Q1 ;)\nmodel = make_model('micro', quantizers=1, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:14.6%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 12:13&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.34288\n4.00282\n03:49\n\n\n100000\n3.27247\n3.88112\n07:38\n\n\n150016\n3.39283\n3.73514\n11:27\n\n\n160000\n3.46310\n3.71837\n12:13\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 12:13&lt;00:00 #160000/160000 loss: 3.463 / 3.718]\n    \n    \n\n\n\n\n\n\n# cross, 1q\nmodel = make_model('micro', quantizers=1, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:22.6%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 12:17&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n2.86980\n3.44263\n03:50\n\n\n100000\n2.82082\n3.44939\n07:41\n\n\n150016\n3.00885\n3.20670\n11:32\n\n\n160000\n3.04030\n3.16246\n12:18\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 12:17&lt;00:00 #160000/160000 loss: 3.040 / 3.162]\n    \n    \n\n\n\n\n\n\n# crossattn, emb 2Q (replace, not add)\nmodel = make_model('micro', quantizers=2, frozen_embeddings_model='vqmodel-4e-hyptuned-32gpu.model', dataset=train_ds).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=50000, run_valid_every_iters=10000, visual_class=CMLMVisual)\n\n192 32 4097\n\n\n\n\n\n\nAccuracies:17.9%11.5%nan%nan%\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 13:38&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n50016\n3.67311\n4.19860\n04:16\n\n\n100000\n3.67140\n4.10176\n08:31\n\n\n150016\n3.73631\n3.94144\n12:47\n\n\n160000\n3.73755\n3.91028\n13:38\n\n\n\n\n\n    \n      \n      100.00% [5000/5000 13:38&lt;00:00 #160000/160000 loss: 3.738 / 3.910]\n    \n    \n\n\n\n\n\n\ns2a = make_model('base', quantizers=2, frozen_embeddings_model='vqmodel-512c-dim64-4e-hyptuned-32gpu.model', dataset=train_ds)\ns2a.load_checkpoint('vital-dust-70.ckpt')\ns2a.save_model('s2a-base-wds-cross-4e.model')\n\n512 64 513"
  },
  {
    "objectID": "C. Word error rate metrics.html",
    "href": "C. Word error rate metrics.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\ndefault_transform([\"Footnote, Somber Tashan, May 12, 1856\", \"FOOTNOTE SUMNER TO SHANNON MAY TWELFTH EIGHTEEN FIFTY SIX\"])\n\n[['footnote', 'somber', 'tashan', 'may', '12', '1856'],\n ['footnote', 'sumner', 'to', 'shannon', 'may', '12th', '1856']]\n\n\n\nsource\n\nlibrispeech_data\n\n librispeech_data (datadir, sample_rate=16000)\n\n\nsource\n\n\nDfBuilder\n\n DfBuilder ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nWERStats\n\n WERStats (transform=&lt;jiwer.transforms.Compose object at 0x7f34cf38d340&gt;)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "5a. t2s dataset preparation.html",
    "href": "5a. t2s dataset preparation.html",
    "title": "T2S dataset preparation",
    "section": "",
    "text": "flac_to_t2s_name('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar')\n\n'librilight-large-6454-t2s-000000.tar.gz'\n\n\n\nds = wds.WebDataset(['/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar'], rename_files=vad.fix_dots_in_names).compose(wds.decode(wds.torch_audio))\nsum([x['flac'][0].shape[-1]/16000/3600 for x in progress_bar(ds, total='noinfer')])\n\n\n\n\n\n\n    \n      \n      100.00% [232/232 00:47&lt;00:00]\n    \n    \n\n\n85.79258390624999\n\n\n\nds = wds.WebDataset(['/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar'], rename_files=vad.fix_dots_in_names).compose(\n    wds.decode(wds.torch_audio),\n    vq_stoks.merge_in(vq_stoks.derived_dataset('/data/whisperspeech-processed-wds/', 'vad')),\n    wds.map_dict(**{\"vad.npy\": lambda s: wh_transcribe.chunk_merger(s, wh_transcribe.random_cutter)}),\n    lambda x: wh_transcribe.split_to_chunks(x),\n    # drop the first and last segment because they tend to be inaccurate\n    # (the transcriptions don't have the \"LibriVox\" header and \"end of chapter\" suffix)\n    wds.select(lambda x: x['i'] != 0 and x['i'] != x['imax']),\n)\nsum([x['samples'].shape[-1] for x in progress_bar(ds, total='noinfer')])\n\n\n\n\n\n\n    \n      \n      100.00% [16479/16479 00:59&lt;00:00]\n    \n    \n\n\ntensor([-15.7778, -14.8559, -13.3381,  ...,  -0.0186,   0.1787,   0.2196])\n\n\n\nprepare_t2s('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=500, batch_size=32)\n\nLightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.9. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../.cache/torch/whisperx-vad-segmentation.bin`\n\n\nModel was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n\n\n\n\n\n\n\n    \n      \n      100.00% [15/15 00:18&lt;00:00]\n    \n    \n\n\n\nprepare_t2s('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=500, batch_size=32)\n\nLightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.9. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../.cache/torch/whisperx-vad-segmentation.bin`\n\n\nModel was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\n\n\n\n\n    \n      \n      100.00% [15/15 00:22&lt;00:00]\n    \n    \n\n\n\n## Batch size tests\n\n\nprepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:55&lt;00:00]\n    \n    \n\n\n\nprepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=1000, batch_size=2)\n\n\n\n\n\n\n    \n      \n      100.00% [500/500 00:34&lt;00:00]\n    \n    \n\n\n\nprepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=1000, batch_size=4)\n\n\n\n\n\n\n    \n      \n      100.00% [250/250 00:24&lt;00:00]\n    \n    \n\n\n\nprepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=1000, batch_size=8)\n\n\n\n\n\n\n    \n      \n      100.00% [125/125 00:20&lt;00:00]\n    \n    \n\n\n\n# stoks only\nprepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=1000, batch_size=4)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:27&lt;00:00]\n    \n    \n\n\n\n# atoks only\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n    prepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=10, batch_size=1)\nprof.export_chrome_trace(\"trace-bs1.json\")\n\nSTAGE:2023-10-06 14:25:45 71030:71030 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\nSTAGE:2023-10-06 14:25:47 71030:71030 ActivityProfilerController.cpp:317] Completed Stage: Collection\nSTAGE:2023-10-06 14:25:47 71030:71030 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:01&lt;00:00]\n    \n    \n\n\n\n!ls -lh librilight-large-6454-s2a-000000.tar.gz.tmp\n!tar -tf librilight-large-6454-s2a-000000.tar.gz.tmp\n\n-rw-r--r-- 1 root root 1.3M Oct  6 09:17 librilight-large-6454-s2a-000000.tar.gz.tmp\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_000.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_001.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_001.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_002.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_002.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_003.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_003.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_004.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_004.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_005.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_005.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_006.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_006.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_007.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_007.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_008.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_008.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_009.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_009.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_010.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_010.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_011.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_011.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_012.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_012.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_013.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_013.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_014.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_014.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_015.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_015.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_016.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_016.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_017.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_017.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_018.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_018.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_019.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_019.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_020.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_020.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_021.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_021.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_022.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_022.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_023.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_023.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_024.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_024.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_025.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_025.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_026.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_026.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_000.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_001.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_001.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_002.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_002.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_003.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_003.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_004.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_004.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_005.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_005.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_006.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_006.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_007.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_007.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_008.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_008.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_009.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_009.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_010.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_010.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_011.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_011.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_012.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_012.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_013.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_013.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_014.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_014.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_015.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_015.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_016.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_016.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_017.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_017.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_018.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_018.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_019.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_019.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_020.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_020.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_000.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_001.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_001.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_002.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_002.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_003.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_003.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_004.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_004.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_005.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_005.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_006.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_006.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_007.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_007.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_008.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_008.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_009.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_009.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_010.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_010.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_011.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_011.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_012.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_012.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_013.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_013.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_014.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_014.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_000.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_001.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_001.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_002.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_002.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_003.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_003.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_004.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_004.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_005.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_005.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_006.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_006.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_007.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_007.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_008.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_008.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_009.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_009.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_010.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_010.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_011.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_011.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_012.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_012.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_013.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_013.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_014.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_014.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_015.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_015.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_016.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_016.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_000.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_001.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_001.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_002.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_002.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_003.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_003.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_004.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_004.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_005.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_005.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_006.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_006.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_007.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_007.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_008.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_008.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_009.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_009.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_010.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_010.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_011.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_011.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_012.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_012.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_013.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_013.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_014.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_014.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_015.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_015.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_016.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_016.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_017.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_017.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_018.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_018.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_06_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_06_kipling_64kb_000.stoks.npy"
  },
  {
    "objectID": "4a. s2a dataset preparation.html",
    "href": "4a. s2a dataset preparation.html",
    "title": "S2A dataset preparation",
    "section": "",
    "text": "source\n\nflac_to_s2a_name\n\n flac_to_s2a_name (input)\n\n\nflac_to_s2a_name('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar')\n\n'librilight-large-6454-s2a-000000.tar.gz'\n\n\n\nds = wds.WebDataset([\"/data2/libritts-r-raw-000002.tar\"]).compose(\n        wds.decode(wds.torch_audio),\n        wds.select(lambda x: 'wav' in x or 'flac' in x),\n        vq_stoks.merge_in(vq_stoks.derived_dataset('/data2/processed/', 'vad')),\n        wds.map_dict(**{\"vad.npy\":wh_transcribe.chunk_merger}),\n        lambda x: wh_transcribe.split_to_chunks(x),\n)\n\n\nfor x in progress_bar(ds, total='noinfer'): pass\nx\n\n\n\n\n\n\n    \n      \n      100.00% [17937/17937 00:20&lt;00:00]\n    \n    \n\n\n{'__key__': 'train-clean-360/1638/84447/1638_84447_000100_000001_000',\n '__url__': '/data2/libritts-r-raw-000002.tar',\n 'i': 0,\n 'imax': 0,\n 'tstart': 0.00844,\n 'tend': 3.232,\n 'total_seconds': 3.28,\n 'lpad': 0,\n 'rpad': 642624,\n 'lpad_s': 0.0,\n 'rpad_s': 26.776,\n 'samples': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n 'sample_rate': 24000}\n\n\n\nprepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=5)\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 00:01&lt;00:00]\n    \n    \n\n\n\n## Batch size tests\n\n\nprepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:55&lt;00:00]\n    \n    \n\n\n\nprepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=1000, batch_size=2)\n\n\n\n\n\n\n    \n      \n      100.00% [500/500 00:34&lt;00:00]\n    \n    \n\n\n\nprepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=1000, batch_size=4)\n\n\n\n\n\n\n    \n      \n      100.00% [250/250 00:24&lt;00:00]\n    \n    \n\n\n\nprepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=1000, batch_size=8)\n\n\n\n\n\n\n    \n      \n      100.00% [125/125 00:20&lt;00:00]\n    \n    \n\n\n\n# stoks only\nprepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=1000, batch_size=4)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:27&lt;00:00]\n    \n    \n\n\n\n# atoks only\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n    prepare_s2a('/data/whisperspeech-wds/librilight-large-6454-flac-000000.tar', '/data/whisperspeech-processed-wds/', vq_model='vqmodel-4e-hyptuned-32gpu.model', n_samples=10, batch_size=1)\nprof.export_chrome_trace(\"trace-bs1.json\")\n\nSTAGE:2023-10-06 14:25:45 71030:71030 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\nSTAGE:2023-10-06 14:25:47 71030:71030 ActivityProfilerController.cpp:317] Completed Stage: Collection\nSTAGE:2023-10-06 14:25:47 71030:71030 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:01&lt;00:00]\n    \n    \n\n\n\n!ls -lh librilight-large-6454-s2a-000000.tar.gz.tmp\n!tar -tf librilight-large-6454-s2a-000000.tar.gz.tmp\n\n-rw-r--r-- 1 root root 1.3M Oct  6 09:17 librilight-large-6454-s2a-000000.tar.gz.tmp\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_000.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_001.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_001.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_002.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_002.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_003.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_003.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_004.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_004.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_005.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_005.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_006.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_006.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_007.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_007.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_008.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_008.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_009.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_009.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_010.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_010.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_011.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_011.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_012.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_012.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_013.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_013.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_014.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_014.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_015.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_015.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_016.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_016.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_017.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_017.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_018.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_018.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_019.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_019.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_020.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_020.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_021.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_021.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_022.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_022.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_023.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_023.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_024.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_024.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_025.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_025.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_026.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_01_kipling_64kb_026.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_000.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_001.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_001.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_002.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_002.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_003.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_003.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_004.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_004.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_005.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_005.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_006.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_006.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_007.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_007.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_008.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_008.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_009.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_009.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_010.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_010.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_011.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_011.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_012.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_012.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_013.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_013.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_014.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_014.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_015.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_015.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_016.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_016.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_017.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_017.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_018.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_018.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_019.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_019.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_020.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_02_kipling_64kb_020.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_000.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_001.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_001.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_002.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_002.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_003.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_003.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_004.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_004.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_005.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_005.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_006.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_006.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_007.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_007.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_008.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_008.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_009.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_009.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_010.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_010.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_011.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_011.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_012.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_012.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_013.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_013.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_014.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_03_kipling_64kb_014.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_000.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_001.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_001.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_002.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_002.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_003.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_003.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_004.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_004.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_005.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_005.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_006.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_006.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_007.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_007.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_008.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_008.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_009.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_009.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_010.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_010.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_011.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_011.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_012.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_012.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_013.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_013.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_014.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_014.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_015.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_015.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_016.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_04_kipling_64kb_016.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_000.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_001.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_001.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_002.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_002.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_003.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_003.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_004.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_004.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_005.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_005.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_006.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_006.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_007.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_007.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_008.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_008.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_009.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_009.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_010.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_010.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_011.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_011.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_012.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_012.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_013.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_013.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_014.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_014.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_015.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_015.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_016.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_016.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_017.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_017.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_018.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_05_kipling_64kb_018.stoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_06_kipling_64kb_000.atoks.npy\nlarge/6454/abaft_funnel_1307_librivox_64kb_mp3/funnel_06_kipling_64kb_000.stoks.npy"
  },
  {
    "objectID": "D. Common dataset utilities.html",
    "href": "D. Common dataset utilities.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\nshard_glob\n\n shard_glob (input)\n\n\nsource\n\n\njoin_datasets\n\n join_datasets (datasets)\n\nAn iterable Dataset.\nAll datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.\nAll subclasses should overwrite :meth:__iter__, which would return an iterator of samples in this dataset.\nWhen a subclass is used with :class:~torch.utils.data.DataLoader, each item in the dataset will be yielded from the :class:~torch.utils.data.DataLoader iterator. When :attr:num_workers &gt; 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:~torch.utils.data.get_worker_info, when called in a worker process, returns information about the worker. It can be used in either the dataset’s :meth:__iter__ method or the :class:~torch.utils.data.DataLoader ’s :attr:worker_init_fn option to modify each copy’s behavior.\nExample 1: splitting workload across all workers in :meth:__iter__::\n&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_DATALOADER)\n&gt;&gt;&gt; # xdoctest: +SKIP(\"Fails on MacOS12\")\n&gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end &gt; start, \"this example code only works with end &gt;= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n&gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n&gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)\n\n&gt;&gt;&gt; # Single-process loading\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[tensor([3]), tensor([4]), tensor([5]), tensor([6])]\n\n&gt;&gt;&gt; # xdoctest: +REQUIRES(POSIX)\n&gt;&gt;&gt; # Mult-process loading with two worker processes\n&gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n&gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non deterministic\")\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[tensor([3]), tensor([5]), tensor([4]), tensor([6])]\n\n&gt;&gt;&gt; # With even more workers\n&gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non deterministic\")\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=12)))\n[tensor([3]), tensor([5]), tensor([4]), tensor([6])]\nExample 2: splitting workload across all workers using :attr:worker_init_fn::\n&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_DATALOADER)\n&gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end &gt; start, \"this example code only works with end &gt;= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         return iter(range(self.start, self.end))\n...\n&gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n&gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)\n\n&gt;&gt;&gt; # Single-process loading\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Directly doing multi-process loading yields duplicate data\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 3, 4, 4, 5, 5, 6, 6]\n\n&gt;&gt;&gt; # Define a `worker_init_fn` that configures each dataset copy differently\n&gt;&gt;&gt; def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n...\n\n&gt;&gt;&gt; # Mult-process loading with the custom `worker_init_fn`\n&gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n[3, 5, 4, 6]\n\n&gt;&gt;&gt; # With even more workers\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=12, worker_init_fn=worker_init_fn)))\n[3, 4, 5, 6]\n\n# will stop as soon as it exhausts one iterator\nfor x in join_datasets(['ABCDEFG', 'abcdefg', range(20)]):\n    print(x)\n\n0\na\n1\n2\n3\nA\n4\n5\nb\nB\nc\nC\nD\nE\n6\nd\ne\n7\nF\nf\ng\n8\nG\n9\n\n\n\nsource\n\n\nresampler\n\n resampler (newsr=24000, key='samples_24k')\n\n\nsource\n\n\nderived_name\n\n derived_name (input, kind, base='audio', suffix='.gz', dir=None)\n\n\nsource\n\n\nderived_dataset\n\n derived_dataset (kind, base='audio', suffix='.gz', decoders=[], dir=None)\n\n\nsource\n\n\nmerge_in\n\n merge_in (dataset_fun)\n\nMerge a dataset into the current one returning samples with the union of keys. Pass in a function that takes a URL of a sample and returns a dataset for it (called everytime the URL changes).\nIt requires (and validates) that both datasets have the same ordering of keys so you have to use it before any sample shuffling. Shard shuffling is ok.\n\nsource\n\n\nAtomicTarWriter\n\n AtomicTarWriter (name, throwaway=False)\n\n\nsource\n\n\nreadlines\n\n readlines (fname)"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html",
    "href": "2A. Whisper quantization dataset preparation.html",
    "title": "Precompute Whisper transcriptions for VQ bottleneck distilation",
    "section": "",
    "text": "Doing transcription means sampling from the Whisper auto-regresive decoder. This is too slow to do for each training batch. Fortunately the trainscriptions are small text snippets so we can precompute them once for the whole dataset.\nWe use segments from Voice Activity Detection to reduce any boundary issues, the we use webdataset to yields multiple chunks from a FLAC file we only load once. The VAD segments are merged into longer chunks to make Whisper processing more efficent (it always processes 30s at a time)\nUsage:\nYou can pass in either a URL or a local file name. Either way it will expect a vad file in the local directory. The result will go into a file in the current directory named after the source file but replacing flac with txt.\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nimport pylab as plt\nimport IPython\nflac_url = 'https://huggingface.co/datasets/collabora/librilight-webdataset/resolve/main/librilight-small-flac-000000.tar'\nflac_url = './librilight-small-flac-000000.tar'"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#merge-vad-segments-into-longer-chunks",
    "href": "2A. Whisper quantization dataset preparation.html#merge-vad-segments-into-longer-chunks",
    "title": "Precompute Whisper transcriptions for VQ bottleneck distilation",
    "section": "Merge VAD segments into longer chunks",
    "text": "Merge VAD segments into longer chunks\n\n# load some VAD ouputs\nds = wds.WebDataset(\n    vad.flac_to_vad_name(flac_url)\n).decode().to_tuple('vad.npy')\nchunks = [x[0] for x in progress_bar(ds, total='noinfer')]\n\n\n\n\n\n\n    \n      \n      100.00% [335/335 00:00&lt;00:00]\n    \n    \n\n\n\n# quick test\nlen(chunks[0]), len(chunk_merger(chunks[0]))\n\n(46, 28)\n\n\n\nplt.hist([te-ts for x in chunks for ts,te in x])\nplt.title('Segment length distribution straight out of the VAD algorithm');\n\n\n\n\n\nplt.hist([te-ts for x in chunks for ts,te in chunk_merger(x)]);\nplt.title('Chunk length distribution after greedy merging');\n\n\n\n\n\n(np.array([te-ts for x in chunks for ts,te in chunk_merger(x)]) &lt; 10).mean()\n\n0.03671825647504738\n\n\nIn the above distribution only 3,7% of the samples have &lt; 10 seconds. We noticed that this limits the ability of the T2S model to generate short sequences reliably.\nIt does not seem to matter for quantizing Whisper so we can keep this distribution (it uses less compute for training).\nFor T2S we can add some more shorter chunks at random:\n\nplt.hist([te-ts for x in chunks for ts,te in chunk_merger(x, random_cutter)])\nplt.title('Chunk length distribution after randomized merging');"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#merge-the-flac-and-vad-datasets",
    "href": "2A. Whisper quantization dataset preparation.html#merge-the-flac-and-vad-datasets",
    "title": "Precompute Whisper transcriptions for VQ bottleneck distilation",
    "section": "Merge the FLAC and VAD datasets",
    "text": "Merge the FLAC and VAD datasets\nFirst we want to merge the VAD dataset with the FLAC audio data.\n\nds = wds_compose(vad.load_dataset(flac_url),\n    merge_in(wds.WebDataset(vad.flac_to_vad_name(flac_url)).decode())\n)\n\n\nfor s in ds: break\ns # notice the 'vad.npy' values that was missing from the FLAC dataset\n\n{'__key__': 'small/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb',\n '__url__': 'librilight-small-vad-000000.tar.gz',\n 'flac': (tensor([[0., 0., 0.,  ..., 0., 0., 0.]]), 16000),\n 'json': {'speaker': '100',\n  'book_meta': {'id': '2315',\n   'title': 'Sea Fairies',\n   'description': \"&lt;p&gt;In 1910, Baum hoped to end the Oz series and follow with a new series about a little girl named Trot and her sailor companion, Cap'n Bill. The Sea Fairies (1911) was the first book in the projected series and took Trot and Cap'n Bill under the sea where they had adventures with mermaids and other fantastic creatures. It was followed by Sky Island (1912) and then Baum returned to the Oz titles. He brought Trot and Cap'n Bill to Oz in the Scarecrow of Oz (1915). (Summary by Judy Bieber)&lt;/p&gt;\",\n   'url_text_source': 'http://www.gutenberg.org/etext/4358',\n   'language': 'English',\n   'copyright_year': '1911',\n   'num_sections': '22',\n   'url_rss': 'https://librivox.org/rss/2315',\n   'url_zip_file': 'http://www.archive.org/download/sea_fairies_0812_librivox/sea_fairies_0812_librivox_64kb_mp3.zip',\n   'url_project': 'http://en.wikipedia.org/wiki/The_Sea_Fairies',\n   'url_librivox': 'https://librivox.org/the-sea-fairies-by-l-frank-baum/',\n   'url_other': None,\n   'totaltimesecs': 15311,\n   'authors': [{'id': '406',\n     'first_name': 'L. Frank',\n     'last_name': 'Baum',\n     'dob': '1856',\n     'dod': '1919'}],\n   'genre': ['Action & Adventure'],\n   'Dramatic Readings': False,\n   'meta_genre': 'Literature'},\n  'snr': 11.4471,\n  'voice_activity': [[1.52, 11.2],\n   [11.84, 14.08],\n   [15.12, 35.76],\n   [36.32, 55.6],\n   [56.24, 70.48],\n   [71.28, 79.52],\n   [80.08, 89.76],\n   [90.24, 97.52],\n   [98.0, 101.28],\n   [102.8, 124.88],\n   [125.36, 133.12],\n   [133.68, 154.16],\n   [154.64, 177.2],\n   [178.0, 196.96],\n   [197.68, 211.44],\n   [212.32, 216.32],\n   [216.96, 243.52],\n   [244.0, 250.72],\n   [251.52, 268.32],\n   [268.96, 308.56],\n   [309.04, 315.28],\n   [316.0, 317.36],\n   [317.92, 325.44],\n   [326.24, 343.6],\n   [344.08, 350.32],\n   [350.88, 356.64],\n   [357.2, 363.2],\n   [363.76, 365.2],\n   [365.2, 373.2],\n   [373.84, 392.0],\n   [392.56, 401.04],\n   [401.6, 456.96],\n   [457.68, 501.92],\n   [502.4, 531.04],\n   [531.6, 554.48],\n   [554.96, 568.32],\n   [568.96, 585.84],\n   [587.04, 588.48],\n   [597.12, 597.92]]},\n 'vad.npy': array([[  1.764,   6.49 ],\n        [  6.773,  11.18 ],\n        [ 11.98 ,  14.03 ],\n        [ 15.31 ,  36.3  ],\n        [ 36.3  ,  56.06 ],\n        [ 56.4  ,  70.6  ],\n        [ 71.4  , 101.2  ],\n        [102.75 , 103.56 ],\n        [103.7  , 121.75 ],\n        [122.06 , 125.   ],\n        [125.44 , 133.4  ],\n        [133.8  , 154.6  ],\n        [154.6  , 177.6  ],\n        [178.1  , 197.2  ],\n        [197.9  , 212.1  ],\n        [212.5  , 222.5  ],\n        [222.8  , 243.6  ],\n        [244.2  , 246.5  ],\n        [246.8  , 251.1  ],\n        [251.5  , 256.2  ],\n        [256.5  , 257.8  ],\n        [258.2  , 259.8  ],\n        [259.8  , 268.5  ],\n        [269.2  , 289.8  ],\n        [289.8  , 315.8  ],\n        [316.   , 317.2  ],\n        [318.   , 319.   ],\n        [319.8  , 344.   ],\n        [344.2  , 350.2  ],\n        [351.   , 352.5  ],\n        [353.   , 356.8  ],\n        [357.5  , 373.5  ],\n        [374.   , 388.   ],\n        [388.2  , 397.2  ],\n        [397.5  , 401.5  ],\n        [401.8  , 423.5  ],\n        [423.5  , 448.   ],\n        [448.   , 457.2  ],\n        [457.8  , 460.8  ],\n        [461.   , 477.8  ],\n        [478.5  , 502.2  ],\n        [502.2  , 527.5  ],\n        [527.5  , 550.5  ],\n        [550.5  , 576.5  ],\n        [577.   , 586.   ],\n        [587.5  , 588.5  ]], dtype=float16)}"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#split-the-audio-into-chunks",
    "href": "2A. Whisper quantization dataset preparation.html#split-the-audio-into-chunks",
    "title": "Precompute Whisper transcriptions for VQ bottleneck distilation",
    "section": "Split the audio into chunks",
    "text": "Split the audio into chunks\nAfter we merge the datasets and chunk the segments we can split each audio file into individual samples and pad them to 30s.\n\nsplit_ds = wds_compose(ds,\n   wds.map_dict(**{\"vad.npy\":chunk_merger}),\n   split_to_chunks,\n)\n\n\nfor s in split_ds: break\ns\n\n{'__key__': 'small/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_000',\n '__url__': 'librilight-small-vad-000000.tar.gz',\n 'i': 0,\n 'imax': 27,\n 'tstart': 1.764,\n 'tend': 14.03,\n 'total_seconds': 597.9425,\n 'samples': tensor([0., 0., 0.,  ..., 0., 0., 0.])}"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#transcribe",
    "href": "2A. Whisper quantization dataset preparation.html#transcribe",
    "title": "Precompute Whisper transcriptions for VQ bottleneck distilation",
    "section": "Transcribe",
    "text": "Transcribe\n\nwhmodel = whisper.load_model('base.en')\ndecoding_options = whisper.DecodingOptions(language='en')\n\n\noutput = flac_url.rsplit(\"/\", 1)[1].replace('flac', 'txt') + \".gz\"\nwith wds.TarWriter(output) as sink:\n    for s in progress_bar(split_ds, total=256):\n        mel = whisper.log_mel_spectrogram(s['samples'].unsqueeze(0).cuda())\n        embs = whmodel.encoder(mel)\n        decs = whmodel.decode(embs, decoding_options)\n\n        sink.write({\n            \"__key__\": s['__key__'],\n            \"txt\": decs[0].text,\n        })\n\n\n\n\n\n\n    \n      \n      100.00% [256/256 00:59&lt;00:00]"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#transcribe-in-batches",
    "href": "2A. Whisper quantization dataset preparation.html#transcribe-in-batches",
    "title": "Precompute Whisper transcriptions for VQ bottleneck distilation",
    "section": "Transcribe in batches",
    "text": "Transcribe in batches\nWe have one more thing to add – batch processing makes the transcription quite a bit faster (bs=16 brings a 4.5x speedup).\n\nbatched_ds = wds_compose(split_ds,\n    wds.to_tuple('__key__', 'samples'),\n    wds.batched(16),\n)\n\n\noutput = flac_url.rsplit(\"/\", 1)[1].replace('flac', 'txt') + \".gz\"\nwith wds.TarWriter(output) as sink:\n    for keys, samples in progress_bar(batched_ds, total=256//16):\n        mel = whisper.log_mel_spectrogram(samples).cuda()\n        embs = whmodel.encoder(mel)\n        decs = whmodel.decode(embs, decoding_options)\n        for key, dec in zip(keys, decs):\n            sink.write({\n                \"__key__\": key,\n                \"txt\": dec.text,\n            })\n\n\n\n\n\n\n    \n      \n      100.00% [16/16 00:11&lt;00:00]"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#verify-the-transcripts-and-the-chunks-work-together",
    "href": "2A. Whisper quantization dataset preparation.html#verify-the-transcripts-and-the-chunks-work-together",
    "title": "Precompute Whisper transcriptions for VQ bottleneck distilation",
    "section": "Verify the transcripts and the chunks work together",
    "text": "Verify the transcripts and the chunks work together\n\ntxt_ds = wds_compose(split_ds,\n    merge_in(wds.WebDataset(flac_url.rsplit(\"/\", 1)[1].replace('flac', 'txt') + \".gz\").decode())\n)\n\n\nfor x in txt_ds: break\nx\n\n{'__key__': 'small/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_000',\n '__url__': 'librilight-small-txt-000000.tar.gz',\n 'i': 0,\n 'imax': 27,\n 'tstart': 1.764,\n 'tend': 14.03,\n 'total_seconds': 597.9425,\n 'samples': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n 'txt': 'This is a LibraVox recording. Holy bivox recordings are in the public domain. For more information or to volunteer, please visit libravox.org.'}\n\n\n\nfor x in progress_bar(txt_ds, total=10):\n    IPython.display.display(IPython.display.Markdown(f\"#### {x['__key__']} chunk {x['i']} of {x['imax']}\"))\n    fname = f\"test-{x['i']}.ogg\"\n    torchaudio.save(fname, x['samples'][None,:int((x['tend']-x['tstart'])*16000)], 16000)\n    IPython.display.display(IPython.display.Audio(url=fname, rate=16000))\n    IPython.display.display(IPython.display.Markdown(x['txt']))\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00&lt;00:00]\n    \n    \n\n\nsmall/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_000 chunk 0 of 27\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThis is a LibraVox recording. Holy bivox recordings are in the public domain. For more information or to volunteer, please visit libravox.org.\n\n\nsmall/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_001 chunk 1 of 27\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThe oceans are being in broad. I believe two-thirds of the Earth’s surface is covered with water. What people inhabit this water has always been a subject of curiosity to the inhabitants of the land. Strange creatures come from the seas at times, and perhaps in the ocean depths are many more strange than mortal eye has ever gazed upon.\n\n\nsmall/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_002 chunk 2 of 27\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThis story is fanciful. In it the sea people talk and act much as we do, and the mermaids especially are not unlike the fairies with whom we have learned to be familiar. Yet they are real sea people for all that, and with the exception of Zog the magician, they are all supposed to exist in the ocean steps.\n\n\nsmall/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_003 chunk 3 of 27\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nI am told that some very learned people deny that mermaids or sea serpents have ever inhabited the oceans. But it would be very difficult for them to prove such an assertion, unless they had lived under the water as trot and caten-billed did in this story.\n\n\nsmall/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_004 chunk 4 of 27\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nI hope my readers who have so long followed Dorothy’s adventures in the Land of Oz will be interested in Trot’s equally strange experiences. The ocean has always appealed to me as a veritable wonderland, and this story has been suggested to me many times by my young correspondence in their letters. Indeed, a good many children have implored me to write something about the mermaids, and I have willingly granted the request.\n\n\nsmall/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_005 chunk 5 of 27\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nCHAPTER I. TROT AND CAPTAIN BILL. Nobody said Captain Bill solemnly, ever saw a mermaid and lived to tell the tale. Why not, asked TROT, looking earnestly up into the old sailor’s face? They were seated on a bench built around a giant acacia tree that grew just at the edge of the bluff. Below them rolled the blue waves of the great Pacific.\n\n\nsmall/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_006 chunk 6 of 27\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nA little way behind them was the house, a neat framed cottage painted white and surrounded by huge eucalyptus and pepper trees.\n\n\nsmall/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_007 chunk 7 of 27\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nStill farther behind that, a quarter of a mile distant but built upon a bend of the coast was the village overlooking a pretty bay. Catonville and Trot came often to this tree to sit and watch the ocean below them. The sailor man had one meat leg and one hickory leg, and he often said the wooden one was the best of the two.\n\n\nsmall/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_008 chunk 8 of 27\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nOnce Catenville had commanded and owned the anemone, a trading schooner that plied along the coast, and in those days Charlie Griffin’s, who was trot’s father, had been the captain’s mate. But ever since Catenville’s accident when he lost his leg, Charlie Griffiths had been the captain of the little schooner, while his old master lived peacefully ashore with the Griffiths family.\n\n\nsmall/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb_009 chunk 9 of 27\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThis was about the time Trot was born, and the old sailor became very fond of the baby girl. Her real name was Mary, but when she grew big enough to walk, she took so many busy little steps every day that both her mother and Captain Bill nicknamed her Trot, and so she was thereafter mostly called."
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#batch-processing",
    "href": "2A. Whisper quantization dataset preparation.html#batch-processing",
    "title": "Precompute Whisper transcriptions for VQ bottleneck distilation",
    "section": "Batch processing",
    "text": "Batch processing\nLet’s put everything above together."
  }
]